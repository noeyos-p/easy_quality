"""
RAG ì±—ë´‡ API v14.0 + Agent (OpenAI)

 v14.0 ë³€ê²½ì‚¬í•­:
- LLM ë°±ì—”ë“œ ë³€ê²½: Z.AI â†’ OpenAI GPT ê³„ì—´
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•© (ëª¨ë“  ì„œë¸Œ ì—ì´ì „íŠ¸ OpenAI ì‚¬ìš©)
- LLM as a Judge í‰ê°€ ì‹œìŠ¤í…œ (RDB ê²€ì¦ í¬í•¨)
- LangSmith ì¶”ì  ì§€ì› ë° ìµœì í™”
"""

#  .env íŒŒì¼ ìë™ ë¡œë“œ (ë‹¤ë¥¸ importë³´ë‹¤ ë¨¼ì €!)
from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request, Depends, status, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, Response
from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Optional, Literal
from contextlib import asynccontextmanager
import torch
import time
import re
import uuid
import asyncio
import json
import os
import jwt
from io import BytesIO

from backend.sql_store import SQLStore
sql_store = SQLStore()
# sql_store.init_db()  #  main()ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€

# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©)
# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©) ì œê±°ë¨
# LangGraph íŒŒì´í”„ë¼ì¸ì´ ì „ì ìœ¼ë¡œ ì²˜ë¦¬

from sentence_transformers import SentenceTransformer
from backend import vector_store
from backend.vector_store import embed_text
# from backend.prompt import build_rag_prompt, build_chunk_prompt (ì œê±°ë¨)
from backend.llm import (
    get_llm_response,
    ZaiLLM,
    OllamaLLM,
    analyze_search_results,
    HUGGINGFACE_MODELS,
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì • ë° ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 500
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 7
DEFAULT_SIMILARITY_THRESHOLD = 0.30
USE_LANGGRAPH = True
FORCED_LLM_MODEL = "gpt-4o"

def resolve_effective_llm_model(requested_model: Optional[str]) -> str:
    """í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ê°’ê³¼ ë¬´ê´€í•˜ê²Œ ì„œë²„ ëª¨ë¸ì„ gpt-4oë¡œ ê³ ì •."""
    return FORCED_LLM_MODEL

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    user_id: Optional[int] = None
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "gpt-4o"
    llm_backend: str = "openai"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None

class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "gpt-4o"
    llm_backend: str = "openai"
    temperature: float = 0.7
    filter_doc: Optional[str] = None
    language: str = "ko"
    max_tokens: int = 512
    similarity_threshold: Optional[float] = None
    include_sources: bool = True

class LLMRequest(BaseModel):
    prompt: str
    model: str = "qwen2.5:3b"
    backend: str = "ollama"
    max_tokens: int = 256
    temperature: float = 0.1

class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"
    delete_from_neo4j: bool = True

class DeleteDocsBatchRequest(BaseModel):
    doc_names: List[str]
    collection: str = "documents"
    delete_from_neo4j: bool = True

class SaveDocRequest(BaseModel):
    doc_name: str
    content: str
    collection: str = "documents"
    model: str = "multilingual-e5-small"

PRESET_MODELS = {
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
}

device = "cuda" if torch.cuda.is_available() else "cpu"
chat_histories: Dict[str, List[Dict]] = {}
chat_results: Dict[str, Dict] = {}
chat_queue: asyncio.Queue = asyncio.Queue()

# ticketì€ ë‹¨ì¡° ì¦ê°€(ì„œë²„ ì‚´ì•„ìˆëŠ” ë™ì•ˆ 1,2,3,4...)
next_ticket: int = 0
# pendingì€ request_idë§Œ ë“¤ê³  ìˆì§€ ë§ê³  ticketë„ ê°™ì´ ë“¤ê³  ìˆê²Œ
# [{"request_id": "...", "ticket": 1, "kind": "rag"|"agent"} ...]
chat_pending: List[Dict] = []
ticket_lock = asyncio.Lock()

_graph_store = None

QUEUE_STATE_FILE = "backend/queue_state.json"
queue_lock = asyncio.Lock()

# ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ ê´€ë¦¬ (ë¬¸ì„œ ì—…ë¡œë“œ, ìˆ˜ì • ë“±)
processing_tasks: Dict[str, Dict] = {}

def update_task_status(task_id: str, status: str, message: str = "", **kwargs):
    """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ í—¬í¼"""
    if task_id not in processing_tasks:
        processing_tasks[task_id] = {"id": task_id, "created_at": time.time()}
    
    processing_tasks[task_id].update({
        "status": status,
        "message": message,
        "updated_at": time.time(),
        **kwargs
    })
    print(f" â± [Task {task_id}] {status}: {message}")

def load_queue_state():
    """íŒŒì¼ì—ì„œ í ìƒíƒœë¥¼ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì¸ë©”ëª¨ë¦¬ ì „ìš© ëª¨ë“œ)"""
    pass

def save_queue_state():
    """íŒŒì¼ì— í ìƒíƒœë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì¸ë©”ëª¨ë¦¬ ì „ìš© ëª¨ë“œ)"""
    pass

def get_graph_store():
    """Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ ì‹±ê¸€í†¤"""
    global _graph_store
    try:
        if _graph_store is None:
            from backend.graph_store import Neo4jGraphStore
            _graph_store = Neo4jGraphStore()
            _graph_store.connect()
        return _graph_store
    except Exception as e:
        print(f" ğŸ”´ [Graph Store] ì‹±ê¸€í†¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

#  Document pipeline
try:
    from backend.document_pipeline import process_document
    from dataclasses import dataclass

    @dataclass
    class Chunk:
        text: str
        metadata: dict
        index: int = 0

    LANGGRAPH_AVAILABLE = True
    print(" Document pipeline ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    print(f" Document pipeline ì‚¬ìš© ë¶ˆê°€: {e}")

try:
    import langchain
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

@asynccontextmanager
async def lifespan(app: FastAPI):
    global _graph_store
    # Startup
    # (ì¸ë©”ëª¨ë¦¬ ì „ìš© ëª¨ë“œ: íŒŒì¼ ë³µêµ¬ ë¡œì§ ì œê±°)
    requeued_count = 0

    print(f"ğŸš€ ì±„íŒ… ì›Œì»¤(Worker) ê°€ë™ ì¤‘... (ë³µêµ¬ëœ ì‘ì—…: {requeued_count}ê°œ)")
    worker_task = asyncio.create_task(chat_worker())

    yield

    # Shutdown
    print("\n ì„œë²„ ì¢…ë£Œ ì¤‘...")
    worker_task.cancel()
    try:
        await worker_task
    except asyncio.CancelledError:
        print(" ì±„íŒ… ì›Œì»¤ ì¢…ë£Œë¨")

    try:
        vector_store.close_client()
    except Exception as ve:
        print(f" Weaviate ì—°ê²° ì¢…ë£Œ ì‹¤íŒ¨: {ve}")

    if _graph_store:
        try:
            _graph_store.close()
            print(" Neo4j ì—°ê²° ì¢…ë£Œë¨")
        except Exception as ge:
            print(f" Neo4j ì—°ê²° ì¢…ë£Œ ì‹¤íŒ¨: {ge}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PDF ë³€í™˜ ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def md_to_pdf_binary(md_text: str) -> bytes:
    """
    ë§ˆí¬ë‹¤ìš´ì„ PDF ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜
    - <!-- PAGE:n --> ë§ˆì»¤ë¥¼ í˜ì´ì§€ ë¶„í• (<pdf:nextpage />)ë¡œ ë³€í™˜
    - í”„ë¡ íŠ¸ì—”ë“œ UIì™€ ìœ ì‚¬í•œ ê³„ì¸µì  ë“¤ì—¬ì“°ê¸° ë° ìŠ¤íƒ€ì¼ ì ìš©
    - Pretendard í•œê¸€ í°íŠ¸ ì ìš©
    """
    try:
        import markdown
        from xhtml2pdf import pisa
        import re
        
        # 1. í˜ì´ì§€ ë§ˆì»¤ ë³€í™˜
        md_text = re.sub(r'<!-- PAGE:\d+ -->', '<pdf:nextpage />', md_text)
        
        # 2. ê³„ì¸µì  êµ¬ì¡° ë¶„ì„ ë° HTML ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ App.tsx ë¡œì§ ëª¨ë°©)
        lines = md_text.split('\n')
        processed_elements = []
        global_depth = 0
        indent_increment = 12 # pt
        
        paragraph_buffer = []
        table_buffer = []
        in_table = False
        
        def flush_paragraph():
            if paragraph_buffer:
                text = " ".join(paragraph_buffer).strip()
                if text:
                    padding = global_depth * indent_increment
                    html_text = markdown.markdown(text, extensions=['extra', 'nl2br', 'sane_lists'])
                    processed_elements.append(f'<div style="padding-left: {padding}pt; margin-bottom: 6pt; font-size: 10pt; line-height: 1.8; color: #333;">{html_text}</div>')
                paragraph_buffer.clear()

        def flush_table():
            if table_buffer:
                table_md = "\n".join(table_buffer)
                html_table = markdown.markdown(table_md, extensions=['tables'])
                processed_elements.append(f'<div style="margin: 15pt 0;">{html_table}</div>')
                table_buffer.clear()

        for line in lines:
            trimmed = line.strip()
            
            # í˜ì´ì§€ ë¶„í•  íƒœê·¸ ì²˜ë¦¬
            if trimmed == '<pdf:nextpage />':
                flush_paragraph()
                flush_table()
                processed_elements.append('<pdf:nextpage />')
                in_table = False
                continue
            
            # í…Œì´ë¸” ê°ì§€
            if trimmed.startswith('|'):
                if not in_table:
                    flush_paragraph()
                    in_table = True
                table_buffer.append(line)
                continue
            elif in_table and trimmed:
                table_buffer.append(line)
                continue
            elif in_table and not trimmed:
                flush_table()
                in_table = False
                continue

            if not trimmed:
                continue

            # ì¡°í•­ ë²ˆí˜¸ íŒ¨í„´ ë§¤ì¹­
            section_match = re.match(r'^(\d+(?:\.\d+)*)\.?\s+(.+)', trimmed)
            
            # í˜ì´ì§€ ë²ˆí˜¸ ë¬´ì‹œ
            if section_match and re.search(r'of\s+\d+', section_match.group(2), re.I):
                continue

            if section_match:
                flush_paragraph()
                section_num = section_match.group(1)
                section_text = section_match.group(2)
                depth = section_num.count('.')
                global_depth = depth
                
                padding = depth * indent_increment
                display_text = f"{section_num} {section_text}"
                
                if depth == 0:
                    style = f"font-weight: bold; font-size: 12pt; margin-top: 40pt; margin-bottom: 8pt; color: #1a1a1a; border-bottom: 0.5pt solid #e9ecef; padding-bottom: 8pt; padding-left: {padding}pt;"
                else:
                    style = f"font-size: 11pt; margin-top: 20pt; margin-bottom: 8pt; color: #2c3e50; padding-left: {padding}pt;"
                
                processed_elements.append(f'<div style="{style}">{display_text}</div>')
            else:
                paragraph_buffer.append(trimmed)
        
        flush_paragraph()
        flush_table()
        
        final_body_html = "".join(processed_elements)

        # 3. HTML í…œí”Œë¦¿ êµ¬ì„±
        font_path = "/Users/soyeon/Library/Fonts/Pretendard-Regular.ttf"
        bold_font_path = "/Users/soyeon/Library/Fonts/Pretendard-Bold.ttf"
        
        html_template = f"""
        <html>
        <head>
            <meta charset="utf-8">
            <style>
                @font-face {{
                    font-family: 'Pretendard';
                    src: url('{font_path}');
                }}
                @font-face {{
                    font-family: 'Pretendard';
                    src: url('{bold_font_path}');
                    font-weight: bold;
                }}
                @page {{
                    size: a4;
                    margin: 2cm;
                    @frame footer {{
                        -pdf-frame-content: footer_content;
                        bottom: 1cm;
                        margin-left: 2cm;
                        margin-right: 2cm;
                        height: 1cm;
                    }}
                }}
                body {{
                    font-family: 'Malgun Gothic', 'Pretendard', sans-serif;
                    color: #333;
                }}
                table {{ 
                    width: 100%; 
                    border-collapse: collapse; 
                    margin: 15pt 0;
                }}
                th, td {{ 
                    border: 0.5pt solid #ccc; 
                    padding: 8pt; 
                    text-align: left; 
                    font-size: 9pt;
                }}
                th {{ 
                    background-color: #252526; 
                    color: #22D142; 
                    font-weight: bold; 
                }}
                tr:nth-child(even) {{ background-color: #fcfcfc; }}
            </style>
        </head>
        <body>
            {final_body_html}
            <div id="footer_content" style="text-align: center; color: #999; font-size: 9pt;">
                Page <pdf:pagenumber>
            </div>
        </body>
        </html>
        """
        
        out = BytesIO()
        pisa_status = pisa.CreatePDF(html_template, dest=out, encoding='utf-8')
        
        if pisa_status.err:
            raise Exception("PDF ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
        return out.getvalue()
    except Exception as e:
        print(f"âš  PDF ë³€í™˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"PDF ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def md_to_docx_binary(md_text: str, title: str) -> bytes:
    """ë§ˆí¬ë‹¤ìš´ì„ Word(.docx) ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ êµ¬ì¡°)"""
    try:
        from docx import Document
        from docx.shared import Pt
        
        doc = Document()
        doc.add_heading(title, 0)
        
        # ë§ˆí¬ë‹¤ìš´ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ (ì •ë°€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬)
        lines = md_text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('# '):
                doc.add_heading(line[2:], level=1)
            elif line.startswith('## '):
                doc.add_heading(line[3:], level=2)
            elif line.startswith('### '):
                doc.add_heading(line[4:], level=3)
            elif line.startswith('- ') or line.startswith('* '):
                doc.add_paragraph(line[2:], style='List Bullet')
            else:
                doc.add_paragraph(line)
        
        out = BytesIO()
        doc.save(out)
        return out.getvalue()
    except Exception as e:
        print(f"âš  Word ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"Word ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

app = FastAPI(title="RAG Chatbot API", version="9.2.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


async def process_save_document_task(
    doc_name: str,
    content: str,
    collection: str,
    model: str,
    task_id: str
):
    """ë¬¸ì„œ ìˆ˜ì • ì €ì¥ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""
    start_time = time.time()
    update_task_status(task_id, "processing", f"'{doc_name}' ë¬¸ì„œ ìˆ˜ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    try:
        # 1. ë¬¸ì„œ ì¬ë¶„ì„ (íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©)
        content_bytes = content.encode('utf-8')
        model_path = resolve_model_path(model)
        embed_model = SentenceTransformer(model_path)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        result = await asyncio.to_thread(
            process_document,
            file_path=f"{doc_name}.md",
            content=content_bytes,
            use_llm_metadata=True, # ë©”íƒ€ë°ì´í„° ë° ë²„ì „ ì¶”ì¶œì„ ìœ„í•´ í™œì„±í™”
            embed_model=embed_model
        )
        
        if not result.get("success"):
            raise Exception(f"ğŸ”´ ë¶„ì„ ì‹¤íŒ¨: {result.get('errors')}")
            
        final_version = result.get("version", "1.0")
        chunks_data = result["chunks"]
        doc_id = result.get("doc_id", doc_name)
        
        update_task_status(task_id, "processing", f"ë¶„ì„ ì™„ë£Œ (ë²„ì „ {final_version}). DB ë™ê¸°í™” ì¤‘...", doc_id=doc_id)
        
        # 2. ê¸°ì¡´ ê²€ìƒ‰ ë°ì´í„° ì‚­ì œ (Overwrite ì •ì œ)
        await asyncio.to_thread(vector_store.delete_by_doc_name, doc_name, collection_name=collection)
        
        try:
            graph = get_graph_store()
            if graph and graph.test_connection():
                sop_id = doc_id
                if not re.search(r'[A-Z]+-[A-Z]+-\d+', sop_id):
                    sop_match = re.search(r'([A-Z]+-[A-Z]+-\d+)', doc_name, re.IGNORECASE)
                    if sop_match:
                        sop_id = sop_match.group(1).upper()
                
                await asyncio.to_thread(graph.delete_document, sop_id)
        except Exception as ge:
            print(f"  âš  Neo4j ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {ge}")

        # 3. RDB ì‹ ê·œ ë²„ì „ ì €ì¥
        doc_id_db = await asyncio.to_thread(
            sql_store.save_document,
            doc_name=doc_name,
            content=content,
            doc_type="text/markdown",
            version=final_version
        )
        
        if doc_id_db:
            batch_chunks = [
                {
                    "clause": c["metadata"].get("clause_id"),
                    "content": c["text"],
                    "metadata": c["metadata"]
                }
                for c in chunks_data
            ]
            await asyncio.to_thread(sql_store.save_chunks_batch, doc_id_db, batch_chunks)
        
        # 4. ë²¡í„° DB ì¬ì—…ë¡œë“œ
        texts = [c["text"] for c in chunks_data]
        metadatas = [
            {
                **c["metadata"],
                "chunk_method": "article",
                "model": model,
                "pipeline_version": "edit-save-v2.0",
            }
            for c in chunks_data
        ]
        
        await asyncio.to_thread(
            vector_store.add_documents,
            texts=texts,
            metadatas=metadatas,
            collection_name=collection,
            model_name=model_path
        )
        
        # 5. ê·¸ë˜í”„ DB ì¬ì—…ë¡œë“œ
        try:
            graph = get_graph_store()
            if graph and graph.test_connection():
                await asyncio.to_thread(_upload_to_neo4j_from_pipeline, graph, result, f"{doc_name}.md")
        except Exception as ge:
            print(f"  âš  Neo4j ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): {ge}")
            
        elapsed = round(time.time() - start_time, 2)
        update_task_status(task_id, "completed", f"ë¬¸ì„œ ìˆ˜ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({elapsed}ì´ˆ)", doc_name=doc_name, doc_id=doc_name, version=final_version)
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        update_task_status(task_id, "error", f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", doc_name=doc_name)

@app.post("/rag/document/save")
async def save_document_content(request: SaveDocRequest, background_tasks: BackgroundTasks):
    """
    ìˆ˜ì •ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì €ì¥í•˜ê³  DB ë™ê¸°í™” (ë¹„ë™ê¸°)
    """
    task_id = f"save_{uuid.uuid4().hex[:8]}"
    update_task_status(task_id, "waiting", f"'{request.doc_name}' ìˆ˜ì • ì €ì¥ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.", doc_name=request.doc_name)
    
    background_tasks.add_task(
        process_save_document_task,
        doc_name=request.doc_name,
        content=request.content,
        collection=request.collection,
        model=request.model,
        task_id=task_id
    )
    
    return {
        "success": True,
        "message": "ë¬¸ì„œ ìˆ˜ì •ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "task_id": task_id,
        "doc_name": request.doc_name
    }

@app.post("/rag/upload-docx")
async def upload_docx(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    doc_name: str = Form(...),
    collection: str = Form("documents"),
    model: str = Form("multilingual-e5-small"),
    version: str = Form("1.0")
):
    """
    DOCX íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ (ë¹„ë™ê¸°)
    """
    try:
        content = await file.read()
        filename = file.filename
        
        task_id = f"upload_docx_{uuid.uuid4().hex[:8]}"
        # doc_nameì„ filename ëŒ€ì‹  ì‚¬ìš©í•˜ì—¬ ì•Œë¦¼ì—ì„œ ë¬¸ì„œ IDê°€ ë³´ì´ê²Œ í•¨
        update_task_status(task_id, "waiting", f"'{doc_name}' ì—…ë¡œë“œ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.", doc_name=doc_name, filename=filename)

        # ê¸°ì¡´ ì—…ë¡œë“œ íƒœìŠ¤í¬ ì¬ì‚¬ìš© (DOCXë„ ë™ì¼ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ìœ ë„)
        background_tasks.add_task(
            process_upload_task,
            filename=doc_name, # íŒŒì´í”„ë¼ì¸ì—ì„œ doc_idë¡œ ì‚¬ìš©ë¨
            content=content,
            collection=collection,
            chunk_size=500, # ê¸°ë³¸ê°’
            chunk_method="article",
            model=model,
            overlap=50,
            use_langgraph=True,
            use_llm_metadata=True,
            task_id=task_id,
            version=version,
        )

        return {
            "success": True,
            "message": f"'{doc_name}' ë¬¸ì„œì˜ ì—…ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "task_id": task_id,
            "doc_name": doc_name
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ğŸ”´ DOCX ì—…ë¡œë“œ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model: str) -> str:
    """ëª¨ë¸ í”„ë¦¬ì…‹ â†’ ì „ì²´ ê²½ë¡œ"""
    return PRESET_MODELS.get(model, model)


def format_context(results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ â†’ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    context_parts = []
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        text = r.get("text", "")
        similarity = r.get("similarity", 0)
        
        #  v9.2: ê°œì„ ëœ ì¶œì²˜ í‘œì‹œ
        doc_id = meta.get("doc_id", "")
        section_path = meta.get("section_path", "")
        page = meta.get("page", "")
        article_num = meta.get("article_num", "")
        
        # ì¶œì²˜ í—¤ë” êµ¬ì„±
        source_parts = []
        if doc_id:
            source_parts.append(f"[{doc_id}]")
        if section_path:
            source_parts.append(f"> {section_path}")
        if page:
            source_parts.append(f"(p.{page})")
        if similarity:
            source_parts.append(f"ê´€ë ¨ë„: {similarity:.0%}")
        
        source_header = " ".join(source_parts) if source_parts else f"[ë¬¸ì„œ {i}]"
        
        context_parts.append(f"{source_header}\n{text}")
    
    return "\n\n---\n\n".join(context_parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG Chatbot API v9.2",
        "features": [
            "LangGraph íŒŒì´í”„ë¼ì¸",
            "í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì ",
            "Parent-Child ê³„ì¸µ",
            "Question ì¶”ì  (Neo4j)",
            "Weaviate + Neo4j ë™ê¸°í™” ì‚­ì œ"
        ],
        "endpoints": {
            "upload": "/rag/upload",
            "search": "/rag/search",
            "chat": "/chat",
            "ask": "/rag/ask",
            "graph": "/graph/*"
        },
        "langgraph_enabled": LANGGRAPH_AVAILABLE and USE_LANGGRAPH
    }


@app.get("/health")
def health():
    return {
        "status": "healthy",
        "cuda": torch.cuda.is_available(),
        "device": device,
        "ollama": OllamaLLM.is_available(),
        "langgraph": LANGGRAPH_AVAILABLE
    }


@app.get("/models/embedding")
def list_embedding_models():
    return {
        "presets": PRESET_MODELS,
        "specs": vector_store.EMBEDDING_MODEL_SPECS,
        "compatible": vector_store.filter_compatible_models()
    }


@app.get("/models/llm")
def list_llm_models():
    available_ollama = []
    if OllamaLLM.is_available():
        available_ollama = OllamaLLM.list_models()
    return {
        "ollama": {"presets": OLLAMA_MODELS, "available": available_ollama},
        "huggingface": HUGGINGFACE_MODELS
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - ì—…ë¡œë“œ (LangGraph v9.2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_upload_task(
    filename: str,
    content: bytes,
    collection: str,
    chunk_size: int,
    chunk_method: str,
    model: str,
    overlap: int,
    use_langgraph: bool,
    use_llm_metadata: bool,
    task_id: str,
    version: Optional[str] = None,
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ì²˜ë¦¬ (ë°°ê²½ ì‘ì—…)
    - ê¸°ì¡´ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """
    start_time = time.time()
    update_task_status(task_id, "processing", f"'{filename}' ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    try:
        print(f"\n{'='*70}", flush=True)
        print(f"ë¬¸ì„œ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹œì‘: {filename}", flush=True)
        print(f"{'='*70}\n", flush=True)

        # ========================================
        # ë¬¸ì„œ íŒŒì‹±
        # ========================================
        print(f"[1ë‹¨ê³„] ë¬¸ì„œ íŒŒì‹±", flush=True)
        print(f"  íŒŒì´í”„ë¼ì¸: PDF ì¡°í•­ v2.0", flush=True)
        print(f"  LLM ë©”íƒ€ë°ì´í„°: {'ğŸŸ¢ í™œì„±' if use_llm_metadata else 'ë¹„í™œì„±'}", flush=True)
        if use_llm_metadata:
            print(f"  LLM ëª¨ë¸: gpt-4o", flush=True)
        print("", flush=True)

        model_path = resolve_model_path(model)
        embed_model = SentenceTransformer(model_path)

        result = process_document(
            file_path=filename,
            content=content,
            use_llm_metadata=use_llm_metadata,
            embed_model=embed_model
        )

        if not result.get("success"):
            errors = result.get("errors", ["ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"])
            raise HTTPException(400, f"ğŸ”´ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {errors}")

        chunks_data = result["chunks"]
        if not chunks_data:
            raise HTTPException(400, "ğŸ”´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

        from dataclasses import dataclass
        @dataclass
        class Chunk:
            text: str
            metadata: dict
            index: int = 0

        chunks = [Chunk(text=c["text"], metadata=c["metadata"], index=c["index"]) for c in chunks_data]
        doc_id = result.get("doc_id")
        doc_title = result.get("doc_title")
        pipeline_version = "pdf-clause-v2.0"

        print(f"  ğŸŸ¢ íŒŒì‹± ì™„ë£Œ", flush=True)
        print(f"     â€¢ ID: {doc_id}", flush=True)
        print(f"     â€¢ ì œëª©: {doc_title}", flush=True)
        print(f"     â€¢ ì¡°í•­: {result.get('total_clauses')}ê°œ", flush=True)
        print(f"     â€¢ ì²­í¬: {len(chunks)}ê°œ\n", flush=True)
        
        update_task_status(task_id, "processing", f"íŒŒì‹± ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬). ë²¡í„° DB ì €ì¥ ì¤‘...", doc_id=doc_id)
        
        # ========================================
        # Weaviate ë²¡í„° ì €ì¥
        # ========================================
        print(f"[2ë‹¨ê³„] Weaviate ë²¡í„° ì €ì¥", flush=True)

        texts = [c.text for c in chunks]
        metadatas = [
            {
                **c.metadata,
                "chunk_method": chunk_method,
                "model": model,
                "pipeline_version": pipeline_version,
            }
            for c in chunks
        ]

        vector_store.add_documents(
            texts=texts,
            metadatas=metadatas,
            collection_name=collection,
            model_name=model_path
        )
        print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬\n", flush=True)
        update_task_status(task_id, "processing", "ë²¡í„° DB ì €ì¥ ì™„ë£Œ. PostgreSQL ë° ê·¸ë˜í”„ DB ì €ì¥ ì¤‘...", doc_id=doc_id)
        
        # ========================================
        # PostgreSQL ë¬¸ì„œ ì €ì¥
        # ========================================
        print(f"[3ë‹¨ê³„] PostgreSQL ì €ì¥", flush=True)

        try:
            # PDFì—ì„œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¡°í•­ ë²ˆí˜¸ í¬í•¨)
            original_text = result.get("markdown", "")
            if not original_text:
                # fallback: ì²­í¬ë“¤ì„ í•©ì¹¨
                original_text = "\n\n".join([c.text for c in chunks])

            # íŒŒì´í”„ë¼ì¸ì—ì„œ ì¶”ì¶œëœ ë²„ì „ ë˜ëŠ” ì‚¬ìš©ì ì…ë ¥ ë²„ì „ ê²°ì •
            final_version = version or result.get("version", "1.0")
            
            if final_version != "1.0":
                print(f"     [ì¶”ì¶œ] ìµœì¢… ê²°ì •ëœ ë²„ì „: {final_version}", flush=True)

            doc_id_db = sql_store.save_document(
                doc_name=doc_id,
                content=original_text,  # PDF ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥
                doc_type=filename.split('.')[-1] if '.' in filename else None,
                version=final_version
            )

            # ì›ë³¸ PDFë¥¼ S3ì— ì €ì¥
            if filename.lower().endswith('.pdf'):
                try:
                    get_s3_store().upload_pdf(doc_id, final_version, content)
                    print(f"  ğŸŸ¢ ì›ë³¸ PDF S3 ì €ì¥ ì™„ë£Œ: {doc_id}/v{final_version}", flush=True)
                except Exception as e:
                    print(f"  ğŸŸ¡ S3 PDF ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}", flush=True)

            if doc_id_db and chunks:
                batch_chunks = [
                    {
                        "clause": c.metadata.get("clause_id"),
                        "content": c.text,
                        "metadata": c.metadata
                    }
                    for c in chunks
                ]
                sql_store.save_chunks_batch(doc_id_db, batch_chunks)
                print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: ë¬¸ì„œ + {len(chunks)}ê°œ ì²­í¬\n", flush=True)
            else:
                print(f"  ğŸ”´ ì €ì¥ ì‹¤íŒ¨: DB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ID ìƒì„± ë¶ˆê°€)\n", flush=True)
        except Exception as sql_err:
            print(f"  ğŸ”´ ì €ì¥ ì‹¤íŒ¨: {sql_err}\n", flush=True)

        # ========================================
        # Neo4j ê·¸ë˜í”„ ì €ì¥
        # ========================================
        print(f"[4ë‹¨ê³„] Neo4j ê·¸ë˜í”„ ì €ì¥", flush=True)
        graph_uploaded = False
        graph_sections = 0

        try:
            from backend.graph_store import Neo4jGraphStore

            graph = get_graph_store()
            if graph.test_connection():
                _upload_to_neo4j_from_pipeline(graph, result, filename)
                graph_uploaded = True
                stats = graph.get_graph_stats()
                graph_sections = stats.get("sections", 0)
                print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: {graph_sections}ê°œ ì„¹ì…˜\n", flush=True)
        except Exception as graph_error:
            # [ë””ë²„ê·¸ ë¡œê·¸ ë³´ê°•] ì—°ê²° ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
            print(f"  ğŸ”´ Neo4j ì—°ê²° ì‹¤íŒ¨: {graph_error}", flush=True)
            import traceback
            traceback.print_exc()
            print(f"  âš  ê·¸ë˜í”„ ì—°ë™ì„ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n", flush=True)
        
        # ì™„ë£Œ
        # ========================================
        elapsed = round(time.time() - start_time, 2)
        update_task_status(task_id, "completed", f"ë¬¸ì„œ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({elapsed}ì´ˆ)", doc_id=doc_id, doc_name=doc_id, filename=filename, version=final_version)

        print(f"{'='*70}", flush=True)
        print(f"ğŸŸ¢ ì—…ë¡œë“œ ì²˜ë¦¬ ì™„ë£Œ ({elapsed}ì´ˆ)", flush=True)
        print(f"{'='*70}\n", flush=True)

    except HTTPException as e:
        print(f"ğŸ”´ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e.detail}", flush=True)
        update_task_status(task_id, "error", f"ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e.detail}", filename=filename)
        return
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"ğŸ”´ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", flush=True)
        update_task_status(task_id, "error", f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {str(e)}", filename=filename)
        return


@app.post("/rag/upload")
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    use_langgraph: bool = Form(True),
    use_llm_metadata: bool = Form(True),
    version: Optional[str] = Form(None),
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ìš”ì²­ì„ ì¦‰ì‹œ ì ‘ìˆ˜í•˜ê³ , ì‹¤ì œ ì²˜ë¦¬ëŠ” ë°°ê²½ ì‘ì—…ìœ¼ë¡œ ìˆ˜í–‰.
    """
    try:
        content = await file.read()
        filename = file.filename

        print(f"\n{'='*70}", flush=True)
        print(f"ë¬¸ì„œ ì—…ë¡œë“œ ìš”ì²­ ì ‘ìˆ˜: {filename}", flush=True)
        print("  ì²˜ë¦¬ ë°©ì‹: ë¹„ë™ê¸° (Background Tasks)", flush=True)
        print(f"{'='*70}\n", flush=True)

        task_id = f"upload_{uuid.uuid4().hex[:8]}"
        update_task_status(task_id, "waiting", f"'{filename}' ì—…ë¡œë“œ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.", filename=filename)

        background_tasks.add_task(
            process_upload_task,
            filename=filename,
            content=content,
            collection=collection,
            chunk_size=chunk_size,
            chunk_method=chunk_method,
            model=model,
            overlap=overlap,
            use_langgraph=use_langgraph,
            use_llm_metadata=use_llm_metadata,
            task_id=task_id,
            version=version,
        )

        return {
            "success": True,
            "message": f"'{filename}' ë¬¸ì„œì˜ ì—…ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "task_id": task_id,
            "filename": filename,
            "processing_mode": "background",
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ğŸ”´ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")


def _upload_to_neo4j_from_pipeline(graph, result: dict, filename: str):
    """ìƒˆ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ Neo4jì— ì—…ë¡œë“œ (ê°„ì†Œí™”)"""
    from backend.graph_store import upload_document_to_graph
    upload_document_to_graph(graph, result, filename)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# /rag/search ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨ (Agentê°€ ë‚´ë¶€ ìˆ˜í–‰)


# /rag/search/advanced ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì±—ë´‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def process_chat_request(request: ChatRequest) -> Dict:
    """ì‹¤ì œ ì—ì´ì „íŠ¸ ì§ˆì˜ ì²˜ë¦¬ ë³¸ë¬¸ (ì›Œì»¤/ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ ê³µìš©)."""
    print(f" [Agent] ìš”ì²­ ìˆ˜ì‹ : {request.message}")

    # Agent ì´ˆê¸°í™”
    init_agent_tools(vector_store, get_graph_store(), sql_store)

    # ì„¸ì…˜ ID ì„ í• ë‹¹ (None ë°©ì§€)
    session_id = request.session_id or str(uuid.uuid4())

    # ë¡±í…€ ë©”ëª¨ë¦¬: ì´ì „ ëŒ€í™” ê¸°ë¡ + ìœ ì‚¬ ê¸°ì–µ ë¡œë“œ
    chat_history = []
    user_id = request.user_id
    query_embedding = None

    if user_id:
        try:
            if request.session_id:
                chat_history = sql_store.get_conversation_history_by_session(
                    user_id,
                    request.session_id,
                    limit=100
                )
                print(f"  ğŸ§  [Memory] ì‚¬ìš©ì {user_id}, ì„¸ì…˜ {request.session_id} ê¸°ë¡ {len(chat_history)}ê±´ ë¡œë“œ")
            else:
                chat_history = sql_store.get_conversation_history(user_id, limit=10)
                print(f"  ğŸ§  [Memory] ì‚¬ìš©ì {user_id} ìµœì‹  ê¸°ë¡ {len(chat_history)}ê±´ ë¡œë“œ (ì„¸ì…˜ ë¯¸ì§€ì •)")

            print(f"  ğŸ§  [Memory] ì‚¬ìš©ì {user_id} ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ [Memory] ì¡°íšŒ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    effective_model = resolve_effective_llm_model(request.llm_model)
    print(f" [LLM] effective_model={effective_model} (requested={request.llm_model})")

    response = await asyncio.to_thread(
        run_agent,
        query=request.message,
        session_id=session_id,
        model_name=effective_model,
        chat_history=chat_history
    )

    answer = response.get("answer") or ""

    # ë¡±í…€ ë©”ëª¨ë¦¬: ìƒˆë¡œìš´ ëŒ€í™” ì €ì¥ (ì„¸ì…˜ ê¸°ì¤€)
    if user_id and answer:
        try:
            target_session_id = session_id

            query_embedding = None
            try:
                # ë¬´ê±°ìš´ ëª¨ë¸ ë¡œë“œëŠ” ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                def _get_embedding():
                    embed_model = SentenceTransformer("intfloat/multilingual-e5-small")
                    return embed_model.encode(request.message).tolist()
                
                query_embedding = await asyncio.to_thread(_get_embedding)
            except Exception as embed_error:
                print(f"  âš ï¸ [Memory] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {embed_error}")

            await asyncio.to_thread(
                sql_store.save_memory,
                request.message,
                answer,
                user_id,
                target_session_id,
                embedding=query_embedding
            )
            print(f"  ğŸ’¾ [Memory] ì„¸ì…˜ {target_session_id}ì— ëŒ€í™” ë‚´ìš© ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ [Memory] ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    # LLM as a Judge í‰ê°€
    evaluation_scores = None
    error_patterns = ["ì˜¤ë¥˜ê°€ ë°œìƒ", "ì—ëŸ¬", "ì‹¤íŒ¨", "Error", "Exception", "ì°¾ì„ ìˆ˜ ì—†", "ì¤€ë¹„í•˜ì§€ ëª»", "ë¡œë”© ì—ëŸ¬"]
    is_error_message = any(pattern in answer for pattern in error_patterns)

    try:
        from backend.evaluation import AgentEvaluator
        if len(answer) >= 20 and not is_error_message:
            # í‰ê°€ê¸°(AgentEvaluator)ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë™ê¸° í†µì‹ ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            def _evaluate():
                evaluator = AgentEvaluator(judge_model="gpt-4o", sql_store=sql_store)
                context = response.get("agent_log", {}).get("context", "")
                if isinstance(context, list):
                    context = "\n\n".join(context)

                return evaluator.evaluate_single(
                    question=request.message,
                    answer=answer,
                    context=context,
                    metrics=["faithfulness", "groundness", "relevancy", "correctness"]
                )
            evaluation_scores = await asyncio.to_thread(_evaluate)
    except ImportError:
        print("í‰ê°€ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ (ì„ íƒì  ê¸°ëŠ¥)")
    except Exception as eval_error:
        print(f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {eval_error}")
        evaluation_scores = None

    return {
        "session_id": session_id,
        "answer": answer,
        "sources": [],
        "agent_log": response,
        "evaluation_scores": evaluation_scores
    }


async def chat_worker():
    """íì—ì„œ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© êº¼ë‚´ì–´ ìˆœì°¨ì ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì›Œì»¤."""
    while True:
        request_id, ticket, kind, payload = await chat_queue.get()
        try:
            print(f" ğŸš€ [Chat Worker] ì²˜ë¦¬ ì‹œì‘: {request_id} (ticket: {ticket}, kind: {kind})")
            
            load_queue_state()
            if request_id in chat_results:
                chat_results[request_id]["status"] = "processing"
            save_queue_state()

            if kind == "rag":
                req = ChatRequest(**payload)
                result_data = await process_chat_request(req)

            elif kind == "agent":
                req = AgentRequest(**payload)
                # ì—ì´ì „íŠ¸ ë„êµ¬ ì´ˆê¸°í™” (ë™ì‘ í™•ì¸ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘ , thread ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ ê¶Œì¥)
                from backend.agent import init_agent_tools, run_agent
                
                # ë¹„ì°¨ë‹¨/ë©€í‹°ìŠ¤ë ˆë“œ í˜¸ì¶œë¡œ ë³€ê²½ (ë™ê¸° í•¨ìˆ˜ì¸ run_agentê°€ ë£¨í”„ë¥¼ ë§‰ì§€ ì•Šê²Œ í•¨)
                result = await asyncio.to_thread(
                    run_agent,
                    query=req.message,
                    session_id=req.session_id,
                    model_name=req.llm_model,
                    embedding_model=resolve_model_path(req.embedding_model)
                )

                answer = result.get("answer", "")
                reasoning = result.get("reasoning", "")
                
                # ë³¸ë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ì‚¬ê³  ê³¼ì •(Think)ì„ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš© (ì „ì²˜ë¦¬ ë¡œì§ ë™ì¼í™”)
                if not answer and reasoning:
                    answer = f"[AI ë¶„ì„ ë¦¬í¬íŠ¸]\n\n{reasoning}"

                result_data = {
                    "session_id": req.session_id,
                    "answer": answer,
                    "reasoning": reasoning,
                    "tool_calls": result.get("tool_calls", []),
                    "success": result.get("success", False),
                    "mode": "langgraph" if (req.use_langgraph and LANGGRAPH_AGENT_AVAILABLE) else "simple"
                }
            else:
                raise ValueError(f"Unknown kind: {kind}")
            
            load_queue_state()
            if request_id in chat_results:
                chat_results[request_id].update({
                    "status": "completed",
                    "result": result_data
                })
            save_queue_state()
            
            print(f" âœ… [Chat Worker] ì²˜ë¦¬ ì™„ë£Œ: {request_id}")
        except Exception as e:
            print(f" ğŸ”´ [Chat Worker] ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            load_queue_state()
            if request_id in chat_results:
                chat_results[request_id].update({
                    "status": "error",
                    "error": str(e)
                })
            save_queue_state()
        finally:
            load_queue_state()
            # pendingì—ì„œ ì œê±°(request_id ê¸°ì¤€)
            # pendingì—ì„œ ì œê±°(request_id ê¸°ì¤€) - ì¸ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸ ê°ì²´ ë³´ì¡´ì„ ìœ„í•´ ìŠ¬ë¼ì´ìŠ¤ í• ë‹¹ ì‚¬ìš©
            chat_pending[:] = [x for x in chat_pending if x["request_id"] != request_id]
            save_queue_state()
            chat_queue.task_done()

def _extract_user_id_from_auth_header(auth_header: Optional[str]) -> Optional[int]:
    """Authorization í—¤ë”ì˜ JWTì—ì„œ user_idë¥¼ ì¶”ì¶œ."""
    if not auth_header or not auth_header.startswith("Bearer "):
        return None
    token = auth_header.split(" ", 1)[1].strip()
    if not token:
        return None
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id = payload.get("user_id")
        return int(user_id) if user_id is not None else None
    except Exception:
        return None


async def enqueue_job(kind: str, payload: dict) -> dict:
    """
    kind: "rag" | "agent"
    payload: workerê°€ ì²˜ë¦¬í•  ìš”ì²­ ë°ì´í„°(ì§ë ¬í™” ê°€ëŠ¥í•œ dict)
    """
    global next_ticket, chat_pending

    request_id = str(uuid.uuid4())

    async with ticket_lock:
        next_ticket += 1
        ticket = next_ticket

    load_queue_state()

    # ëŒ€ê¸°ì—´ ì—”íŠ¸ë¦¬ ì¶”ê°€
    chat_pending.append({"request_id": request_id, "ticket": ticket, "kind": kind})

    # ìƒíƒœ ì €ì¥(í´ë¼ì´ì–¸íŠ¸ status ì¡°íšŒìš©)
    chat_results[request_id] = {
        "status": "waiting",
        "ticket": ticket,
        "kind": kind,
        "result": None,
        "request": payload,
    }
    save_queue_state()

    await chat_queue.put((request_id, ticket, kind, payload))

    # position(í˜„ì¬ ëŒ€ê¸° ìˆœì„œ) = pendingì—ì„œ ticket ì‘ì€ ì• ë“¤ ìˆ˜ + 1
    position = sum(1 for x in chat_pending if x["ticket"] < ticket) + 1

    return {
        "success": True,
        "request_id": request_id,
        "ticket": ticket,
        "status": "waiting",
        "position": position,
        "message": f"ì§ˆë¬¸ì´ ëŒ€ê¸°ì—´ì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. (í˜„ì¬ ëŒ€ê¸° ìˆœë²ˆ: {position}ë²ˆì§¸)",
    }


@app.post("/chat")
async def chat(chat_request: ChatRequest, http_request: Request):
    """ìˆœì°¨ ëŒ€ê¸°ì—´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸."""
    session_id = chat_request.session_id or str(uuid.uuid4())
    user_id = chat_request.user_id
    if user_id is None:
        auth_header = http_request.headers.get("Authorization")
        user_id = _extract_user_id_from_auth_header(auth_header)

    effective_model = resolve_effective_llm_model(chat_request.llm_model)
    queued_request = chat_request.model_copy(update={
        "session_id": session_id,
        "user_id": user_id,
        "llm_model": effective_model
    })

    # í•µì‹¬: í ë“±ë¡ì€ ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©
    return await enqueue_job(
        kind="rag",
        payload=queued_request.model_dump()
    )


@app.get("/chat/status/{request_id}")
async def get_chat_status(request_id: str):
    """ì±„íŒ… ì‘ì—…ì˜ ìƒíƒœì™€ ëŒ€ê¸° ìˆœë²ˆ ì¡°íšŒ."""
    load_queue_state()
    
    if request_id not in chat_results:
        raise HTTPException(404, "ìš”ì²­ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status_data = chat_results[request_id].copy()

    # ticket ìƒì‹œ í¬í•¨ ë³´ì¥ (ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ)
    # statusê°€ 'waiting'ì¼ ë•Œë§Œ position ê³„ì‚°í•˜ì—¬ ë…¸ì¶œ
    if status_data.get("status") == "waiting":
        mine = next((x for x in chat_pending if x["request_id"] == request_id), None)
        if mine:
            my_ticket = mine["ticket"]
            # position(í˜„ì¬ ëŒ€ê¸° ìˆœì„œ) = pendingì—ì„œ ë‚´ í‹°ì¼“ë³´ë‹¤ ì‘ì€ ì• ë“¤ ìˆ˜ + 1
            status_data["position"] = sum(1 for x in chat_pending if x["ticket"] < my_ticket) + 1
        else:
            status_data.pop("position", None)
    else:
        # processing, completed ë“±ì—ì„œëŠ” position ì œì™¸
        status_data.pop("position", None)

    return status_data


@app.get("/chat/history/{session_id}")
def get_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = chat_histories.get(session_id, [])
    return {"session_id": session_id, "history": history, "count": len(history)}


@app.delete("/chat/history/{session_id}")
def clear_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    if session_id in chat_histories:
        del chat_histories[session_id]
        return {"success": True, "message": f"ì„¸ì…˜ {session_id} ì‚­ì œë¨"}
    return {"success": False, "message": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/llm/generate")
def generate_llm(request: LLMRequest):
    """LLM ì§ì ‘ í˜¸ì¶œ"""
    try:
        response = get_llm_response(
            prompt=request.prompt,
            llm_model=request.model,
            llm_backend=request.backend,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        return {"response": response, "model": request.model, "backend": request.backend}
    except Exception as e:
        raise HTTPException(500, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_document_category(doc_id: str) -> str:
    """ë¬¸ì„œ IDì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (SOP, WI, FRM)"""
    if not doc_id:
        return "ê¸°íƒ€"
    doc_id_upper = doc_id.upper()
    if "SOP" in doc_id_upper:
        return "SOP"
    elif "WI" in doc_id_upper:
        return "WI"
    elif "FRM" in doc_id_upper or "FORM" in doc_id_upper:
        return "FRM"
    else:
        return "ê¸°íƒ€"


@app.get("/rag/documents")
async def list_documents(collection: str = "documents"):
    """ë¬¸ì„œ ëª©ë¡ (RDB + S3 DOCX ë³‘í•© ì¡°íšŒ)"""
    try:
        # SQL Storeì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ (ë¹„ë™ê¸° ì²˜ë¦¬)
        all_docs = await asyncio.to_thread(sql_store.get_all_documents)

        def _normalize_doc_type(v: Optional[str]) -> str:
            t = (v or "").lower()
            if "pdf" in t:
                return "pdf"
            if "docx" in t:
                return "docx"
            return t or "other"

        # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ ë¬¸ì„œì˜ ì—¬ëŸ¬ ë²„ì „)
        grouped: Dict[str, List[Dict]] = {}
        for doc in all_docs:
            doc_name = doc.get("doc_name")
            if not doc_name:
                continue
            grouped.setdefault(doc_name, []).append(doc)

        docs_out: List[Dict] = []
        for doc_name, versions in grouped.items():
            # ë™ì¼ ë¬¸ì„œëª…ì—ì„œ pdfê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ pdf ê³„ì—´ë§Œ ëŒ€í‘œ í›„ë³´ë¡œ ì‚¬ìš©
            pdf_versions = [d for d in versions if _normalize_doc_type(d.get("doc_type")) == "pdf"]
            candidates = pdf_versions if pdf_versions else versions

            def _sort_key(d: Dict):
                created = d.get("created_at")
                if hasattr(created, "timestamp"):
                    created_key = created.timestamp()
                else:
                    created_key = 0
                return (created_key, str(d.get("version") or "0"))

            # ëŒ€í‘œ ë¬¸ì„œëŠ” created_at ìµœì‹  ìš°ì„  (ì—†ìœ¼ë©´ version ë¬¸ìì—´ë¡œ ë³´ì¡°)
            selected = max(candidates, key=_sort_key)

            docs_out.append({
                "doc_id": doc_name,
                "doc_name": doc_name,
                "doc_type": selected.get("doc_type"),
                "doc_format": _normalize_doc_type(selected.get("doc_type")),
                "doc_category": extract_document_category(doc_name),
                "version": selected.get("version"),
                "created_at": selected.get("created_at"),
                "latest_version": selected.get("version"),
                "source": "rdb",
            })

        # S3 DOCX ë¬¸ì„œ ë³‘í•©: ë™ì¼ doc_nameì´ ìˆì–´ë„ ìˆ¨ê¸°ì§€ ì•Šê³  í‘œì‹œ
        try:
            s3_docs = await asyncio.to_thread(get_s3_store().list_docx_documents)
            for s3_doc in s3_docs:
                doc_name = s3_doc.get("doc_name")
                docs_out.append({
                    "doc_id": doc_name,
                    "doc_name": doc_name,
                    "doc_type": "docx",
                    "doc_format": "docx",
                    "doc_category": extract_document_category(doc_name),
                    "version": s3_doc.get("version"),
                    "created_at": s3_doc.get("created_at"),
                    "latest_version": s3_doc.get("latest_version"),
                    "source": "s3",
                })
        except Exception as s3_err:
            print(f"S3 DOCX ëª©ë¡ ë³‘í•© ì‹¤íŒ¨(ë¬´ì‹œ): {s3_err}")

        return {"documents": docs_out, "collection": collection}
    except Exception as e:
        print(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"documents": [], "collection": collection}


@app.get("/rag/doc-names")
async def list_doc_names():
    """ëª¨ë“  ë¬¸ì„œ ì´ë¦„ ëª©ë¡ ì¡°íšŒ (RDB doc_name í…Œì´ë¸”)"""
    try:
        doc_names = await asyncio.to_thread(sql_store.list_doc_names)
        return {"doc_names": doc_names}
    except Exception as e:
        print(f"ë¬¸ì„œ ì´ë¦„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"doc_names": []}


@app.get("/rag/document/{doc_name}/pdf-url")
def get_pdf_presigned_url(doc_name: str, version: Optional[str] = None):
    """PDF ì—´ëŒ URL ë°˜í™˜ - S3 presigned URL ìš°ì„ , ì—†ìœ¼ë©´ download ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©"""
    try:
        # ë²„ì „ì´ ì—†ìœ¼ë©´ ìµœì‹  ë²„ì „ ì‚¬ìš©
        if not version:
            versions = sql_store.get_document_versions(doc_name)
            if not versions:
                raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name}")
            version = versions[0]['version']

        # S3ì—ì„œ ì›ë³¸ PDF ì‹œë„
        try:
            store = get_s3_store()
            if store.pdf_exists(doc_name, version):
                url = store.get_pdf_presigned_url(doc_name, version)
                return {"url": url, "source": "s3", "doc_name": doc_name, "version": version}
        except Exception as s3_err:
            print(f"  S3 PDF ì¡°íšŒ ì‹¤íŒ¨ (downloadë¡œ í´ë°±): {s3_err}")

        # í´ë°±: ë°±ì—”ë“œ download ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (í”„ë¡ íŠ¸ì—ì„œ auth í—¤ë” í•„ìš”)
        return {"url": None, "source": "download", "doc_name": doc_name, "version": version}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"PDF URL ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/document/{doc_name}/versions")
async def get_document_versions(doc_name: str):
    """ë¬¸ì„œ ë²„ì „ ëª©ë¡ ì¡°íšŒ"""
    try:
        versions = await asyncio.to_thread(sql_store.get_document_versions, doc_name)
        if not versions:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name}")
        return {
            "doc_name": doc_name,
            "versions": versions,
            "count": len(versions)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë²„ì „ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/document/{doc_name}/compare")
async def compare_versions(doc_name: str, v1: str, v2: str):
    """ë‘ ë²„ì „ ê°„ì˜ ì¡°í•­ ë‹¨ìœ„ ì°¨ì´ ë¹„êµ"""
    try:
        diffs = await asyncio.to_thread(sql_store.get_clause_diff, doc_name, v1, v2)

        # ì—ëŸ¬ê°€ ìˆëŠ”ì§€ í™•ì¸
        if diffs and isinstance(diffs[0], dict) and 'error' in diffs[0]:
            raise HTTPException(400, diffs[0]['error'])

        return {
            "doc_name": doc_name,
            "v1": v1,
            "v2": v2,
            "diffs": diffs,
            "total_changes": len(diffs)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë²„ì „ ë¹„êµ ì‹¤íŒ¨: {str(e)}")


@app.get("/processing/status/{task_id}")
async def get_processing_status(task_id: str):
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    if task_id not in processing_tasks:
        raise HTTPException(404, "ì‘ì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return processing_tasks[task_id]

@app.get("/processing/list")
async def list_processing_tasks():
    """í˜„ì¬ ê´€ë¦¬ ì¤‘ì¸ ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ëª©ë¡"""
    return list(processing_tasks.values())


@app.get("/rag/changes")
async def get_changes(limit: int = 50):
    """ìµœê·¼ ë¬¸ì„œ ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
    try:
        # SQLStoreì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ ìµœê·¼ ìˆ˜ì •ìˆœìœ¼ë¡œ ë°˜í™˜ (ë¹„ë™ê¸° ì²˜ë¦¬)
        all_docs = await asyncio.to_thread(sql_store.get_all_documents)
        changes = []
        for doc in all_docs[:limit]:
            changes.append({
                "id": str(doc.get('id')),
                "doc_id": doc.get('doc_name'),
                "change_type": "UPDATE" if doc.get('version') != "1.0" else "CREATE",
                "changed_at": doc.get('created_at').isoformat() if doc.get('created_at') else None,
                "changed_by": "System",
                "description": f"Version {doc.get('version')} saved."
            })
        return {"changes": changes, "count": len(changes)}
    except Exception as e:
        raise HTTPException(500, f"ë³€ê²½ ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/document/{doc_name}/content")
async def get_document_content(doc_name: str, version: Optional[str] = None):
    """ë¬¸ì„œ ì „ì²´ ë‚´ìš© ì¡°íšŒ"""
    try:
        doc = await asyncio.to_thread(sql_store.get_document_by_name, doc_name, version)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name} (v{version or 'ìµœì‹ '})")

        # ì²­í¬ ì¡°íšŒ
        chunks = await asyncio.to_thread(sql_store.get_chunks_by_document, doc['id'])

        return {
            "doc_name": doc_name,
            "version": doc.get('version', '1.0'),
            "doc_type": doc.get('doc_type'),
            "created_at": doc.get('created_at'),
            "content": doc.get('content', ''),
            "chunks": chunks,
            "chunk_count": len(chunks)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/document/{doc_name}/metadata")
async def get_document_metadata(doc_name: str, version: Optional[str] = None):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    try:
        doc = await asyncio.to_thread(sql_store.get_document_by_name, doc_name, version)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name}")

        # ì²­í¬ í†µê³„
        chunks = await asyncio.to_thread(sql_store.get_chunks_by_document, doc['id'])

        # ì¡°í•­ í†µê³„ (ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ)
        clause_ids = set()
        for chunk in chunks:
            metadata = chunk.get('metadata', {})
            if isinstance(metadata, dict):
                clause_id = metadata.get('clause_id')
                if clause_id:
                    clause_ids.add(clause_id)

        return {
            "doc_name": doc_name,
            "version": doc.get('version', '1.0'),
            "doc_type": doc.get('doc_type'),
            "created_at": doc.get('created_at'),
            "chunk_count": len(chunks),
            "clause_count": len(clause_ids),
            "total_length": len(doc.get('content', '')),
            "clauses": sorted(list(clause_ids), key=lambda x: [int(n) if n.isdigit() else n for n in x.split('.')])
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/document/download/{doc_name}")
async def download_document(
    doc_name: str, 
    version: Optional[str] = None,
    format: Literal["pdf", "docx", "md"] = "pdf"
):
    """ë¬¸ì„œë¥¼ ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
    try:
        # DBì—ì„œ ë¬¸ì„œ ì¡°íšŒ (ë²„ì „ ëª…ì‹œ ì—†ìœ¼ë©´ ìµœì‹ ë³¸)
        doc = await asyncio.to_thread(sql_store.get_document_by_name, doc_name, version)
        if not doc:
            raise HTTPException(404, "ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        content = doc["content"] # text/markdown ì›ë³¸
        doc_type = doc["doc_type"]
        ver = doc["version"]
        
        # íŒŒì¼ëª… êµ¬ì„± (ê³µë°± ì œê±°)
        safe_filename = doc_name.replace(" ", "_").replace("/", "_")
        
        # 1. ë§ˆí¬ë‹¤ìš´(.md) í˜•ì‹ ìš”ì²­
        if format == "md":
            filename = f"{safe_filename}_v{ver}.md"
            return Response(
                content=content.encode('utf-8'),
                media_type="text/markdown",
                headers={"Content-Disposition": f"attachment; filename={filename}"}
            )
            
        # 2. ì›Œë“œ(.docx) í˜•ì‹ ìš”ì²­
        if format == "docx":
            filename = f"{safe_filename}_v{ver}.docx"
            docx_bytes = await asyncio.to_thread(md_to_docx_binary, content, doc_name)
            return Response(
                content=docx_bytes,
                media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                headers={"Content-Disposition": f"attachment; filename={filename}"}
            )
            
        # 3. PDF í˜•ì‹ ìš”ì²­ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        filename = f"{safe_filename}_v{ver}.pdf"
        
        # 3-1. ì´ë¯¸ PDFì¸ ê²½ìš°
        if doc_type == "pdf" and isinstance(content, bytes):
            return Response(
                content=content,
                media_type="application/pdf",
                headers={"Content-Disposition": f"attachment; filename={filename}"}
            )
            
        # 3-2. í…ìŠ¤íŠ¸(ë§ˆí¬ë‹¤ìš´)ì¸ ê²½ìš° PDFë¡œ ë³€í™˜
        pdf_bytes = await asyncio.to_thread(md_to_pdf_binary, content)
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """
    ë¬¸ì„œ ì‚­ì œ (RDB + Weaviate + Neo4j ì „ì²´ ì‚­ì œ + ì‚­ì œ ê²€ì¦)
    """
    print(f"\n[DELETE] ë‹¨ê±´ ì‚­ì œ ìš”ì²­: doc={request.doc_name}, collection={request.collection}, neo4j={request.delete_from_neo4j}")
    result = _delete_document_everywhere(
        doc_name=request.doc_name,
        collection=request.collection,
        delete_from_neo4j=request.delete_from_neo4j,
    )
    print(f"[DELETE] ë‹¨ê±´ ì‚­ì œ ê²°ê³¼: doc={request.doc_name}, success={result.get('success')}, details={result.get('details')}")
    return {
        "success": result.get("success", False),
        "doc_name": request.doc_name,
        "details": result.get("details", {}),
    }


@app.post("/rag/documents/delete-batch")
def delete_documents_batch(request: DeleteDocsBatchRequest):
    """
    ë¬¸ì„œ ë‹¤ê±´ ì‚­ì œ (RDB + Weaviate + Neo4j ì „ì²´ ì‚­ì œ + ì‚­ì œ ê²€ì¦)
    - í•˜ë‚˜ë¼ë„ ì‚­ì œ ê²€ì¦ì— ì‹¤íŒ¨í•˜ë©´ overall_success=False
    """
    doc_names = [d.strip() for d in request.doc_names if d and d.strip()]
    if not doc_names:
        raise HTTPException(400, "ì‚­ì œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    unique_doc_names = list(dict.fromkeys(doc_names))
    print(f"\n[DELETE] ë°°ì¹˜ ì‚­ì œ ìš”ì²­: count={len(unique_doc_names)}, docs={unique_doc_names}, collection={request.collection}, neo4j={request.delete_from_neo4j}")
    results = []
    success_count = 0

    for doc_name in unique_doc_names:
        item = _delete_document_everywhere(
            doc_name=doc_name,
            collection=request.collection,
            delete_from_neo4j=request.delete_from_neo4j,
        )
        results.append({
            "doc_name": doc_name,
            **item,
        })
        if item.get("success"):
            success_count += 1
        else:
            print(f"[DELETE] ë°°ì¹˜ í•­ëª© ì‹¤íŒ¨: doc={doc_name}, details={item.get('details')}")

    print(f"[DELETE] ë°°ì¹˜ ì‚­ì œ ê²°ê³¼: success={success_count}/{len(unique_doc_names)}")

    return {
        "success": success_count == len(unique_doc_names),
        "requested_count": len(unique_doc_names),
        "deleted_count": success_count,
        "failed_count": len(unique_doc_names) - success_count,
        "results": results,
    }


def _delete_document_everywhere(doc_name: str, collection: str, delete_from_neo4j: bool = True) -> Dict:
    """ë¬¸ì„œ 1ê±´ì„ ì„¸ ì €ì¥ì†Œì—ì„œ ì‚­ì œí•˜ê³  ì‹¤ì œ ì‚­ì œ ì—¬ë¶€ë¥¼ ê²€ì¦í•œë‹¤."""
    result = {"rdb": None, "weaviate": None, "neo4j": None, "s3_docx": None}
    print(f"[DELETE] ì²˜ë¦¬ ì‹œì‘: doc={doc_name}")
    doc_id_match = re.search(r'(EQ-(?:SOP|WI|FRM)-\d+)', doc_name, re.IGNORECASE)
    normalized_doc_id = doc_id_match.group(1).upper() if doc_id_match else None
    candidate_names = [doc_name]
    if normalized_doc_id and normalized_doc_id not in candidate_names:
        candidate_names.insert(0, normalized_doc_id)
    print(f"[DELETE] í›„ë³´ í‚¤: {candidate_names}")

    # 1) RDB ì‚­ì œ + ê²€ì¦
    try:
        for name in candidate_names:
            sql_store.delete_document_by_name(name)
        still_exists = any(sql_store.get_document_by_name(name) is not None for name in candidate_names)
        result["rdb"] = {"success": not still_exists, "exists_after_delete": still_exists, "candidates": candidate_names}
        print(f"[DELETE][RDB] doc={doc_name}, exists_after_delete={still_exists}, candidates={candidate_names}")
    except Exception as e:
        result["rdb"] = {"success": False, "error": str(e)}
        print(f"[DELETE][RDB] doc={doc_name}, error={e}")

    # 2) Weaviate ì‚­ì œ + ê²€ì¦
    try:
        deleted_total = 0
        for name in candidate_names:
            delete_res = vector_store.delete_by_doc_name(doc_name=name, collection_name=collection)
            deleted_total += int(delete_res.get("deleted", 0))
        docs_after = vector_store.list_documents(collection_name=collection)
        remains = any(
            (d.get("doc_id") in candidate_names) or (d.get("doc_name") in candidate_names)
            for d in docs_after
        )
        result["weaviate"] = {
            "success": not remains,
            "deleted": deleted_total,
            "exists_after_delete": remains,
            "candidates": candidate_names,
        }
        print(f"[DELETE][Weaviate] doc={doc_name}, deleted={deleted_total}, exists_after_delete={remains}, candidates={candidate_names}")
    except Exception as e:
        result["weaviate"] = {"success": False, "error": str(e)}
        print(f"[DELETE][Weaviate] doc={doc_name}, error={e}")

    # 3) Neo4j ì‚­ì œ + ê²€ì¦
    try:
        if not delete_from_neo4j:
            result["neo4j"] = {"success": True, "skipped": True}
            print(f"[DELETE][Neo4j] doc={doc_name}, skipped=True")
        else:
            graph = get_graph_store()
            if not graph.test_connection():
                result["neo4j"] = {"success": False, "error": "Neo4j ì—°ê²° ì‹¤íŒ¨"}
                print(f"[DELETE][Neo4j] doc={doc_name}, error=Neo4j ì—°ê²° ì‹¤íŒ¨")
            else:
                candidate_ids = candidate_names[:]

                for candidate in candidate_ids:
                    graph.delete_document(candidate)

                remains = any(graph.get_document(candidate) for candidate in candidate_ids)
                result["neo4j"] = {
                    "success": not remains,
                    "doc_ids": candidate_ids,
                    "exists_after_delete": remains,
                }
                print(f"[DELETE][Neo4j] doc={doc_name}, doc_ids={candidate_ids}, exists_after_delete={remains}")
    except Exception as e:
        result["neo4j"] = {"success": False, "error": str(e)}
        print(f"[DELETE][Neo4j] doc={doc_name}, error={e}")

    # 4) S3 DOCX ì‚­ì œ + ê²€ì¦
    try:
        s3 = get_s3_store()
        deleted_count = 0
        for name in candidate_names:
            deleted_count += s3.delete_docx_versions(name)
        remains = any(s3.has_docx(name) for name in candidate_names)
        result["s3_docx"] = {
            "success": not remains,
            "deleted": deleted_count,
            "exists_after_delete": remains,
            "candidates": candidate_names,
        }
        print(f"[DELETE][S3-DOCX] doc={doc_name}, deleted={deleted_count}, exists_after_delete={remains}, candidates={candidate_names}")
    except Exception as e:
        result["s3_docx"] = {"success": False, "error": str(e)}
        print(f"[DELETE][S3-DOCX] doc={doc_name}, error={e}")

    success = (
        result["rdb"].get("success", False)
        and result["weaviate"].get("success", False)
        and result["neo4j"].get("success", False)
        and result["s3_docx"].get("success", False)
    )
    print(f"[DELETE] ì²˜ë¦¬ ì¢…ë£Œ: doc={doc_name}, success={success}")
    return {"success": success, "details": result}


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    return {"collections": [vector_store.get_collection_info(name) for name in collections]}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› í¬ë§·"""
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²•"""
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - Neo4j ê·¸ë˜í”„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/status")
def graph_status():
    """Neo4j ì—°ê²° ìƒíƒœ"""
    try:
        graph = get_graph_store()
        connected = graph.test_connection()
        stats = graph.get_graph_stats() if connected else {}
        return {"connected": connected, "stats": stats}
    except Exception as e:
        return {"connected": False, "error": str(e)}


@app.post("/graph/init")
def graph_init():
    """Neo4j ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
    try:
        graph = get_graph_store()
        graph.init_schema()
        return {"success": True, "message": "ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/clear")
def graph_clear():
    """Neo4j ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        graph = get_graph_store()
        graph.clear_all()
        return {"success": True, "message": "ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/graph/upload")
async def graph_upload_document(
    file: UploadFile = File(...),
    use_langgraph: bool = Form(True)
):
    """ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ë¡œë§Œ ì—…ë¡œë“œ"""
    try:
        content = await file.read()
        filename = file.filename
        
        if not LANGGRAPH_AVAILABLE:
            raise HTTPException(500, "LangGraph ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        result = process_document(filename, content, debug=True)
        if not result.get("success"):
            raise HTTPException(400, f"ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")
        
        graph = get_graph_store()
        _upload_to_neo4j_from_pipeline(graph, result, filename)
        
        return {
            "success": True,
            "filename": filename,
            "doc_id": result.get("metadata", {}).get("doc_id"),
            "sections": len(result.get("sections", [])),
            "pipeline": "langgraph"
        }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/rag/documents")
async def list_documents(
    user_id: Optional[str] = None,
    doc_type: Optional[str] = None
):
    """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        docs = await asyncio.to_thread(sql_store.get_all_documents, user_id, doc_type)
        return docs
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}")
def graph_get_document(doc_id: str):
    """íŠ¹ì • ë¬¸ì„œ ìƒì„¸"""
    try:
        graph = get_graph_store()
        doc = graph.get_document(doc_id)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
        return doc
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/document/{doc_id}")
def graph_delete_document(doc_id: str):
    """Neo4jì—ì„œ ë¬¸ì„œ ì‚­ì œ"""
    try:
        graph = get_graph_store()
        result = graph.delete_document(doc_id)
        return {"success": True, "doc_id": doc_id, "result": result}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}/hierarchy")
def graph_get_hierarchy(doc_id: str):
    """ë¬¸ì„œ ì„¹ì…˜ ê³„ì¸µ"""
    try:
        graph = get_graph_store()
        hierarchy = graph.get_section_hierarchy(doc_id)
        return {"doc_id": doc_id, "hierarchy": hierarchy}
    except Exception as e:
        raise HTTPException(500, f"ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}/references")
def graph_get_references(doc_id: str):
    """ë¬¸ì„œ ì°¸ì¡° ê´€ê³„"""
    try:
        graph = get_graph_store()
        refs = graph.get_document_references(doc_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
        return refs
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì°¸ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/sections")
def graph_search_sections(keyword: str, doc_id: str = None):
    """ì„¹ì…˜ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(keyword, doc_id)
        return {"keyword": keyword, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/terms")
def graph_search_terms(term: str):
    """ìš©ì–´ ê²€ìƒ‰ (ê°„ì†Œí™” ë²„ì „: ì„¹ì…˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´)"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(term)
        return {"term": term, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ìš©ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/visualization/all")
async def graph_get_full_visualization():
    """ì „ì²´ ë¬¸ì„œ ê·¸ë˜í”„ ì‹œê°í™” ë°ì´í„° (ëª¨ë“  ë¬¸ì„œ + ê´€ê³„)"""
    try:
        graph = get_graph_store()
        full_graph = await asyncio.to_thread(graph.get_full_graph)

        return {
            "success": True,
            "nodes": full_graph["nodes"],
            "links": full_graph["links"],
            "node_count": len(full_graph["nodes"]),
            "link_count": len(full_graph["links"])
        }
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/visualization/{doc_id}")
def graph_get_visualization(doc_id: str, format: str = "mermaid"):
    """
    ë¬¸ì„œ ê´€ê³„ ì‹œê°í™” ë°ì´í„° (í”„ë¡ íŠ¸ì—”ë“œìš©)

    format:
    - mermaid: Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ
    - d3: D3.jsìš© JSON (nodes + links)
    - cytoscape: Cytoscape.jsìš© JSON
    """
    try:
        graph = get_graph_store()

        # ë¬¸ì„œ ì°¸ì¡° ê´€ê³„ ì¡°íšŒ
        refs = graph.get_document_references(doc_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")

        doc = refs.get("document", {})
        references = refs.get("references_to", [])
        referenced_by = refs.get("referenced_by", [])

        if format == "mermaid":
            # Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
            lines = ["graph LR"]
            safe_doc_id = doc_id.replace("-", "_")
            title = doc.get("title", doc_id).replace('"', "'")

            lines.append(f'    Main[\"{doc_id}<br/>({title})\"]:::mainNode')

            # ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œë“¤
            for ref in references:
                ref_id = ref.get("doc_id", "").replace("-", "_")
                ref_title = ref.get("title", "").replace('"', "'")
                if ref_id:
                    lines.append(f'    Main --> {ref_id}[\"{ref.get("doc_id", "")}<br/>({ref_title})\"]')

            # ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œë“¤
            for cited in referenced_by:
                cited_id = cited.get("doc_id", "").replace("-", "_")
                cited_title = cited.get("title", "").replace('"', "'")
                if cited_id:
                    lines.append(f'    {cited_id}[\"{cited.get("doc_id", "")}<br/>({cited_title})\"] --> Main')

            lines.append("    classDef mainNode fill:#f96,stroke:#333,stroke-width:4px,color:#000;")
            lines.append("    classDef default fill:#eee,stroke:#333,color:#000;")

            return {
                "format": "mermaid",
                "doc_id": doc_id,
                "code": "\n".join(lines)
            }

        elif format == "d3":
            # D3.jsìš© JSON ìƒì„±
            nodes = []
            links = []

            # ë©”ì¸ ë…¸ë“œ
            nodes.append({
                "id": doc_id,
                "label": doc.get("title", doc_id),
                "type": "main",
                "group": 0
            })

            # ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œë“¤ (ìƒìœ„)
            for idx, ref in enumerate(references):
                ref_id = ref.get("doc_id")
                if ref_id:
                    nodes.append({
                        "id": ref_id,
                        "label": ref.get("title", ref_id),
                        "type": "reference",
                        "group": 1
                    })
                    links.append({
                        "source": doc_id,
                        "target": ref_id,
                        "type": "references"
                    })

            # ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œë“¤ (í•˜ìœ„)
            for idx, cited in enumerate(referenced_by):
                cited_id = cited.get("doc_id")
                if cited_id:
                    nodes.append({
                        "id": cited_id,
                        "label": cited.get("title", cited_id),
                        "type": "cited_by",
                        "group": 2
                    })
                    links.append({
                        "source": cited_id,
                        "target": doc_id,
                        "type": "cites"
                    })

            return {
                "format": "d3",
                "doc_id": doc_id,
                "data": {
                    "nodes": nodes,
                    "links": links
                }
            }

        elif format == "cytoscape":
            # Cytoscape.jsìš© JSON ìƒì„±
            elements = []

            # ë©”ì¸ ë…¸ë“œ
            elements.append({
                "data": {
                    "id": doc_id,
                    "label": doc.get("title", doc_id),
                    "type": "main"
                },
                "classes": "main-node"
            })

            # ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œë“¤
            for ref in references:
                ref_id = ref.get("doc_id")
                if ref_id:
                    elements.append({
                        "data": {
                            "id": ref_id,
                            "label": ref.get("title", ref_id),
                            "type": "reference"
                        }
                    })
                    elements.append({
                        "data": {
                            "source": doc_id,
                            "target": ref_id,
                            "type": "references"
                        }
                    })

            # ì°¸ì¡°ë°›ëŠ” ë¬¸ì„œë“¤
            for cited in referenced_by:
                cited_id = cited.get("doc_id")
                if cited_id:
                    elements.append({
                        "data": {
                            "id": cited_id,
                            "label": cited.get("title", cited_id),
                            "type": "cited_by"
                        }
                    })
                    elements.append({
                        "data": {
                            "source": cited_id,
                            "target": doc_id,
                            "type": "cites"
                        }
                    })

            return {
                "format": "cytoscape",
                "doc_id": doc_id,
                "elements": elements
            }

        else:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§·ì…ë‹ˆë‹¤: {format}")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/impact/{doc_id}")
def graph_get_impact_analysis(doc_id: str):
    """
    ë¬¸ì„œ ë³€ê²½ ì‹œ ì˜í–¥ ë¶„ì„
    - ì´ ë¬¸ì„œê°€ ë³€ê²½ë˜ë©´ ì˜í–¥ì„ ë°›ëŠ” ë‹¤ë¥¸ ë¬¸ì„œë“¤ê³¼ ì¡°í•­ë“¤
    """
    try:
        graph = get_graph_store()

        # ì˜í–¥ ë¶„ì„ ì‹¤í–‰
        impacts = graph.get_impact_analysis(doc_id)

        if not impacts:
            return {
                "doc_id": doc_id,
                "impacts": [],
                "count": 0,
                "message": "ì´ ë¬¸ì„œì˜ ë³€ê²½ìœ¼ë¡œ ì˜í–¥ë°›ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            }

        # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
        impact_by_doc = {}
        for impact in impacts:
            src_doc = impact.get("source_doc_id")
            if src_doc not in impact_by_doc:
                impact_by_doc[src_doc] = {
                    "doc_id": src_doc,
                    "sections": []
                }

            impact_by_doc[src_doc]["sections"].append({
                "section_id": impact.get("citing_section"),
                "section_title": impact.get("citing_section_title", ""),
                "context": impact.get("context", "")
            })

        return {
            "doc_id": doc_id,
            "impacts": list(impact_by_doc.values()),
            "count": len(impact_by_doc),
            "total_sections": len(impacts)
        }

    except Exception as e:
        raise HTTPException(500, f"ì˜í–¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - Question ì¶”ì 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/questions")
def graph_list_questions(limit: int = 50, session_id: str = None):
    """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        questions = graph.get_question_history(session_id=session_id, limit=limit)
        return {"questions": questions, "count": len(questions)}
    except Exception as e:
        raise HTTPException(500, f"ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/questions/{question_id}/sources")
def graph_get_question_sources(question_id: str):
    """ì§ˆë¬¸ì´ ì°¸ì¡°í•œ ì„¹ì…˜ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        result = graph.get_question_sources(question_id)
        if not result:
            raise HTTPException(404, f"ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {question_id}")
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/stats/section-usage")
def graph_section_usage_stats(doc_id: str = None):
    """ì„¹ì…˜ ì‚¬ìš© í†µê³„ (ê°„ì†Œí™”: Question íˆìŠ¤í† ë¦¬ë¡œ ëŒ€ì²´)"""
    try:
        graph = get_graph_store()
        # ê°„ì†Œí™” ë²„ì „: ì „ì²´ í†µê³„ë§Œ ì œê³µ
        stats = graph.get_graph_stats()
        return {"stats": stats}
    except Exception as e:
        raise HTTPException(500, f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - ì—ì´ì „íŠ¸ (NEW!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from backend.agent import (
        init_agent_tools, 
        run_agent, 
        AGENT_TOOLS,
        LANGCHAIN_AVAILABLE,
        LANGGRAPH_AGENT_AVAILABLE,
        ZAI_AVAILABLE
    )
    AGENT_AVAILABLE = True
    print(" ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    AGENT_AVAILABLE = False
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False
    ZAI_AVAILABLE = False
    print(f" ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class AgentRequest(BaseModel):
    """ì—ì´ì „íŠ¸ ìš”ì²­"""
    message: str
    session_id: Optional[str] = None
    llm_model: str = "gpt-4o"
    embedding_model: str = "multilingual-e5-small" # ì¶”ê°€
    n_results: int = DEFAULT_N_RESULTS #  ì¶”ê°€
    use_langgraph: bool = True  # LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€


@app.post("/agent/chat")
async def agent_chat(request: AgentRequest):
    """
     ì—ì´ì „íŠ¸ ì±„íŒ… - LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰
    
    ì¼ë°˜ RAGì™€ ë‹¤ë¥´ê²Œ ì—ì´ì „íŠ¸ê°€ ìƒí™©ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:
    - search_sop_documents: ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰
    """
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    session_id = request.session_id or str(uuid.uuid4())

    # ê¸°ì¡´ run_agent() ì§ì ‘ í˜¸ì¶œ ëŒ€ì‹  íì— ë„£ê¸°
    payload = request.model_dump()
    payload["session_id"] = session_id
    payload["llm_model"] = resolve_effective_llm_model(request.llm_model)

    return await enqueue_job(
        kind="agent",
        payload=payload
    )


@app.get("/agent/status")
def agent_status():
    """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    return {
        "agent_available": AGENT_AVAILABLE,
        "langchain_available": LANGCHAIN_AVAILABLE if AGENT_AVAILABLE else False,
        "langgraph_agent_available": LANGGRAPH_AGENT_AVAILABLE if AGENT_AVAILABLE else False,
        "tools": [t.name for t in AGENT_TOOLS] if AGENT_AVAILABLE else [],
        "message": "ì—ì´ì „íŠ¸ ì‚¬ìš© ê°€ëŠ¥" if AGENT_AVAILABLE else "ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"
    }


@app.get("/agent/tools")
def agent_tools():
    """ì—ì´ì „íŠ¸ ë„êµ¬ ëª©ë¡"""
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    tools_info = []
    for tool in AGENT_TOOLS:
        tools_info.append({
            "name": tool.name,
            "description": tool.description
        })

    return {"tools": tools_info, "count": len(tools_info)}


#  í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” ì—”ë“œí¬ì¸íŠ¸
class SimpleRequest(BaseModel):
    message: str

@app.post("/test/echo")
def test_echo(request: SimpleRequest):
    """í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” API"""
    return {
        "session_id": str(uuid.uuid4()),
        "answer": f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {request.message}",
        "success": True
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM as a Judge í‰ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EvaluationRequest(BaseModel):
    """í‰ê°€ ìš”ì²­ ëª¨ë¸"""
    question: str
    answer: str
    context: Optional[str] = ""
    metrics: Optional[List[str]] = None  # ["faithfulness", "groundness", "relevancy", "correctness"]
    reference_answer: Optional[str] = None

@app.post("/evaluate")
def evaluate_answer(request: EvaluationRequest):
    """
    ğŸ” LLM as a Judge - ë‹µë³€ í‰ê°€ (RDB ê²€ì¦ í¬í•¨)

    í‰ê°€ ë©”íŠ¸ë¦­:
    - faithfulness: ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ì„± (í™˜ê° ë°©ì§€)
    - groundness: ê·¼ê±° ëª…í™•ì„±
    - relevancy: ì§ˆë¬¸ ê´€ë ¨ì„±
    - correctness: ì •í™•ì„±ê³¼ ì™„ì „ì„±

    **ë¬´ì¡°ê±´ RDBì—ì„œ ì‹¤ì œ ë¬¸ì„œë¥¼ ì¡°íšŒí•˜ì—¬ ì¸ìš© ì •í™•ì„± ê²€ì¦**
    """
    try:
        from backend.evaluation import AgentEvaluator

        # RDB ê²€ì¦ì„ ìœ„í•´ sql_store í•„ìˆ˜ ì „ë‹¬
        evaluator = AgentEvaluator(
            judge_model="gpt-4o",
            sql_store=sql_store
        )

        # í‰ê°€ ì‹¤í–‰
        results = evaluator.evaluate_single(
            question=request.question,
            answer=request.answer,
            context=request.context,
            metrics=request.metrics,
            reference_answer=request.reference_answer
        )

        return {
            "success": True,
            "evaluation": results,
            "average_score": results.get("average_score", 0)
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# === Security Context ===
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from passlib.context import CryptContext
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from datetime import datetime, timedelta

pwd_context = CryptContext(schemes=["pbkdf2_sha256", "bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/login")

# JWT ì„¤ì •
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-change-this-in-production")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7  # 7ì¼

# === Auth Models ===
class Token(BaseModel):
    access_token: str
    token_type: str
    user: Dict

class TokenData(BaseModel):
    username: Optional[str] = None

class UserRegister(BaseModel):
    username: str
    password: str
    name: str
    email: str
    rank: Optional[str] = None
    dept: Optional[str] = None

    @field_validator("email")
    @classmethod
    def validate_email(cls, v):
        if not v:
            return v
        regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(regex, v):
            raise ValueError("ì˜¬ë°”ë¥¸ ì´ë©”ì¼ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return v

class UserLogin(BaseModel):
    username: str
    password: str

class UserSnippet(BaseModel):
    username: str
    name: str

class PasswordReset(BaseModel):
    user_id: int
    new_password: str

class FindUsernameRequest(BaseModel):
    name: str
    dept: Optional[str] = None

# === Auth Helper Functions ===
def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user(token: str = Depends(oauth2_scheme)) -> Dict:
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
        token_data = TokenData(username=username)
    except JWTError:
        raise credentials_exception

    user = sql_store.get_user_by_username(token_data.username)
    if user is None:
        raise credentials_exception
    return user


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# === Auth Endpoints ===
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ë¶€ì„œ/ì§ì±… ê¸°ë³¸ ëª©ë¡
DEFAULT_DEPARTMENTS = ["í’ˆì§ˆê´€ë¦¬ë¶€", "í’ˆì§ˆë³´ì¦ë¶€", "ìƒì‚°ë¶€", "ì—°êµ¬ê°œë°œë¶€", "ê²½ì˜ì§€ì›ë¶€", "ì˜ì—…ë¶€"]
DEFAULT_RANKS = ["ì‚¬ì›", "ì£¼ì„", "ëŒ€ë¦¬", "ê³¼ì¥", "ì°¨ì¥", "ë¶€ì¥", "ì´ì‚¬", "ìƒë¬´"]

@app.get("/auth/options")
def get_auth_options():
    """íšŒì›ê°€ì… ì‹œ ë¶€ì„œ/ì§ì±… ë“œë¡­ë‹¤ìš´ ì˜µì…˜ ì¡°íšŒ"""
    depts = set(DEFAULT_DEPARTMENTS)
    ranks = set(DEFAULT_RANKS)
    try:
        with sql_store._get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT DISTINCT dept FROM users WHERE dept IS NOT NULL AND dept != ''")
                for row in cur.fetchall():
                    depts.add(row[0])
                cur.execute("SELECT DISTINCT rank FROM users WHERE rank IS NOT NULL AND rank != ''")
                for row in cur.fetchall():
                    ranks.add(row[0])
    except Exception:
        pass
    return {
        "departments": sorted(list(depts)),
        "ranks": sorted(list(ranks))
    }

@app.post("/auth/register", response_model=Token)
async def register(user: UserRegister):
    # ì¤‘ë³µ ì²´í¬
    existing_user = sql_store.get_user_by_username(user.username)
    if existing_user:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì•„ì´ë””ì…ë‹ˆë‹¤.")
    
    # ë¹„ë°€ë²ˆí˜¸ í•´ì‹œ & ì €ì¥
    hashed_pw = get_password_hash(user.password)
    user_id = sql_store.register_user(
        username=user.username,
        password_hash=hashed_pw,
        name=user.name,
        email=user.email,
        rank=user.rank,
        dept=user.dept
    )
    
    if not user_id:
        raise HTTPException(status_code=500, detail="íšŒì›ê°€ì… ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # íšŒì›ê°€ì… ì§í›„ ë¡œê·¸ì¸ ìƒíƒœë¡œ ê°„ì£¼í•˜ì—¬ last_login ê°±ì‹ 
    sql_store.update_last_login(user_id)
    
    # ìë™ ë¡œê·¸ì¸ ì²˜ë¦¬ (í† í° ë°œê¸‰)
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username, "user_id": user_id},
        expires_delta=access_token_expires
    )
    
    # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ (ì‘ë‹µìš©)
    new_user = sql_store.get_user(user_id)
    
    return {"access_token": access_token, "token_type": "bearer", "user": new_user}

@app.post("/auth/login", response_model=Token)
async def login(user_req: UserLogin):
    user = sql_store.get_user_by_username(user_req.username)
    if not user or not verify_password(user_req.password, user['password_hash']):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # ë¡œê·¸ì¸ ì„±ê³µ: last_login ê°±ì‹  & í† í° ë°œê¸‰
    sql_store.update_last_login(user['id'])
    # ê°±ì‹ ëœ ì •ë³´(last_login ë“±)ë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜´
    user = sql_store.get_user_by_username(user_req.username)
    
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user['username'], "user_id": user['id']},
        expires_delta=access_token_expires
    )
    
    # ë¯¼ê° ì •ë³´ ì œê±°
    user_resp = {k: v for k, v in user.items() if k != 'password_hash'}
    
    return {"access_token": access_token, "token_type": "bearer", "user": user_resp}

@app.get("/auth/me")
async def read_users_me(current_user: Dict = Depends(get_current_user)):
    user_resp = {k: v for k, v in current_user.items() if k != 'password_hash'}
    return {"user": user_resp}

@app.post("/auth/find-username")
async def find_username(req: FindUsernameRequest):
    """ì´ë¦„ê³¼ ë¶€ì„œë¡œ ì•„ì´ë”” ì°¾ê¸°"""
    with sql_store._get_connection() as conn:
        with conn.cursor() as cur:
            query = "SELECT username FROM users WHERE name = %s"
            params = [req.name]
            if req.dept and req.dept != 'ì „ì²´':
                query += " AND dept = %s"
                params.append(req.dept)
            
            cur.execute(query, tuple(params))
            res = cur.fetchone()
            if res:
                return {"username": res[0]}
            raise HTTPException(status_code=404, detail="ì¼ì¹˜í•˜ëŠ” ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

@app.post("/auth/verify-user")
async def verify_user_identity(req: UserSnippet):
    """ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •ì„ ìœ„í•œ ë³¸ì¸ í™•ì¸ (ì•„ì´ë”” + ì´ë¦„)"""
    user = sql_store.get_user_by_username(req.username)
    if user and user['name'] == req.name:
        return {"user_id": user['id']}
    raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

@app.post("/auth/reset-password")
async def reset_password_endpoint(req: PasswordReset):
    """ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •"""
    hashed_pw = get_password_hash(req.new_password)
    try:
        with sql_store._get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("UPDATE users SET password_hash = %s WHERE id = %s", (hashed_pw, req.user_id))
                conn.commit()
        return {"message": "ë¹„ë°€ë²ˆí˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception:
        raise HTTPException(status_code=500, detail="ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ ì‹¤íŒ¨")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OnlyOffice + S3 ì—”ë“œí¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OnlyOfficeConfigRequest(BaseModel):
    doc_name: str
    version: Optional[str] = None
    user_name: str = "í¸ì§‘ì"
    mode: str = "view"


async def process_docx_upload_task(
    content: bytes,
    doc_name: str,
    version: str,
    collection: str,
    task_id: str
):
    """DOCX ì—…ë¡œë“œ ë° RAG íŒŒì´í”„ë¼ì¸ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…"""
    start_time = time.time()
    update_task_status(task_id, "processing", f"'{doc_name}' DOCX íŒŒì¼ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.")
    
    try:
        # 1. S3 ì €ì¥
        s3 = get_s3_store()
        s3_key = await asyncio.to_thread(s3.upload_docx, doc_name, version, content)
        
        # 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        model_path = resolve_model_path("multilingual-e5-small")
        embed_model = SentenceTransformer(model_path)
        filename = f"{doc_name}_v{version}.docx"

        result = await asyncio.to_thread(
            process_document,
            file_path=filename,
            content=content,
            doc_id=doc_name,
            use_llm_metadata=False,
            embed_model=embed_model,
        )

        if not result.get("success"):
            raise Exception(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")

        chunks_data = result.get("chunks", [])
        
        @dataclass
        class _Chunk:
            text: str
            metadata: dict
            index: int = 0

        chunks = [_Chunk(text=c["text"], metadata=c["metadata"], index=c["index"]) for c in chunks_data]
        update_task_status(task_id, "processing", f"DOCX íŒŒì‹± ì™„ë£Œ (ì²­í¬ {len(chunks)}ê°œ). DB ì €ì¥ ì¤‘...")

        # 3. Weaviate ì €ì¥
        pipeline_version = "pdf-clause-v2.0"
        texts = [c.text for c in chunks]
        metadatas = [
            {**c.metadata, "chunk_method": "article", "model": "multilingual-e5-small", "pipeline_version": pipeline_version}
            for c in chunks
        ]
        await asyncio.to_thread(vector_store.add_documents, texts=texts, metadatas=metadatas, collection_name=collection, model_name=model_path)

        # 4. PostgreSQL ì €ì¥
        from backend.document_pipeline import docx_to_markdown
        markdown_text = await asyncio.to_thread(docx_to_markdown, content)

        doc_id_db = await asyncio.to_thread(
            sql_store.save_document,
            doc_name=doc_name,
            content=markdown_text,
            doc_type="docx",
            version=version,
        )
        if doc_id_db and chunks:
            batch_chunks = [
                {"clause": c.metadata.get("clause_id"), "content": c.text, "metadata": c.metadata}
                for c in chunks
            ]
            await asyncio.to_thread(sql_store.save_chunks_batch, doc_id_db, batch_chunks)

        # 5. Neo4j ì €ì¥
        try:
            graph = get_graph_store()
            if graph and graph.test_connection():
                await asyncio.to_thread(_upload_to_neo4j_from_pipeline, graph, result, filename)
        except Exception as graph_err:
            print(f"  Neo4j ì €ì¥ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {graph_err}")

        elapsed = round(time.time() - start_time, 2)
        update_task_status(task_id, "completed", f"DOCX ì—…ë¡œë“œ ë° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({elapsed}ì´ˆ)", doc_name=doc_name, version=version, s3_key=s3_key)

    except Exception as e:
        import traceback
        traceback.print_exc()
        update_task_status(task_id, "error", f"DOCX ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/rag/upload-docx")
async def upload_docx_to_s3(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    doc_name: str = Form(...),
    version: str = Form("1.0"),
    collection: str = Form("documents"),
):
    """
    DOCX íŒŒì¼ì„ S3ì— ì €ì¥ í›„ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë¹„ë™ê¸°)
    """
    if not file.filename.lower().endswith('.docx'):
        raise HTTPException(400, "DOCX íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    content = await file.read()
    task_id = f"docx_{uuid.uuid4().hex[:8]}"
    update_task_status(task_id, "waiting", f"'{doc_name}' DOCX ì—…ë¡œë“œ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤.", doc_name=doc_name)

    background_tasks.add_task(
        process_docx_upload_task,
        content=content,
        doc_name=doc_name,
        version=version,
        collection=collection,
        task_id=task_id
    )

    return {
        "success": True,
        "message": "DOCX ì—…ë¡œë“œ ë° ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "task_id": task_id,
        "doc_name": doc_name
    }


class UploadS3Request(BaseModel):
    s3_key: str
    doc_name: str
    version: str = "1.0"


# S3 / OnlyOffice ì‹±ê¸€í†¤ (í•„ìš” ì‹œ lazy ì´ˆê¸°í™”)
_s3_store = None

def get_s3_store():
    global _s3_store
    if _s3_store is None:
        from backend.s3_store import S3Store
        _s3_store = S3Store()
    return _s3_store


@app.post("/onlyoffice/config")
async def onlyoffice_config(request: OnlyOfficeConfigRequest):
    """
    OnlyOffice ì—ë””í„° ì„¤ì • JSON ë°˜í™˜

    ì…ë ¥: { doc_name, version(optional), user_name }
    ë™ì‘:
      1. SQLì—ì„œ ìµœì‹  ë²„ì „ ì¡°íšŒ (version ë¯¸ì§€ì • ì‹œ)
      2. S3 presigned URL ìƒì„±
      3. OnlyOffice ì„¤ì • JSON ë°˜í™˜
    """
    try:
        from backend.onlyoffice_service import create_editor_config, get_onlyoffice_server_url, BACKEND_URL

        doc_name = request.doc_name

        # ë²„ì „ ê²°ì •: ì‚¬ìš©ì ì§€ì • ì—†ìœ¼ë©´ DB ìµœì‹  ë²„ì „ ì‚¬ìš©
        version = request.version
        if not version:
            versions_data = sql_store.get_document_versions(doc_name)
            if not versions_data:
                raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name}")
            version = versions_data[0].get('version', '1.0')

        # ë°±ì—”ë“œ ë‚´ë¶€ URL ì‚¬ìš© (S3 presigned URL ëŒ€ì‹ )
        # OnlyOfficeê°€ JWT í—¤ë”ë¥¼ ë¶™ì—¬ ìš”ì²­ â†’ ë°±ì—”ë“œê°€ ê²€ì¦ í›„ S3ì—ì„œ íŒŒì¼ ì„œë¹™
        file_url = f"{BACKEND_URL}/onlyoffice/document/{doc_name}/{version}"

        # OnlyOffice ì„¤ì • ìƒì„±
        config = create_editor_config(
            doc_id=doc_name,
            version=version,
            user_name=request.user_name,
            file_url=file_url,
            mode=request.mode,
        )

        return {
            "config": config,
            "doc_name": doc_name,
            "version": version,
            "onlyoffice_server_url": get_onlyoffice_server_url(),
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"OnlyOffice ì„¤ì • ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.get("/onlyoffice/document/{doc_name}/{version}")
async def serve_docx_for_onlyoffice(doc_name: str, version: str):
    """
    OnlyOffice Document Serverê°€ DOCXë¥¼ ê°€ì ¸ê°€ëŠ” ë‚´ë¶€ ì—”ë“œí¬ì¸íŠ¸.
    S3 presigned URL ëŒ€ì‹  ì´ URLì„ document.urlë¡œ ì‚¬ìš©:
      - OnlyOfficeê°€ JWT Authorization í—¤ë”ë¥¼ ë¶™ì—¬ë„ S3ì™€ ì¶©ëŒ ì—†ìŒ
      - ë°±ì—”ë“œê°€ S3ì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì™€ OnlyOfficeì— ì§ì ‘ ì„œë¹™
    """
    try:
        s3 = get_s3_store()
        content = s3.download_docx(doc_name, version)
        return Response(
            content=content,
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={"Content-Disposition": f'attachment; filename="{doc_name}_v{version}.docx"'},
        )
    except Exception as e:
        raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_name} v{version} ({e})")


@app.post("/onlyoffice/callback")
async def onlyoffice_callback(request: Request):
    """
    OnlyOffice ì½œë°± ì²˜ë¦¬

    OnlyOfficeê°€ ì €ì¥ ì™„ë£Œ ì‹œ í˜¸ì¶œ.
    status 2(ì €ì¥ ì¤‘) ë˜ëŠ” 6(í¸ì§‘ ì™„ë£Œ) ì‹œ DOCXë¥¼ S3ì— ìƒˆ ë²„ì „ìœ¼ë¡œ ì €ì¥í•˜ê³ 
    RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    ë°˜í™˜: {"error": 0} (OnlyOffice ìš”êµ¬ì‚¬í•­)
    """
    try:
        callback_data = await request.json()
        status = callback_data.get('status')
        download_url = callback_data.get('url')
        key = callback_data.get('key', '')

        print(f"\n[OnlyOffice Callback] status={status}, key={key}")

        # status 2 = ë¬¸ì„œ ì €ì¥ ì¤‘, status 6 = í¸ì§‘ ì™„ë£Œ(ê°•ì œ ì €ì¥)
        if status not in (2, 6):
            return {"error": 0}

        if not download_url:
            print("  ì½œë°± URL ì—†ìŒ - ê±´ë„ˆëœ€")
            return {"error": 0}

        # key í˜•ì‹: {doc_id}_v{version}_{timestamp}
        parts = key.split('_v')
        doc_id = parts[0] if len(parts) > 1 else key
        
        task_id = f"onlyoffice_{doc_id}_{int(time.time())}"
        update_task_status(task_id, "processing", f"'{doc_id}' OnlyOffice í¸ì§‘ë³¸ì„ ì €ì¥ ì¤‘ì…ë‹ˆë‹¤.", doc_name=doc_id)
        version_part = parts[1].rsplit('_', 1)[0] if len(parts) > 1 else '1.0'

        # ìƒˆ ë²„ì „ ë²ˆí˜¸ ê²°ì • (í˜„ì¬ ë²„ì „ + 0.1)
        try:
            current_v = float(version_part)
            new_version = f"{current_v + 0.1:.1f}"
        except ValueError:
            new_version = version_part + "_edited"

        print(f"  ë¬¸ì„œ: {doc_id}, í˜„ì¬ë²„ì „: {version_part} â†’ ìƒˆë²„ì „: {new_version}")

        # 1. OnlyOffice ì„œë²„ì—ì„œ í¸ì§‘ëœ DOCX ë‹¤ìš´ë¡œë“œ
        from backend.onlyoffice_service import download_from_onlyoffice
        docx_content = await download_from_onlyoffice(download_url)
        print(f"  DOCX ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(docx_content)} bytes")

        # 2. S3ì— ìƒˆ ë²„ì „ìœ¼ë¡œ ì €ì¥
        s3 = get_s3_store()
        s3_key = s3.upload_docx(doc_id, new_version, docx_content)
        print(f"  S3 ì €ì¥ ì™„ë£Œ: {s3_key}")

        # 3. DOCX â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        from backend.document_pipeline import docx_to_markdown
        markdown_text = docx_to_markdown(docx_content)

        # 4. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (process_document)
        model_path = resolve_model_path("multilingual-e5-small")
        embed_model = SentenceTransformer(model_path)

        result = process_document(
            file_path=f"{doc_id}_v{new_version}.docx",
            content=docx_content,
            doc_id=doc_id,
            use_llm_metadata=False,
            embed_model=embed_model,
        )

        if not result.get("success"):
            print(f"  íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result.get('errors')}")
            return {"error": 0}

        chunks_data = result.get("chunks", [])

        # 5. Weaviate ì €ì¥
        from dataclasses import dataclass

        @dataclass
        class _Chunk:
            text: str
            metadata: dict
            index: int = 0

        chunks = [_Chunk(text=c["text"], metadata=c["metadata"], index=c["index"]) for c in chunks_data]
        texts = [c.text for c in chunks]
        metadatas = [
            {**c.metadata, "chunk_method": "article", "model": "multilingual-e5-small", "pipeline_version": "pdf-clause-v2.0"}
            for c in chunks
        ]
        vector_store.add_documents(
            texts=texts, metadatas=metadatas, collection_name="documents", model_name=model_path
        )

        # 6. PostgreSQL ì €ì¥
        doc_id_db = sql_store.save_document(
            doc_name=doc_id,
            content=markdown_text,
            doc_type="docx",
            version=new_version,
        )
        if doc_id_db and chunks:
            batch_chunks = [
                {"clause": c.metadata.get("clause_id"), "content": c.text, "metadata": c.metadata}
                for c in chunks
            ]
            sql_store.save_chunks_batch(doc_id_db, batch_chunks)

        # 7. Neo4j ì €ì¥
        try:
            graph = get_graph_store()
            if graph.test_connection():
                _upload_to_neo4j_from_pipeline(graph, result, f"{doc_id}_v{new_version}.docx")
        except Exception as graph_err:
            print(f"  Neo4j ì €ì¥ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {graph_err}")

        elapsed = round(time.time() - start_time, 2)
        update_task_status(task_id, "completed", f"ë¬¸ì„œ ìˆ˜ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({elapsed}ì´ˆ)", doc_id=doc_id, version=new_version)
        print(f"  [OnlyOffice Callback] ì™„ë£Œ - ìƒˆ ë²„ì „ {new_version} ì €ì¥ë¨")
        return {"error": 0}

    except Exception as e:
        import traceback
        traceback.print_exc()
        if 'task_id' in locals():
            update_task_status(task_id, "error", f"OnlyOffice ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        print(f"  [OnlyOffice Callback] ì˜¤ë¥˜: {e}")
        # OnlyOfficeëŠ” error: 0ì´ ì•„ë‹ˆë©´ ì¬ì‹œë„í•˜ë¯€ë¡œ í•­ìƒ 0 ë°˜í™˜
        return {"error": 0}


@app.post("/rag/upload-s3")
async def upload_from_s3(request: UploadS3Request):
    """
    S3ì— ì´ë¯¸ ìˆëŠ” DOCX íŒŒì¼ì„ RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬

    ì…ë ¥: { s3_key, doc_name, version }
    ë™ì‘:
      1. S3ì—ì„œ DOCX ë‹¤ìš´ë¡œë“œ
      2. ê¸°ì¡´ /rag/upload íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    """
    start_time = time.time()
    try:
        import boto3
        import os

        # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        s3_client = boto3.client(
            's3',
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
            region_name=os.getenv('AWS_REGION', 'ap-northeast-2'),
        )
        bucket = os.getenv('S3_BUCKET_NAME')
        response = s3_client.get_object(Bucket=bucket, Key=request.s3_key)
        content = response['Body'].read()

        filename = f"{request.doc_name}_v{request.version}.docx"
        model_path = resolve_model_path("multilingual-e5-small")
        embed_model = SentenceTransformer(model_path)

        result = process_document(
            file_path=filename,
            content=content,
            doc_id=request.doc_name,
            use_llm_metadata=False,
            embed_model=embed_model,
        )

        if not result.get("success"):
            raise HTTPException(400, f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")

        chunks_data = result.get("chunks", [])

        from dataclasses import dataclass

        @dataclass
        class _Chunk:
            text: str
            metadata: dict
            index: int = 0

        chunks = [_Chunk(text=c["text"], metadata=c["metadata"], index=c["index"]) for c in chunks_data]

        # Weaviate ì €ì¥
        pipeline_version = "pdf-clause-v2.0"
        texts = [c.text for c in chunks]
        metadatas = [
            {**c.metadata, "chunk_method": "article", "model": "multilingual-e5-small", "pipeline_version": pipeline_version}
            for c in chunks
        ]
        vector_store.add_documents(
            texts=texts, metadatas=metadatas, collection_name="documents", model_name=model_path
        )

        # PostgreSQL ì €ì¥
        from backend.document_pipeline import docx_to_markdown
        markdown_text = docx_to_markdown(content)

        final_version = request.version or result.get("version", "1.0")
        doc_id_db = sql_store.save_document(
            doc_name=request.doc_name,
            content=markdown_text,
            doc_type="docx",
            version=final_version,
        )
        if doc_id_db and chunks:
            batch_chunks = [
                {"clause": c.metadata.get("clause_id"), "content": c.text, "metadata": c.metadata}
                for c in chunks
            ]
            sql_store.save_chunks_batch(doc_id_db, batch_chunks)

        # Neo4j ì €ì¥
        try:
            graph = get_graph_store()
            if graph.test_connection():
                _upload_to_neo4j_from_pipeline(graph, result, filename)
        except Exception as graph_err:
            print(f"  Neo4j ì €ì¥ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {graph_err}")

        elapsed = round(time.time() - start_time, 2)
        return {
            "success": True,
            "doc_name": request.doc_name,
            "version": final_version,
            "chunks": len(chunks),
            "elapsed_seconds": elapsed,
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"S3 ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("[ì‹œìŠ¤í…œ] ì´ˆê¸°í™” ì¤‘...")
    sql_store.init_db()
    
    # Neo4j ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” connect ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        get_graph_store()
    except Exception as e:
        print(f" Neo4j ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨: {e}")

    # Weaviate ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” get_client ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        wv_client = vector_store.get_client()
        if not wv_client.is_connected():
            print(" Weaviate v4 ì—°ê²° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
    except Exception as e:
        print(f" Weaviate v4 ì—°ê²° ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

    
    import uvicorn
    
    print("\n" + "=" * 60)
    print(" RAG Chatbot API v14.0 + OpenAI Agent")
    print("=" * 60)
    print(f" LLM ë°±ì—”ë“œ: OpenAI (gpt-4o)")
    print(f" ì—ì´ì „íŠ¸: {' í™œì„±í™”' if LANGGRAPH_AVAILABLE else ' ë¹„í™œì„±í™”'}")
    
    if LANGGRAPH_AVAILABLE:
        print(f"   - LangChain: {'í™œì„±í™”' if LANGCHAIN_AVAILABLE else 'ë¹„í™œì„±í™”'}")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("ì£¼ìš” ê¸°ëŠ¥:")
    print("  - LangGraph ë¬¸ì„œ íŒŒì´í”„ë¼ì¸")
    print("  -  ReAct ì—ì´ì „íŠ¸ (/agent/chat)")
    print("  - Weaviate(v4) + Neo4j + PostgreSQL")
    print("  - LangSmith ì¶”ì  ì§€ì›")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=1)


if __name__ == "__main__":
    main()
