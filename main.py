"""
RAG ì±—ë´‡ API v11.0 + Agent (Z.AI)

ğŸ”¥ v11.0 ë³€ê²½ì‚¬í•­:
- LLM ë°±ì—”ë“œ ë³€ê²½: Ollama â†’ Z.AI GLM-4.7-Flash
- ì—ì´ì „íŠ¸ ë„êµ¬ ì„±ëŠ¥ ê°•í™”
- LangSmith ì¶”ì  ì§€ì› ë° ìµœì í™”
- ë˜ë¬»ê¸° ë¡œì§ ì œê±° ë° ê²€ìƒ‰ ê²°ê³¼ ì§ì ‘ ì¶œë ¥
"""

# ğŸ”¥ .env íŒŒì¼ ìë™ ë¡œë“œ (ë‹¤ë¥¸ importë³´ë‹¤ ë¨¼ì €!)
from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import torch
import time
import uuid

from backend.sql_store import SQLStore
sql_store = SQLStore()
# sql_store.init_db()  # ğŸ”¥ main()ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€

# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©)
# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©) ì œê±°ë¨
# LangGraph íŒŒì´í”„ë¼ì¸ì´ ì „ì ìœ¼ë¡œ ì²˜ë¦¬

from backend import vector_store
# from backend.prompt import build_rag_prompt, build_chunk_prompt (ì œê±°ë¨)
from backend.llm import (
    get_llm_response,
    ZaiLLM,
    OllamaLLM,
    analyze_search_results,
    HUGGINGFACE_MODELS,
)

# ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ (v9.2)
try:
    from backend.document_pipeline import process_document, state_to_chunks, Chunk
    LANGGRAPH_AVAILABLE = True
    print("âœ… LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    print(f"âš ï¸ LangGraph ì‚¬ìš© ë¶ˆê°€, ë ˆê±°ì‹œ ëª¨ë“œ: {e}")


app = FastAPI(title="RAG Chatbot API", version="9.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.on_event("shutdown")
def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    print("\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    vector_store.close_client()
    if _graph_store:
        _graph_store.close()
        print("ğŸ›‘ Neo4j ì—°ê²° ì¢…ë£Œë¨")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 500
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 7  # ğŸ”¥ 5 -> 7 ìƒí–¥
DEFAULT_SIMILARITY_THRESHOLD = 0.30  # ğŸ”¥ 0.35 -> 0.30 (ë” ë§ì€ ë§¥ë½ í™•ë³´)
USE_LANGGRAPH = True  # ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì—¬ë¶€

PRESET_MODELS = {
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
}


device = "cuda" if torch.cuda.is_available() else "cpu"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë©”ëª¨ë¦¬)
chat_histories: Dict[str, List[Dict]] = {}

# Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ (ì‹±ê¸€í†¤)
_graph_store = None


def get_graph_store():
    """Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ ì‹±ê¸€í†¤"""
    global _graph_store
    if _graph_store is None:
        from backend.graph_store import Neo4jGraphStore
        _graph_store = Neo4jGraphStore()
        _graph_store.connect()
    return _graph_store


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "qwen2.5:3b"
    llm_backend: str = "ollama"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "glm-4.7-flash"
    llm_backend: str = "zai"  # ğŸ”¥ ê¸°ë³¸ê°’ zaië¡œ ë³€ê²½
    temperature: float = 0.7
    filter_doc: Optional[str] = None
    language: str = "ko"
    max_tokens: int = 512
    similarity_threshold: Optional[float] = None
    include_sources: bool = True


class LLMRequest(BaseModel):
    prompt: str
    model: str = "qwen2.5:3b"
    backend: str = "ollama"
    max_tokens: int = 256
    temperature: float = 0.1


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"
    delete_from_neo4j: bool = True  # ğŸ”¥ Neo4jì—ì„œë„ ì‚­ì œ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model: str) -> str:
    """ëª¨ë¸ í”„ë¦¬ì…‹ â†’ ì „ì²´ ê²½ë¡œ"""
    return PRESET_MODELS.get(model, model)


def format_context(results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ â†’ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    context_parts = []
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        text = r.get("text", "")
        similarity = r.get("similarity", 0)
        
        # ğŸ”¥ v9.2: ê°œì„ ëœ ì¶œì²˜ í‘œì‹œ
        sop_id = meta.get("sop_id", "")
        section_path = meta.get("section_path", "")
        page = meta.get("page", "")
        article_num = meta.get("article_num", "")
        
        # ì¶œì²˜ í—¤ë” êµ¬ì„±
        source_parts = []
        if sop_id:
            source_parts.append(f"[{sop_id}]")
        if section_path:
            source_parts.append(f"> {section_path}")
        if page:
            source_parts.append(f"(p.{page})")
        if similarity:
            source_parts.append(f"ê´€ë ¨ë„: {similarity:.0%}")
        
        source_header = " ".join(source_parts) if source_parts else f"[ë¬¸ì„œ {i}]"
        
        context_parts.append(f"ğŸ“„ {source_header}\n{text}")
    
    return "\n\n---\n\n".join(context_parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG Chatbot API v9.2",
        "features": [
            "LangGraph íŒŒì´í”„ë¼ì¸",
            "í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì ",
            "Parent-Child ê³„ì¸µ",
            "Question ì¶”ì  (Neo4j)",
            "Weaviate + Neo4j ë™ê¸°í™” ì‚­ì œ"
        ],
        "endpoints": {
            "upload": "/rag/upload",
            "search": "/rag/search",
            "chat": "/chat",
            "ask": "/rag/ask",
            "graph": "/graph/*"
        },
        "langgraph_enabled": LANGGRAPH_AVAILABLE and USE_LANGGRAPH
    }


@app.get("/health")
def health():
    return {
        "status": "healthy",
        "cuda": torch.cuda.is_available(),
        "device": device,
        "ollama": OllamaLLM.is_available(),
        "langgraph": LANGGRAPH_AVAILABLE
    }


@app.get("/models/embedding")
def list_embedding_models():
    return {
        "presets": PRESET_MODELS,
        "specs": vector_store.EMBEDDING_MODEL_SPECS,
        "compatible": vector_store.filter_compatible_models()
    }


@app.get("/models/llm")
def list_llm_models():
    available_ollama = []
    if OllamaLLM.is_available():
        available_ollama = OllamaLLM.list_models()
    return {
        "ollama": {"presets": OLLAMA_MODELS, "available": available_ollama},
        "huggingface": HUGGINGFACE_MODELS
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ API ì—”ë“œí¬ì¸íŠ¸ - ì—…ë¡œë“œ (LangGraph v9.2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    use_langgraph: bool = Form(True),  # ğŸ”¥ LangGraph ì‚¬ìš© ì—¬ë¶€
    use_llm_metadata: bool = Form(False),  # ğŸ”¥ LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‚¬ìš© ì—¬ë¶€
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ (LangGraph v9.2 íŒŒì´í”„ë¼ì¸)
    
    - ChromaDBì— ë²¡í„° ì €ì¥
    - Neo4jì— ê·¸ë˜í”„ ì €ì¥
    - í˜ì´ì§€ ë²ˆí˜¸, Parent-Child ê³„ì¸µ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    start_time = time.time()
    
    try:
        content = await file.read()
        filename = file.filename
        
        print(f"\n{'='*70}")
        print(f"ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ: {filename}")
        print(f"{'='*70}")
        
        # LangGraph íŒŒì´í”„ë¼ì¸ í•„ìˆ˜
        if not LANGGRAPH_AVAILABLE:
            raise HTTPException(500, "LangGraph íŒŒì´í”„ë¼ì¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # === LangGraph íŒŒì´í”„ë¼ì¸ (v9.2) ===
        print(f"   ğŸ”¥ LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
        print(f"   ğŸ”¥ LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: {'í™œì„±í™”' if use_llm_metadata else 'ë¹„í™œì„±í™”'}")

        result = process_document(
            filename=filename,
            content=content,
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            debug=True,
            use_llm_metadata=use_llm_metadata,
            use_clause_parsing=True  # GXP ë¬¸ì„œ ì¡°í•­ ë²ˆí˜¸ ê¸°ë°˜ íŒŒì‹±
        )
        
        if not result.get("success"):
            errors = result.get("errors", ["ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"])
            raise HTTPException(400, f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {errors}")
        
        chunks = state_to_chunks(result)
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        metadata_base = result.get("metadata", {})
        sop_id = metadata_base.get("doc_id") or metadata_base.get("sop_id")
        
        # ğŸ”¥ IDê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ëìë¦¬ ìˆ«ìë¡œë¼ë„ ìœ ì¶” ì‹œë„
        if not sop_id:
            import re
            id_match = re.search(r'([A-Z0-9]+-[A-Z0-9]+-\d+)', filename)
            if id_match:
                sop_id = id_match.group(1)
            else:
                sop_id = filename.split('.')[0] # ìµœí›„ì˜ ìˆ˜ë‹¨: íŒŒì¼ëª…
        
        # ì œëª© ì„¤ì •: ì›ë³¸ íŒŒì¼ëª… ìœ ì§€ (ì‚¬ìš©ì ìš”ì²­)
        doc_title = filename 
        extracted_title = metadata_base.get("title")
        if extracted_title and extracted_title not in filename:
            doc_title = f"{filename} ({extracted_title})"
        
        print(f"   DOC ID: {sop_id}")
        print(f"   ì œëª©: {doc_title}")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {result.get('quality_score', 0):.0%}")
        print(f"   ë³€í™˜ ë°©ë²•: {result.get('conversion_method')}")
        print(f"   ì´ ì²­í¬: {len(chunks)}")
        
        pipeline_version = "langgraph-v9.2"
        quality_score = result.get("quality_score", 0)
        conversion_method = result.get("conversion_method", "unknown")
        
        if not chunks:
            raise HTTPException(400, "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # === Weaviate ì €ì¥ ===
        model_path = resolve_model_path(model)
        texts = [c.text for c in chunks]
        metadatas = [
            {
                **c.metadata,
                "chunk_method": chunk_method,
                "model": model,
                "pipeline_version": pipeline_version,
            }
            for c in chunks
        ]
        
        vector_store.add_documents(
            texts=texts,
            metadatas=metadatas,
            collection_name=collection,
            model_name=model_path
        )
        print(f"   âœ… Weaviate ì €ì¥ ì™„ë£Œ: {len(chunks)} ì²­í¬")
        
        # === PostgreSQL ì €ì¥ ===
        try:
            # ì›ë³¸ ë§ˆí¬ë‹¤ìš´ ê²°ì • (LangGraph ê²°ê³¼ ìš°ì„ , ì—†ìœ¼ë©´ ì²­í¬ í•©ì‚°)
            full_markdown = ""
            if use_langgraph and 'result' in locals() and result.get("markdown"):
                full_markdown = result.get("markdown")
            else:
                full_markdown = "\n\n".join([c.text for c in chunks])

            doc_id_db = sql_store.save_document(
                doc_name=sop_id,
                content=full_markdown,
                doc_type=filename.split('.')[-1] if '.' in filename else None,
                version=metadata_base.get("version", "1.0")
            )
            
            # ğŸ”¥ ì²­í¬ ì •ë³´ë„ PostgreSQLì— ì €ì¥ (ë¬¸ì„œ-ì²­í¬ ì—°ê²°)
            if doc_id_db and chunks:
                batch_chunks = [
                    {
                        "clause": c.metadata.get("clause_id"),
                        "content": c.text,
                        "metadata": c.metadata
                    }
                    for c in chunks
                ]
                sql_store.save_chunks_batch(doc_id_db, batch_chunks)
        except Exception as sql_err:
            print(f"   âš ï¸ PostgreSQL ìƒì„¸ ì €ì¥ ì‹¤íŒ¨: {sql_err}")
            # í´ë°±: ê¸°ì¡´ ìœ ì € ì½”ë“œ ë°©ì‹ (í•„ìš” ì‹œ)
            # save_chunks_to_db(sop_id, filename, chunks)
        
        # === Neo4j ê·¸ë˜í”„ ì €ì¥ ===
        graph_uploaded = False
        graph_sections = 0
        
        try:
            from backend.graph_store import Neo4jGraphStore
            
            graph = get_graph_store()
            if graph.test_connection():
                # LangGraph ê²°ê³¼ì—ì„œ ê·¸ë˜í”„ ìƒì„±
                if use_langgraph and LANGGRAPH_AVAILABLE:
                    # ì§ì ‘ ì„¹ì…˜ ë°ì´í„°ë¡œ ê·¸ë˜í”„ ìƒì„±
                    _upload_to_neo4j_from_pipeline(graph, result, filename)
                else:
                    # ë ˆê±°ì‹œ: ParsedDocumentì—ì„œ ìƒì„±
                    from backend.graph_store import document_to_graph
                    document_to_graph(graph, parsed_doc, sop_id)
                
                graph_uploaded = True
                stats = graph.get_graph_stats()
                graph_sections = stats.get("sections", 0)
                print(f"   âœ… Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì™„ë£Œ")
        except Exception as graph_error:
            print(f"   âš ï¸ Neo4j ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {graph_error}")
        
        elapsed = round(time.time() - start_time, 2)
        
        return {
            "success": True,
            "filename": filename,
            "sop_id": sop_id,
            "doc_title": doc_title,
            "chunks": len(chunks),
            "chunk_method": chunk_method,
            "pipeline_version": pipeline_version,
            "quality_score": quality_score,
            "conversion_method": conversion_method,
            "graph_uploaded": graph_uploaded,
            "elapsed_seconds": elapsed,
            "metadata": metadata_base,  # ğŸ”¥ ì „ì²´ ë©”íƒ€ë°ì´í„° ë°˜í™˜
            "sample_metadata": metadatas[0] if metadatas else {},
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


def _upload_to_neo4j_from_pipeline(graph, result: dict, filename: str):
    """LangGraph íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ Neo4jì— ì—…ë¡œë“œ (V22.0 ëŒ€ì‘)"""
    metadata = result.get("metadata", {})
    doc_id = metadata.get("doc_id") or "UNKNOWN"
    title = metadata.get("title") or filename
    version = metadata.get("version") or "1.0"
    effective_date = metadata.get("effective_date")
    owning_dept = metadata.get("owning_dept")
    
    # 1. Document ë…¸ë“œ ìƒì„±
    graph.create_document(
        doc_id=doc_id,
        title=title,
        version=version,
        effective_date=effective_date,
        owning_dept=owning_dept,
        metadata=metadata
    )
    
    # 2. DocumentType ì²˜ë¦¬ (ì½”ë“œ ê¸°ë°˜)
    doc_type_code = "SOP" # ê¸°ë³¸ê°’
    if "SOP" in doc_id: doc_type_code = "SOP"
    elif "WI" in doc_id: doc_type_code = "WI"
    
    graph.create_document_type(doc_type_code, "í‘œì¤€ì‘ì—…ì ˆì°¨ì„œ" if doc_type_code == "SOP" else "ì‘ì—…ì§€ì¹¨ì„œ", doc_type_code)
    graph.link_doc_to_type(doc_id, doc_type_code)
    
    # 3. Section ë…¸ë“œ ìƒì„± ë° ê´€ê³„ ì„¤ì •
    sections = result.get("sections", [])
    
    for sec in sections:
        headers = sec.get("headers", {})
        content = sec.get("content", "")
        page = sec.get("page", 1)
        parent_name = sec.get("parent")
        clause_meta = sec.get("clause_meta", {})
        
        # clause_level ë° section_id ìœ ì¶”
        clause_level = 0
        current_title = ""
        for level in range(6, 0, -1):
            if headers.get(f"H{level}"):
                clause_level = level
                current_title = headers[f"H{level}"]
                break
        
        clause_id = None
        import re
        num_match = re.match(r'^(\d+(?:\.\d+)*)', current_title)
        if num_match:
            clause_id = num_match.group(1)
        
        if not clause_id: continue
        
        section_id = f"{doc_id}:{clause_id}"
        main_section = clause_id.split('.')[0] if '.' in clause_id else clause_id
        
        # Section ë…¸ë“œ ìƒì„± (ìƒì„¸ ë©”íƒ€ë°ì´í„° í¬í•¨)
        graph.create_section(
            doc_id=doc_id,
            section_id=section_id,
            title=current_title,
            content=content,
            clause_level=clause_level,
            main_section=main_section,
            llm_meta=clause_meta,
            page=page
        )
        
        # 4. ê³„ì¸µ ê´€ê³„ (Parent-Child)
        if parent_name:
            # ë¶€ëª¨ ID ìœ ì¶” (ë‹¨ìˆœí™”: ê°™ì€ ë¬¸ì„œ ë‚´ì—ì„œ ì  í•˜ë‚˜ ëº€ íŒ¨í„´)
            if '.' in clause_id:
                parent_clause_id = '.'.join(clause_id.split('.')[:-1])
                parent_section_id = f"{doc_id}:{parent_clause_id}"
                graph.create_section_hierarchy(parent_section_id, section_id)
        
        # 5. Concept ì—°ë™ (intent_scope í™œìš©)
        intent_scope = clause_meta.get("intent_scope")
        if intent_scope:
            graph.create_concept(intent_scope, intent_scope, intent_scope)
            graph.link_section_to_concept(section_id, intent_scope)
            
        # 6. íƒ€ ë¬¸ì„œ ì–¸ê¸‰ (MENTIONS) ì¶”ì 
        mentions = re.findall(r'((?:EQ-)?SOP[-_]?\d{4,5})', content, re.IGNORECASE)
        for m in set(mentions):
            m_id = m.upper().replace('_', '-')
            if not m_id.startswith('EQ-'): m_id = 'EQ-' + m_id
            if m_id != doc_id:
                graph.link_section_to_mention_doc(section_id, m_id)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# /rag/search ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨ (Agentê°€ ë‚´ë¶€ ìˆ˜í–‰)


# /rag/search/advanced ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì±—ë´‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/chat")
def chat(request: ChatRequest):
    """
    Main Agent Chat Endpoint
    - Manual RAG ë¡œì§ ì œê±°ë¨
    - ì˜¤ì§ Agent Orchestratorë¥¼ í†µí•´ì„œë§Œ ë‹µë³€
    """
    print(f"ğŸ¤– [Agent] ìš”ì²­ ìˆ˜ì‹ : {request.message}")
    
    try:
        # Agent ì‹¤í–‰
        # llm.py ì—…ë°ì´íŠ¸ì— ë”°ë¼ model_name íŒŒë¼ë¯¸í„° ë“±ì„ ì ì ˆíˆ ì „ë‹¬
        init_agent_tools(vector_store, get_graph_store(), sql_store)
        
        response = run_agent(
            query=request.message,
            session_id=request.session_id or str(uuid.uuid4()),
            model_name=request.llm_model or "gpt-4o-mini"
        )
        
        return {
            "session_id": request.session_id,
            "answer": response.get("answer"),
            "sources": [], 
            "agent_log": response
        }
    except Exception as e:
        print(f"âŒ [Agent] ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))



@app.get("/chat/history/{session_id}")
def get_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = chat_histories.get(session_id, [])
    return {"session_id": session_id, "history": history, "count": len(history)}


@app.delete("/chat/history/{session_id}")
def clear_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    if session_id in chat_histories:
        del chat_histories[session_id]
        return {"success": True, "message": f"ì„¸ì…˜ {session_id} ì‚­ì œë¨"}
    return {"success": False, "message": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/llm/generate")
def generate_llm(request: LLMRequest):
    """LLM ì§ì ‘ í˜¸ì¶œ"""
    try:
        response = get_llm_response(
            prompt=request.prompt,
            llm_model=request.model,
            llm_backend=request.backend,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        return {"response": response, "model": request.model, "backend": request.backend}
    except Exception as e:
        raise HTTPException(500, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """
    ğŸ”¥ ë¬¸ì„œ ì‚­ì œ (Weaviate + Neo4j ë™ì‹œ ì‚­ì œ)
    """
    result = {"chromadb": None, "neo4j": None}
    
    # 1. Weaviate ì‚­ì œ
    chroma_result = vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )
    result["weaviate"] = chroma_result
    
    # 2. Neo4j ì‚­ì œ (ì˜µì…˜)
    if request.delete_from_neo4j:
        try:
            graph = get_graph_store()
            if graph.test_connection():
                # doc_nameì—ì„œ sop_id ì¶”ì¶œ ì‹œë„
                import re
                sop_match = re.search(r'(EQ-SOP-\d+)', request.doc_name, re.IGNORECASE)
                if sop_match:
                    sop_id = sop_match.group(1).upper()
                    neo4j_result = graph.delete_document(sop_id)
                    result["neo4j"] = {"success": True, "sop_id": sop_id, "deleted": neo4j_result}
                else:
                    result["neo4j"] = {"success": False, "message": "SOP IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"}
        except Exception as e:
            result["neo4j"] = {"success": False, "error": str(e)}
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    success = chroma_result.get("success", False)
    
    return {
        "success": success,
        "doc_name": request.doc_name,
        "details": result
    }


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    return {"collections": [vector_store.get_collection_info(name) for name in collections]}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› í¬ë§·"""
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²•"""
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - Neo4j ê·¸ë˜í”„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/status")
def graph_status():
    """Neo4j ì—°ê²° ìƒíƒœ"""
    try:
        graph = get_graph_store()
        connected = graph.test_connection()
        stats = graph.get_graph_stats() if connected else {}
        return {"connected": connected, "stats": stats}
    except Exception as e:
        return {"connected": False, "error": str(e)}


@app.post("/graph/init")
def graph_init():
    """Neo4j ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
    try:
        graph = get_graph_store()
        graph.init_schema()
        return {"success": True, "message": "ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/clear")
def graph_clear():
    """Neo4j ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        graph = get_graph_store()
        graph.clear_all()
        return {"success": True, "message": "ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/graph/upload")
async def graph_upload_document(
    file: UploadFile = File(...),
    use_langgraph: bool = Form(True)
):
    """ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ë¡œë§Œ ì—…ë¡œë“œ"""
    try:
        content = await file.read()
        filename = file.filename
        
        if not LANGGRAPH_AVAILABLE:
            raise HTTPException(500, "LangGraph ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        result = process_document(filename, content, debug=True)
        if not result.get("success"):
            raise HTTPException(400, f"ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")
        
        graph = get_graph_store()
        _upload_to_neo4j_from_pipeline(graph, result, filename)
        
        return {
            "success": True,
            "filename": filename,
            "sop_id": result.get("metadata", {}).get("sop_id"),
            "sections": len(result.get("sections", [])),
            "pipeline": "langgraph"
        }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/documents")
def graph_list_documents():
    """Neo4j ë¬¸ì„œ ëª©ë¡"""
    try:
        graph = get_graph_store()
        docs = graph.get_all_documents()
        return {"documents": docs, "count": len(docs)}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}")
def graph_get_document(sop_id: str):
    """íŠ¹ì • ë¬¸ì„œ ìƒì„¸"""
    try:
        graph = get_graph_store()
        doc = graph.get_document(sop_id)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return doc
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/document/{sop_id}")
def graph_delete_document(sop_id: str):
    """Neo4jì—ì„œ ë¬¸ì„œ ì‚­ì œ"""
    try:
        graph = get_graph_store()
        result = graph.delete_document(sop_id)
        return {"success": True, "sop_id": sop_id, "result": result}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/hierarchy")
def graph_get_hierarchy(sop_id: str):
    """ë¬¸ì„œ ì„¹ì…˜ ê³„ì¸µ"""
    try:
        graph = get_graph_store()
        hierarchy = graph.get_section_hierarchy(sop_id)
        return {"sop_id": sop_id, "hierarchy": hierarchy}
    except Exception as e:
        raise HTTPException(500, f"ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{sop_id}/references")
def graph_get_references(sop_id: str):
    """ë¬¸ì„œ ì°¸ì¡° ê´€ê³„"""
    try:
        graph = get_graph_store()
        refs = graph.get_document_references(sop_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sop_id}")
        return refs
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì°¸ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/sections")
def graph_search_sections(keyword: str, sop_id: str = None):
    """ì„¹ì…˜ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(keyword, sop_id)
        return {"keyword": keyword, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/terms")
def graph_search_terms(term: str):
    """ìš©ì–´ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_by_term(term)
        return {"term": term, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ìš©ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¥ API ì—”ë“œí¬ì¸íŠ¸ - Question ì¶”ì 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/questions")
def graph_list_questions(limit: int = 50, session_id: str = None):
    """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        questions = graph.get_question_history(session_id=session_id, limit=limit)
        return {"questions": questions, "count": len(questions)}
    except Exception as e:
        raise HTTPException(500, f"ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/questions/{question_id}/sources")
def graph_get_question_sources(question_id: str):
    """ì§ˆë¬¸ì´ ì°¸ì¡°í•œ ì„¹ì…˜ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        result = graph.get_question_sources(question_id)
        if not result:
            raise HTTPException(404, f"ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {question_id}")
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/stats/section-usage")
def graph_section_usage_stats(sop_id: str = None):
    """ì„¹ì…˜ ì‚¬ìš© í†µê³„"""
    try:
        graph = get_graph_store()
        stats = graph.get_section_usage_stats(sop_id)
        return {"stats": stats, "count": len(stats)}
    except Exception as e:
        raise HTTPException(500, f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– API ì—”ë“œí¬ì¸íŠ¸ - ì—ì´ì „íŠ¸ (NEW!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from backend.agent import (
        init_agent_tools, 
        run_agent, 
        AGENT_TOOLS,
        LANGCHAIN_AVAILABLE,
        LANGGRAPH_AGENT_AVAILABLE,
        ZAI_AVAILABLE
    )
    AGENT_AVAILABLE = True
    print("âœ… ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    AGENT_AVAILABLE = False
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False
    ZAI_AVAILABLE = False
    print(f"âš ï¸ ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class AgentRequest(BaseModel):
    """ì—ì´ì „íŠ¸ ìš”ì²­"""
    message: str
    session_id: Optional[str] = None
    llm_model: str = "glm-4.7-flash"
    embedding_model: str = "multilingual-e5-small" # ì¶”ê°€
    n_results: int = DEFAULT_N_RESULTS # ğŸ”¥ ì¶”ê°€
    use_langgraph: bool = True  # LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€


@app.post("/agent/chat")
def agent_chat(request: AgentRequest):
    """
    ğŸ¤– ì—ì´ì „íŠ¸ ì±„íŒ… - LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰
    
    ì¼ë°˜ RAGì™€ ë‹¤ë¥´ê²Œ ì—ì´ì „íŠ¸ê°€ ìƒí™©ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:
    - search_sop_documents: ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰
    - get_document_references: ë¬¸ì„œ ê°„ ì°¸ì¡° ê´€ê³„
    - search_sections_by_keyword: í‚¤ì›Œë“œë¡œ ì„¹ì…˜ ê²€ìƒ‰
    - get_document_structure: ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨
    - list_all_documents: ì „ì²´ ë¬¸ì„œ ëª©ë¡
    """
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    session_id = request.session_id or str(uuid.uuid4())
    
    print(f"\n{'='*50}")
    print(f"ğŸ¤– ì—ì´ì „íŠ¸ ì§ˆë¬¸: {request.message}")
    print(f"   ì„¸ì…˜: {session_id}")
    print(f"   ëª¨ë“œ: {'LangGraph' if request.use_langgraph else 'Simple'}")
    
    try:
        # ë„êµ¬ ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)
        init_agent_tools(vector_store, get_graph_store(), sql_store)
        
        # í†µí•©ëœ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = run_agent(
            query=request.message,
            session_id=session_id,
            model_name=request.llm_model,
            embedding_model=resolve_model_path(request.embedding_model)
        )
        
        reasoning = result.get("reasoning")
        answer = result.get("answer", "")

        # ë³¸ë¬¸(answer)ì´ ë¹„ì–´ìˆëŠ”ë° reasoningë§Œ ìˆëŠ” ê²½ìš° (í† í° í•œë„ ì´ˆê³¼ ë“±ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ)
        if not answer and reasoning:
            print("âš ï¸ ë³¸ë¬¸ì´ ì§ì ‘ì ìœ¼ë¡œ ìˆ˜ì‹ ë˜ì§€ ì•Šì•„ ì‚¬ê³  ê³¼ì •(Reasoning)ì„ ë‹µë³€ìœ¼ë¡œ ìµœìš°ì„  ë…¸ì¶œí•©ë‹ˆë‹¤.")
            result["answer"] = f"[AI ë¶„ì„ ë¦¬í¬íŠ¸]\n\n{reasoning}"
            answer = result["answer"]
        
        if reasoning:
            print(f"ğŸ§  ëª¨ë¸ì˜ ìƒê°(Reasoning) ì¶”ì¶œë¨ ({len(reasoning)}ì)")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì²« 100ì ì •ë„ ì¶œë ¥
            print(f"   [THINK] {reasoning[:150].replace('\n', ' ')}...")
        
        print(f"   ë„êµ¬ í˜¸ì¶œ: {len(result.get('tool_calls', []))}íšŒ")
        print(f"   ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))} ê¸€ì")
        print(f"{'='*50}\n")
        
        return {
            "session_id": session_id,
            "answer": result.get("answer", ""),
            "tool_calls": result.get("tool_calls", []),
            "success": result.get("success", False),
            "mode": "langgraph" if (request.use_langgraph and LANGGRAPH_AGENT_AVAILABLE) else "simple"
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/agent/status")
def agent_status():
    """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    return {
        "agent_available": AGENT_AVAILABLE,
        "langchain_available": LANGCHAIN_AVAILABLE if AGENT_AVAILABLE else False,
        "langgraph_agent_available": LANGGRAPH_AGENT_AVAILABLE if AGENT_AVAILABLE else False,
        "tools": [t.name for t in AGENT_TOOLS] if AGENT_AVAILABLE else [],
        "message": "ì—ì´ì „íŠ¸ ì‚¬ìš© ê°€ëŠ¥" if AGENT_AVAILABLE else "ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"
    }


@app.get("/agent/tools")
def agent_tools():
    """ì—ì´ì „íŠ¸ ë„êµ¬ ëª©ë¡"""
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    tools_info = []
    for tool in AGENT_TOOLS:
        tools_info.append({
            "name": tool.name,
            "description": tool.description
        })

    return {"tools": tools_info, "count": len(tools_info)}


# ğŸ”¥ í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” ì—”ë“œí¬ì¸íŠ¸
class SimpleRequest(BaseModel):
    message: str

@app.post("/test/echo")
def test_echo(request: SimpleRequest):
    """í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” API"""
    return {
        "session_id": str(uuid.uuid4()),
        "answer": f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {request.message}",
        "success": True
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("ğŸš¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    sql_store.init_db()
    
    # Neo4j ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” connect ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        get_graph_store()
    except Exception as e:
        print(f"âŒ Neo4j ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨: {e}")

    # Weaviate ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” get_client ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        wv_client = vector_store.get_client()
        if not wv_client.is_connected():
            print("âŒ Weaviate v4 ì—°ê²° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
    except Exception as e:
        print(f"âŒ Weaviate v4 ì—°ê²° ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

    
    import uvicorn
    
    print("\n" + "=" * 60)
    print("ğŸ¤– RAG Chatbot API v11.0 + Z.AI Agent")
    print("=" * 60)
    print(f"ğŸ”¥ LLM ë°±ì—”ë“œ: {'âœ… Z.AI (GLM-4.7-Flash)' if ZaiLLM.is_available() else 'âŒ ZAI_API_KEY ì„¤ì • í•„ìš”'}")
    print(f"ğŸ¤– ì—ì´ì „íŠ¸: {'âœ… í™œì„±í™”' if AGENT_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
    
    if AGENT_AVAILABLE:
        print(f"   - LangChain: {'âœ…' if LANGCHAIN_AVAILABLE else 'âŒ'}")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("ì£¼ìš” ê¸°ëŠ¥:")
    print("  - LangGraph ë¬¸ì„œ íŒŒì´í”„ë¼ì¸")
    print("  - ğŸ¤– ReAct ì—ì´ì „íŠ¸ (/agent/chat)")
    print("  - Weaviate(v4) + Neo4j + PostgreSQL")
    print("  - LangSmith ì¶”ì  ì§€ì›")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)


if __name__ == "__main__":
    main()