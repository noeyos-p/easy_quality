"""
RAG ì±—ë´‡ API v14.0 + Agent (OpenAI)

 v14.0 ë³€ê²½ì‚¬í•­:
- LLM ë°±ì—”ë“œ ë³€ê²½: Z.AI â†’ OpenAI GPT-4o-mini
- ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•© (ëª¨ë“  ì„œë¸Œ ì—ì´ì „íŠ¸ OpenAI ì‚¬ìš©)
- LLM as a Judge í‰ê°€ ì‹œìŠ¤í…œ (RDB ê²€ì¦ í¬í•¨)
- LangSmith ì¶”ì  ì§€ì› ë° ìµœì í™”
"""

#  .env íŒŒì¼ ìë™ ë¡œë“œ (ë‹¤ë¥¸ importë³´ë‹¤ ë¨¼ì €!)
from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
from contextlib import asynccontextmanager
import torch
import time
import uuid
import re

from backend.sql_store import SQLStore
sql_store = SQLStore()
# sql_store.init_db()  #  main()ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€

# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©)
# RAG ëª¨ë“ˆ - ë ˆê±°ì‹œ (í´ë°±ìš©) ì œê±°ë¨
# LangGraph íŒŒì´í”„ë¼ì¸ì´ ì „ì ìœ¼ë¡œ ì²˜ë¦¬

from sentence_transformers import SentenceTransformer
from backend import vector_store
# from backend.prompt import build_rag_prompt, build_chunk_prompt (ì œê±°ë¨)
from backend.llm import (
    get_llm_response,
    ZaiLLM,
    OllamaLLM,
    analyze_search_results,
    HUGGINGFACE_MODELS,
)

#  Document pipeline
try:
    from backend.document_pipeline import process_document
    from dataclasses import dataclass

    @dataclass
    class Chunk:
        text: str
        metadata: dict
        index: int = 0

    LANGGRAPH_AVAILABLE = True
    print(" Document pipeline ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    print(f" Document pipeline ì‚¬ìš© ë¶ˆê°€: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    yield
    # Shutdown
    print("\n ì„œë²„ ì¢…ë£Œ ì¤‘...")
    vector_store.close_client()
    if _graph_store:
        _graph_store.close()
        print(" Neo4j ì—°ê²° ì¢…ë£Œë¨")

app = FastAPI(title="RAG Chatbot API", version="9.2.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_CHUNK_SIZE = 500
DEFAULT_OVERLAP = 50
DEFAULT_CHUNK_METHOD = "article"
DEFAULT_N_RESULTS = 7  #  5 -> 7 ìƒí–¥
DEFAULT_SIMILARITY_THRESHOLD = 0.30  #  0.35 -> 0.30 (ë” ë§ì€ ë§¥ë½ í™•ë³´)
USE_LANGGRAPH = True  #  LangGraph íŒŒì´í”„ë¼ì¸ ì‚¬ìš© ì—¬ë¶€

PRESET_MODELS = {
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
}


device = "cuda" if torch.cuda.is_available() else "cpu"

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë©”ëª¨ë¦¬)
chat_histories: Dict[str, List[Dict]] = {}

# Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ (ì‹±ê¸€í†¤)
_graph_store = None


def get_graph_store():
    """Neo4j ê·¸ë˜í”„ ìŠ¤í† ì–´ ì‹±ê¸€í†¤"""
    global _graph_store
    if _graph_store is None:
        from backend.graph_store import Neo4jGraphStore
        _graph_store = Neo4jGraphStore()
        _graph_store.connect()
    return _graph_store


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pydantic ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    model: str = "multilingual-e5-small"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "gpt-4o-mini"
    llm_backend: str = "openai"
    filter_doc: Optional[str] = None
    similarity_threshold: Optional[float] = None


class AskRequest(BaseModel):
    query: str
    collection: str = "documents"
    n_results: int = DEFAULT_N_RESULTS
    embedding_model: str = "multilingual-e5-small"
    llm_model: str = "gpt-4o-mini"
    llm_backend: str = "openai"  #  ê¸°ë³¸ê°’ openaië¡œ ë³€ê²½
    temperature: float = 0.7
    filter_doc: Optional[str] = None
    language: str = "ko"
    max_tokens: int = 512
    similarity_threshold: Optional[float] = None
    include_sources: bool = True


class LLMRequest(BaseModel):
    prompt: str
    model: str = "qwen2.5:3b"
    backend: str = "ollama"
    max_tokens: int = 256
    temperature: float = 0.1


class DeleteDocRequest(BaseModel):
    doc_name: str
    collection: str = "documents"
    delete_from_neo4j: bool = True  #  Neo4jì—ì„œë„ ì‚­ì œ


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í—¬í¼ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def resolve_model_path(model: str) -> str:
    """ëª¨ë¸ í”„ë¦¬ì…‹ â†’ ì „ì²´ ê²½ë¡œ"""
    return PRESET_MODELS.get(model, model)


def format_context(results: List[Dict]) -> str:
    """ê²€ìƒ‰ ê²°ê³¼ â†’ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    context_parts = []
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        text = r.get("text", "")
        similarity = r.get("similarity", 0)
        
        #  v9.2: ê°œì„ ëœ ì¶œì²˜ í‘œì‹œ
        doc_id = meta.get("doc_id", "")
        section_path = meta.get("section_path", "")
        page = meta.get("page", "")
        article_num = meta.get("article_num", "")
        
        # ì¶œì²˜ í—¤ë” êµ¬ì„±
        source_parts = []
        if doc_id:
            source_parts.append(f"[{doc_id}]")
        if section_path:
            source_parts.append(f"> {section_path}")
        if page:
            source_parts.append(f"(p.{page})")
        if similarity:
            source_parts.append(f"ê´€ë ¨ë„: {similarity:.0%}")
        
        source_header = " ".join(source_parts) if source_parts else f"[ë¬¸ì„œ {i}]"
        
        context_parts.append(f"{source_header}\n{text}")
    
    return "\n\n---\n\n".join(context_parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê¸°ë³¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
def root():
    return {
        "message": "RAG Chatbot API v9.2",
        "features": [
            "LangGraph íŒŒì´í”„ë¼ì¸",
            "í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì ",
            "Parent-Child ê³„ì¸µ",
            "Question ì¶”ì  (Neo4j)",
            "Weaviate + Neo4j ë™ê¸°í™” ì‚­ì œ"
        ],
        "endpoints": {
            "upload": "/rag/upload",
            "search": "/rag/search",
            "chat": "/chat",
            "ask": "/rag/ask",
            "graph": "/graph/*"
        },
        "langgraph_enabled": LANGGRAPH_AVAILABLE and USE_LANGGRAPH
    }


@app.get("/health")
def health():
    return {
        "status": "healthy",
        "cuda": torch.cuda.is_available(),
        "device": device,
        "ollama": OllamaLLM.is_available(),
        "langgraph": LANGGRAPH_AVAILABLE
    }


@app.get("/models/embedding")
def list_embedding_models():
    return {
        "presets": PRESET_MODELS,
        "specs": vector_store.EMBEDDING_MODEL_SPECS,
        "compatible": vector_store.filter_compatible_models()
    }


@app.get("/models/llm")
def list_llm_models():
    available_ollama = []
    if OllamaLLM.is_available():
        available_ollama = OllamaLLM.list_models()
    return {
        "ollama": {"presets": OLLAMA_MODELS, "available": available_ollama},
        "huggingface": HUGGINGFACE_MODELS
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - ì—…ë¡œë“œ (LangGraph v9.2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/rag/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection: str = Form("documents"),
    chunk_size: int = Form(DEFAULT_CHUNK_SIZE),
    chunk_method: str = Form(DEFAULT_CHUNK_METHOD),
    model: str = Form("multilingual-e5-small"),
    overlap: int = Form(DEFAULT_OVERLAP),
    use_langgraph: bool = Form(True),  #  LangGraph ì‚¬ìš© ì—¬ë¶€
    use_llm_metadata: bool = Form(True),  #  LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‚¬ìš© ì—¬ë¶€
    version: Optional[str] = Form(None), # ì‚¬ìš©ìê°€ ì§ì ‘ ì§€ì •í•˜ëŠ” ë²„ì „
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ (LangGraph v9.2 íŒŒì´í”„ë¼ì¸)
    
    - ChromaDBì— ë²¡í„° ì €ì¥
    - Neo4jì— ê·¸ë˜í”„ ì €ì¥
    - í˜ì´ì§€ ë²ˆí˜¸, Parent-Child ê³„ì¸µ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    start_time = time.time()
    
    try:
        content = await file.read()
        filename = file.filename
        
        print(f"\n{'='*70}")
        print(f"ë¬¸ì„œ ì—…ë¡œë“œ: {filename}")
        print(f"{'='*70}\n")

        # ========================================
        # ë¬¸ì„œ íŒŒì‹±
        # ========================================
        print(f"[1ë‹¨ê³„] ë¬¸ì„œ íŒŒì‹±")
        print(f"  íŒŒì´í”„ë¼ì¸: PDF ì¡°í•­ v2.0")
        print(f"  LLM ë©”íƒ€ë°ì´í„°: {'ğŸŸ¢ í™œì„±' if use_llm_metadata else 'ë¹„í™œì„±'}")
        if use_llm_metadata:
            print(f"  LLM ëª¨ë¸: gpt-4o-mini")
        print()

        model_path = resolve_model_path(model)
        embed_model = SentenceTransformer(model_path)

        result = process_document(
            file_path=filename,
            content=content,
            use_llm_metadata=use_llm_metadata,
            embed_model=embed_model
        )

        if not result.get("success"):
            errors = result.get("errors", ["ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"])
            raise HTTPException(400, f"ğŸ”´ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {errors}")

        chunks_data = result["chunks"]
        if not chunks_data:
            raise HTTPException(400, "ğŸ”´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

        from dataclasses import dataclass
        @dataclass
        class Chunk:
            text: str
            metadata: dict
            index: int = 0

        chunks = [Chunk(text=c["text"], metadata=c["metadata"], index=c["index"]) for c in chunks_data]
        doc_id = result.get("doc_id")
        doc_title = result.get("doc_title")
        pipeline_version = "pdf-clause-v2.0"

        print(f"  ğŸŸ¢ íŒŒì‹± ì™„ë£Œ")
        print(f"     â€¢ ID: {doc_id}")
        print(f"     â€¢ ì œëª©: {doc_title}")
        print(f"     â€¢ ì¡°í•­: {result.get('total_clauses')}ê°œ")
        print(f"     â€¢ ì²­í¬: {len(chunks)}ê°œ\n")
        
        # ========================================
        # Weaviate ë²¡í„° ì €ì¥
        # ========================================
        print(f"[2ë‹¨ê³„] Weaviate ë²¡í„° ì €ì¥")

        texts = [c.text for c in chunks]
        metadatas = [
            {
                **c.metadata,
                "chunk_method": chunk_method,
                "model": model,
                "pipeline_version": pipeline_version,
            }
            for c in chunks
        ]

        vector_store.add_documents(
            texts=texts,
            metadatas=metadatas,
            collection_name=collection,
            model_name=model_path
        )
        print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬\n")
        
        # ========================================
        # PostgreSQL ë¬¸ì„œ ì €ì¥
        # ========================================
        print(f"[3ë‹¨ê³„] PostgreSQL ì €ì¥")

        try:
            full_markdown = "\n\n".join([c.text for c in chunks])

            # íŒŒì´í”„ë¼ì¸ì—ì„œ ì¶”ì¶œëœ ë²„ì „ ë˜ëŠ” ì‚¬ìš©ì ì…ë ¥ ë²„ì „ ê²°ì •
            final_version = version or result.get("version", "1.0")
            
            if final_version != "1.0":
                print(f"     [ì¶”ì¶œ] ìµœì¢… ê²°ì •ëœ ë²„ì „: {final_version}")

            doc_id_db = sql_store.save_document(
                doc_name=doc_id,
                content=full_markdown,
                doc_type=filename.split('.')[-1] if '.' in filename else None,
                version=final_version
            )

            if doc_id_db and chunks:
                batch_chunks = [
                    {
                        "clause": c.metadata.get("clause_id"),
                        "content": c.text,
                        "metadata": c.metadata
                    }
                    for c in chunks
                ]
                sql_store.save_chunks_batch(doc_id_db, batch_chunks)
                print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: ë¬¸ì„œ + {len(chunks)}ê°œ ì²­í¬\n")
            else:
                print(f"  ğŸ”´ ì €ì¥ ì‹¤íŒ¨: DB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ID ìƒì„± ë¶ˆê°€)\n")
        except Exception as sql_err:
            print(f"  ğŸ”´ ì €ì¥ ì‹¤íŒ¨: {sql_err}\n")

        # ========================================
        # Neo4j ê·¸ë˜í”„ ì €ì¥
        # ========================================
        print(f"[4ë‹¨ê³„] Neo4j ê·¸ë˜í”„ ì €ì¥")
        graph_uploaded = False
        graph_sections = 0

        try:
            from backend.graph_store import Neo4jGraphStore

            graph = get_graph_store()
            if graph.test_connection():
                _upload_to_neo4j_from_pipeline(graph, result, filename)
                graph_uploaded = True
                stats = graph.get_graph_stats()
                graph_sections = stats.get("sections", 0)
                print(f"  ğŸŸ¢ ì €ì¥ ì™„ë£Œ: {graph_sections}ê°œ ì„¹ì…˜\n")
        except Exception as graph_error:
            # [ë””ë²„ê·¸ ë¡œê·¸ ë³´ê°•] ì—°ê²° ì‹¤íŒ¨ ì‹œ êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
            print(f"  ğŸ”´ Neo4j ì—°ê²° ì‹¤íŒ¨: {graph_error}")
            import traceback
            traceback.print_exc()
            print(f"  âš  ê·¸ë˜í”„ ì—°ë™ì„ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\n")
        
        # ========================================
        # ì™„ë£Œ
        # ========================================
        elapsed = round(time.time() - start_time, 2)

        print(f"{'='*70}")
        print(f"ğŸŸ¢ ì—…ë¡œë“œ ì™„ë£Œ ({elapsed}ì´ˆ)")
        print(f"{'='*70}\n")

        return {
            "success": True,
            "filename": filename,
            "doc_id": doc_id,
            "doc_title": doc_title,
            "version": final_version,
            "chunks": len(chunks),
            "total_clauses": result.get("total_clauses"),
            "chunk_method": chunk_method,
            "pipeline_version": pipeline_version,
            "graph_uploaded": graph_uploaded,
            "elapsed_seconds": elapsed,
            "sample_metadata": metadatas[0] if metadatas else {},
        }

    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ğŸ”´ ì‹¤íŒ¨: {str(e)}")


def _upload_to_neo4j_from_pipeline(graph, result: dict, filename: str):
    """ìƒˆ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ Neo4jì— ì—…ë¡œë“œ (ê°„ì†Œí™”)"""
    from backend.graph_store import upload_document_to_graph
    upload_document_to_graph(graph, result, filename)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# /rag/search ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨ (Agentê°€ ë‚´ë¶€ ìˆ˜í–‰)


# /rag/search/advanced ì—”ë“œí¬ì¸íŠ¸ ì œê±°ë¨


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ì±—ë´‡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/chat")
def chat(request: ChatRequest):
    """
    Main Agent Chat Endpoint
    - Manual RAG ë¡œì§ ì œê±°ë¨
    - ì˜¤ì§ Agent Orchestratorë¥¼ í†µí•´ì„œë§Œ ë‹µë³€
    """
    print(f" [Agent] ìš”ì²­ ìˆ˜ì‹ : {request.message}")
    
    try:
        # Agent ì‹¤í–‰
        # llm.py ì—…ë°ì´íŠ¸ì— ë”°ë¼ model_name íŒŒë¼ë¯¸í„° ë“±ì„ ì ì ˆíˆ ì „ë‹¬
        init_agent_tools(vector_store, get_graph_store(), sql_store)
        
        response = run_agent(
            query=request.message,
            session_id=request.session_id or str(uuid.uuid4()),
            model_name=request.llm_model or "gpt-4o-mini"
        )

        answer = response.get("answer")

        # LLM as a Judge í‰ê°€
        evaluation_scores = None

        # ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ê°ì§€
        error_patterns = ["ì˜¤ë¥˜ê°€ ë°œìƒ", "ì—ëŸ¬", "ì‹¤íŒ¨", "Error", "Exception", "ì°¾ì„ ìˆ˜ ì—†", "ì¤€ë¹„í•˜ì§€ ëª»", "ë¡œë”© ì—ëŸ¬"]
        is_error_message = any(pattern in answer for pattern in error_patterns)

        try:
            from backend.evaluation import AgentEvaluator

            # í‰ê°€ ìƒëµ ì¡°ê±´
            if len(answer) < 20:
                print("í‰ê°€ ìƒëµ: ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ")
            elif is_error_message:
                print("í‰ê°€ ìƒëµ: ì—ëŸ¬ ë©”ì‹œì§€")
            else:
                # í‰ê°€ ì‹¤í–‰ (RDB ê²€ì¦ í•„ìˆ˜!)
                evaluator = AgentEvaluator(
                    judge_model="gpt-4o-mini",
                    sql_store=sql_store  # âœ… RDB ê²€ì¦ì„ ìœ„í•´ í•„ìˆ˜ ì „ë‹¬
                )

                # context ì¶”ì¶œ (agent_logì—ì„œ)
                context = response.get("agent_log", {}).get("context", "")
                if isinstance(context, list):
                    context = "\n\n".join(context)

                evaluation_scores = evaluator.evaluate_single(
                    question=request.message,
                    answer=answer,
                    context=context,
                    metrics=["faithfulness", "groundness", "relevancy", "correctness"]
                )

                # ë¡œê·¸ ì¶œë ¥
                if evaluation_scores:
                    print(f"\n{'='*60}")
                    print(f"í‰ê°€ ê²°ê³¼ (í‰ê· : {evaluation_scores.get('average_score', 0)}/5)")
                    print(f"{'='*60}")
                    for metric, result in evaluation_scores.items():
                        # average_scoreëŠ” ê±´ë„ˆëœ€ (floatì´ë¯€ë¡œ .get() ë©”ì„œë“œ ì—†ìŒ)
                        if metric == "average_score":
                            continue

                        score = result.get("score", 0)
                        reasoning = result.get("reasoning", "")
                        print(f"\n[{metric.upper()}]")
                        print(f"  ì ìˆ˜: {score}/5")
                        print(f"  ì´ìœ : {reasoning}")

                        # RDB ê²€ì¦ ê²°ê³¼ ì¶œë ¥
                        if "rdb_verification" in result:
                            rdb = result["rdb_verification"]
                            print(f"  ğŸ“Š RDB ê²€ì¦: ì •í™•ë„ {rdb.get('accuracy_rate', 0)}% ({rdb.get('verified_citations', 0)}/{rdb.get('total_citations', 0)})")
                    print(f"{'='*60}\n")

        except ImportError:
            print("í‰ê°€ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ (ì„ íƒì  ê¸°ëŠ¥)")
        except Exception as eval_error:
            print(f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {eval_error}")
            evaluation_scores = None

        return {
            "session_id": request.session_id,
            "answer": answer,
            "sources": [],
            "agent_log": response,
            "evaluation_scores": evaluation_scores
        }
    except Exception as e:
        print(f" [Agent] ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))



@app.get("/chat/history/{session_id}")
def get_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    history = chat_histories.get(session_id, [])
    return {"session_id": session_id, "history": history, "count": len(history)}


@app.delete("/chat/history/{session_id}")
def clear_chat_history(session_id: str):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    if session_id in chat_histories:
        del chat_histories[session_id]
        return {"success": True, "message": f"ì„¸ì…˜ {session_id} ì‚­ì œë¨"}
    return {"success": False, "message": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/llm/generate")
def generate_llm(request: LLMRequest):
    """LLM ì§ì ‘ í˜¸ì¶œ"""
    try:
        response = get_llm_response(
            prompt=request.prompt,
            llm_model=request.model,
            llm_backend=request.backend,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        return {"response": response, "model": request.model, "backend": request.backend}
    except Exception as e:
        raise HTTPException(500, f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - ë¬¸ì„œ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/rag/documents")
def list_documents(collection: str = "documents"):
    """ë¬¸ì„œ ëª©ë¡"""
    docs = vector_store.list_documents(collection)
    return {"documents": docs, "collection": collection}


@app.delete("/rag/document")
def delete_document(request: DeleteDocRequest):
    """
     ë¬¸ì„œ ì‚­ì œ (Weaviate + Neo4j ë™ì‹œ ì‚­ì œ)
    """
    result = {"chromadb": None, "neo4j": None}
    
    # 1. Weaviate ì‚­ì œ
    chroma_result = vector_store.delete_by_doc_name(
        doc_name=request.doc_name,
        collection_name=request.collection
    )
    result["weaviate"] = chroma_result
    
    # 2. Neo4j ì‚­ì œ (ì˜µì…˜)
    if request.delete_from_neo4j:
        try:
            graph = get_graph_store()
            if graph.test_connection():
                # doc_nameì—ì„œ doc_id ì¶”ì¶œ ì‹œë„
                import re
                sop_match = re.search(r'(EQ-SOP-\d+)', request.doc_name, re.IGNORECASE)
                if sop_match:
                    doc_id = sop_match.group(1).upper()
                    neo4j_result = graph.delete_document(doc_id)
                    result["neo4j"] = {"success": True, "doc_id": doc_id, "deleted": neo4j_result}
                else:
                    result["neo4j"] = {"success": False, "message": "SOP IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ"}
        except Exception as e:
            result["neo4j"] = {"success": False, "error": str(e)}
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    success = chroma_result.get("success", False)
    
    return {
        "success": success,
        "doc_name": request.doc_name,
        "details": result
    }


@app.get("/rag/collections")
def list_collections():
    """ì»¬ë ‰ì…˜ ëª©ë¡"""
    collections = vector_store.list_collections()
    return {"collections": [vector_store.get_collection_info(name) for name in collections]}


@app.delete("/rag/collection/{collection_name}")
def delete_collection(collection_name: str):
    """ì»¬ë ‰ì…˜ ì‚­ì œ"""
    return vector_store.delete_all(collection_name)


@app.get("/rag/supported-formats")
def get_supported_formats():
    """ì§€ì› í¬ë§·"""
    return {"supported_extensions": get_supported_extensions()}


@app.get("/rag/chunk-methods")
def get_chunk_methods():
    """ì²­í‚¹ ë°©ë²•"""
    return {"methods": get_available_methods()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - Neo4j ê·¸ë˜í”„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/status")
def graph_status():
    """Neo4j ì—°ê²° ìƒíƒœ"""
    try:
        graph = get_graph_store()
        connected = graph.test_connection()
        stats = graph.get_graph_stats() if connected else {}
        return {"connected": connected, "stats": stats}
    except Exception as e:
        return {"connected": False, "error": str(e)}


@app.post("/graph/init")
def graph_init():
    """Neo4j ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
    try:
        graph = get_graph_store()
        graph.init_schema()
        return {"success": True, "message": "ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/clear")
def graph_clear():
    """Neo4j ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        graph = get_graph_store()
        graph.clear_all()
        return {"success": True, "message": "ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(500, f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.post("/graph/upload")
async def graph_upload_document(
    file: UploadFile = File(...),
    use_langgraph: bool = Form(True)
):
    """ë¬¸ì„œë¥¼ Neo4j ê·¸ë˜í”„ë¡œë§Œ ì—…ë¡œë“œ"""
    try:
        content = await file.read()
        filename = file.filename
        
        if not LANGGRAPH_AVAILABLE:
            raise HTTPException(500, "LangGraph ëª¨ë“ˆì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        result = process_document(filename, content, debug=True)
        if not result.get("success"):
            raise HTTPException(400, f"ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('errors')}")
        
        graph = get_graph_store()
        _upload_to_neo4j_from_pipeline(graph, result, filename)
        
        return {
            "success": True,
            "filename": filename,
            "doc_id": result.get("metadata", {}).get("doc_id"),
            "sections": len(result.get("sections", [])),
            "pipeline": "langgraph"
        }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ê·¸ë˜í”„ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/documents")
def graph_list_documents():
    """Neo4j ë¬¸ì„œ ëª©ë¡"""
    try:
        graph = get_graph_store()
        docs = graph.get_all_documents()
        return {"documents": docs, "count": len(docs)}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}")
def graph_get_document(doc_id: str):
    """íŠ¹ì • ë¬¸ì„œ ìƒì„¸"""
    try:
        graph = get_graph_store()
        doc = graph.get_document(doc_id)
        if not doc:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
        return doc
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.delete("/graph/document/{doc_id}")
def graph_delete_document(doc_id: str):
    """Neo4jì—ì„œ ë¬¸ì„œ ì‚­ì œ"""
    try:
        graph = get_graph_store()
        result = graph.delete_document(doc_id)
        return {"success": True, "doc_id": doc_id, "result": result}
    except Exception as e:
        raise HTTPException(500, f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}/hierarchy")
def graph_get_hierarchy(doc_id: str):
    """ë¬¸ì„œ ì„¹ì…˜ ê³„ì¸µ"""
    try:
        graph = get_graph_store()
        hierarchy = graph.get_section_hierarchy(doc_id)
        return {"doc_id": doc_id, "hierarchy": hierarchy}
    except Exception as e:
        raise HTTPException(500, f"ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/document/{doc_id}/references")
def graph_get_references(doc_id: str):
    """ë¬¸ì„œ ì°¸ì¡° ê´€ê³„"""
    try:
        graph = get_graph_store()
        refs = graph.get_document_references(doc_id)
        if not refs:
            raise HTTPException(404, f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
        return refs
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì°¸ì¡° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/sections")
def graph_search_sections(keyword: str, doc_id: str = None):
    """ì„¹ì…˜ ê²€ìƒ‰"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(keyword, doc_id)
        return {"keyword": keyword, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/search/terms")
def graph_search_terms(term: str):
    """ìš©ì–´ ê²€ìƒ‰ (ê°„ì†Œí™” ë²„ì „: ì„¹ì…˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´)"""
    try:
        graph = get_graph_store()
        results = graph.search_sections(term)
        return {"term": term, "results": results, "count": len(results)}
    except Exception as e:
        raise HTTPException(500, f"ìš©ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - Question ì¶”ì 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/graph/questions")
def graph_list_questions(limit: int = 50, session_id: str = None):
    """ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        questions = graph.get_question_history(session_id=session_id, limit=limit)
        return {"questions": questions, "count": len(questions)}
    except Exception as e:
        raise HTTPException(500, f"ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/questions/{question_id}/sources")
def graph_get_question_sources(question_id: str):
    """ì§ˆë¬¸ì´ ì°¸ì¡°í•œ ì„¹ì…˜ ì¡°íšŒ"""
    try:
        graph = get_graph_store()
        result = graph.get_question_sources(question_id)
        if not result:
            raise HTTPException(404, f"ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {question_id}")
        return result
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/graph/stats/section-usage")
def graph_section_usage_stats(doc_id: str = None):
    """ì„¹ì…˜ ì‚¬ìš© í†µê³„ (ê°„ì†Œí™”: Question íˆìŠ¤í† ë¦¬ë¡œ ëŒ€ì²´)"""
    try:
        graph = get_graph_store()
        # ê°„ì†Œí™” ë²„ì „: ì „ì²´ í†µê³„ë§Œ ì œê³µ
        stats = graph.get_graph_stats()
        return {"stats": stats}
    except Exception as e:
        raise HTTPException(500, f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  API ì—”ë“œí¬ì¸íŠ¸ - ì—ì´ì „íŠ¸ (NEW!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from backend.agent import (
        init_agent_tools, 
        run_agent, 
        AGENT_TOOLS,
        LANGCHAIN_AVAILABLE,
        LANGGRAPH_AGENT_AVAILABLE,
        ZAI_AVAILABLE
    )
    AGENT_AVAILABLE = True
    print(" ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    AGENT_AVAILABLE = False
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False
    ZAI_AVAILABLE = False
    print(f" ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class AgentRequest(BaseModel):
    """ì—ì´ì „íŠ¸ ìš”ì²­"""
    message: str
    session_id: Optional[str] = None
    llm_model: str = "gpt-4o-mini"
    embedding_model: str = "multilingual-e5-small" # ì¶”ê°€
    n_results: int = DEFAULT_N_RESULTS #  ì¶”ê°€
    use_langgraph: bool = True  # LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© ì—¬ë¶€


@app.post("/agent/chat")
def agent_chat(request: AgentRequest):
    """
     ì—ì´ì „íŠ¸ ì±„íŒ… - LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰
    
    ì¼ë°˜ RAGì™€ ë‹¤ë¥´ê²Œ ì—ì´ì „íŠ¸ê°€ ìƒí™©ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:
    - search_sop_documents: ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰
    - get_document_references: ë¬¸ì„œ ê°„ ì°¸ì¡° ê´€ê³„
    - search_sections_by_keyword: í‚¤ì›Œë“œë¡œ ì„¹ì…˜ ê²€ìƒ‰
    - get_document_structure: ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨
    - list_all_documents: ì „ì²´ ë¬¸ì„œ ëª©ë¡
    """
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    session_id = request.session_id or str(uuid.uuid4())
    
    print(f"\n{'='*50}")
    print(f"[ì—ì´ì „íŠ¸] ì§ˆë¬¸: {request.message}")
    print(f"  ì„¸ì…˜: {session_id}")
    print(f"  ëª¨ë“œ: {'LangGraph' if request.use_langgraph else 'Simple'}")
    print(f"  Orchestrator: gpt-4o-mini")
    print(f"  Worker: {request.llm_model}")

    try:
        # ë„êµ¬ ì´ˆê¸°í™” (ì²˜ìŒ í•œ ë²ˆë§Œ)
        init_agent_tools(vector_store, get_graph_store(), sql_store)
        
        # í†µí•©ëœ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = run_agent(
            query=request.message,
            session_id=session_id,
            model_name=request.llm_model,
            embedding_model=resolve_model_path(request.embedding_model)
        )
        
        reasoning = result.get("reasoning")
        answer = result.get("answer", "")

        # ë³¸ë¬¸(answer)ì´ ë¹„ì–´ìˆëŠ”ë° reasoningë§Œ ìˆëŠ” ê²½ìš° (í† í° í•œë„ ì´ˆê³¼ ë“±ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ)
        if not answer and reasoning:
            print(" ë³¸ë¬¸ì´ ì§ì ‘ì ìœ¼ë¡œ ìˆ˜ì‹ ë˜ì§€ ì•Šì•„ ì‚¬ê³  ê³¼ì •(Reasoning)ì„ ë‹µë³€ìœ¼ë¡œ ìµœìš°ì„  ë…¸ì¶œí•©ë‹ˆë‹¤.")
            result["answer"] = f"[AI ë¶„ì„ ë¦¬í¬íŠ¸]\n\n{reasoning}"
            answer = result["answer"]
        
        if reasoning:
            print(f" ëª¨ë¸ì˜ ìƒê°(Reasoning) ì¶”ì¶œë¨ ({len(reasoning)}ì)")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì²« 100ì ì •ë„ ì¶œë ¥
            reasoning_preview = reasoning[:150].replace('\n', ' ')
            print(f"   [THINK] {reasoning_preview}...")
        
        print(f"   ë„êµ¬ í˜¸ì¶œ: {len(result.get('tool_calls', []))}íšŒ")
        print(f"   ë‹µë³€ ê¸¸ì´: {len(result.get('answer', ''))} ê¸€ì")
        print(f"{'='*50}\n")
        
        return {
            "session_id": session_id,
            "answer": result.get("answer", ""),
            "tool_calls": result.get("tool_calls", []),
            "success": result.get("success", False),
            "mode": "langgraph" if (request.use_langgraph and LANGGRAPH_AGENT_AVAILABLE) else "simple"
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


@app.get("/agent/status")
def agent_status():
    """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    return {
        "agent_available": AGENT_AVAILABLE,
        "langchain_available": LANGCHAIN_AVAILABLE if AGENT_AVAILABLE else False,
        "langgraph_agent_available": LANGGRAPH_AGENT_AVAILABLE if AGENT_AVAILABLE else False,
        "tools": [t.name for t in AGENT_TOOLS] if AGENT_AVAILABLE else [],
        "message": "ì—ì´ì „íŠ¸ ì‚¬ìš© ê°€ëŠ¥" if AGENT_AVAILABLE else "ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"
    }


@app.get("/agent/tools")
def agent_tools():
    """ì—ì´ì „íŠ¸ ë„êµ¬ ëª©ë¡"""
    if not AGENT_AVAILABLE:
        raise HTTPException(500, "ì—ì´ì „íŠ¸ ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    tools_info = []
    for tool in AGENT_TOOLS:
        tools_info.append({
            "name": tool.name,
            "description": tool.description
        })

    return {"tools": tools_info, "count": len(tools_info)}


#  í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” ì—”ë“œí¬ì¸íŠ¸
class SimpleRequest(BaseModel):
    message: str

@app.post("/test/echo")
def test_echo(request: SimpleRequest):
    """í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì—ì½” API"""
    return {
        "session_id": str(uuid.uuid4()),
        "answer": f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {request.message}",
        "success": True
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸ - LLM as a Judge í‰ê°€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EvaluationRequest(BaseModel):
    """í‰ê°€ ìš”ì²­ ëª¨ë¸"""
    question: str
    answer: str
    context: Optional[str] = ""
    metrics: Optional[List[str]] = None  # ["faithfulness", "groundness", "relevancy", "correctness"]
    reference_answer: Optional[str] = None

@app.post("/evaluate")
def evaluate_answer(request: EvaluationRequest):
    """
    ğŸ” LLM as a Judge - ë‹µë³€ í‰ê°€ (RDB ê²€ì¦ í¬í•¨)

    í‰ê°€ ë©”íŠ¸ë¦­:
    - faithfulness: ì»¨í…ìŠ¤íŠ¸ ì¶©ì‹¤ì„± (í™˜ê° ë°©ì§€)
    - groundness: ê·¼ê±° ëª…í™•ì„±
    - relevancy: ì§ˆë¬¸ ê´€ë ¨ì„±
    - correctness: ì •í™•ì„±ê³¼ ì™„ì „ì„±

    **ë¬´ì¡°ê±´ RDBì—ì„œ ì‹¤ì œ ë¬¸ì„œë¥¼ ì¡°íšŒí•˜ì—¬ ì¸ìš© ì •í™•ì„± ê²€ì¦**
    """
    try:
        from backend.evaluation import AgentEvaluator

        # RDB ê²€ì¦ì„ ìœ„í•´ sql_store í•„ìˆ˜ ì „ë‹¬
        evaluator = AgentEvaluator(
            judge_model="gpt-4o-mini",
            sql_store=sql_store
        )

        # í‰ê°€ ì‹¤í–‰
        results = evaluator.evaluate_single(
            question=request.question,
            answer=request.answer,
            context=request.context,
            metrics=request.metrics,
            reference_answer=request.reference_answer
        )

        return {
            "success": True,
            "evaluation": results,
            "average_score": results.get("average_score", 0)
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    print("[ì‹œìŠ¤í…œ] ì´ˆê¸°í™” ì¤‘...")
    sql_store.init_db()
    
    # Neo4j ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” connect ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        get_graph_store()
    except Exception as e:
        print(f" Neo4j ì´ˆê¸° ì—°ê²° ì‹¤íŒ¨: {e}")

    # Weaviate ì—°ê²° í™•ì¸ (ì„±ê³µ ë¡œê·¸ëŠ” get_client ë‚´ë¶€ì—ì„œ ì¶œë ¥ë¨)
    try:
        wv_client = vector_store.get_client()
        if not wv_client.is_connected():
            print(" Weaviate v4 ì—°ê²° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
    except Exception as e:
        print(f" Weaviate v4 ì—°ê²° ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

    
    import uvicorn
    
    print("\n" + "=" * 60)
    print(" RAG Chatbot API v14.0 + OpenAI Agent")
    print("=" * 60)
    print(f" LLM ë°±ì—”ë“œ: OpenAI (GPT-4o-mini)")
    print(f" ì—ì´ì „íŠ¸: {' í™œì„±í™”' if AGENT_AVAILABLE else ' ë¹„í™œì„±í™”'}")
    
    if AGENT_AVAILABLE:
        print(f"   - LangChain: {'' if LANGCHAIN_AVAILABLE else ''}")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("ì£¼ìš” ê¸°ëŠ¥:")
    print("  - LangGraph ë¬¸ì„œ íŒŒì´í”„ë¼ì¸")
    print("  -  ReAct ì—ì´ì „íŠ¸ (/agent/chat)")
    print("  - Weaviate(v4) + Neo4j + PostgreSQL")
    print("  - LangSmith ì¶”ì  ì§€ì›")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)


if __name__ == "__main__":
    main()