"""
Weaviate ë²¡í„° ìŠ¤í† ì–´ - v8.0 (v4 Client ê¸°ë°˜)
- Weaviate Python Client v4 ì ìš©
- ì§€ëŠ¥í˜• ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ë° ê³ ì„±ëŠ¥ Batch ì§€ì›
- gRPC ê¸°ë°˜ì˜ ë¹ ë¥¸ ê²€ìƒ‰ êµ¬í˜„
"""

import weaviate
import weaviate.classes as wvc
from weaviate.classes.query import MetadataQuery, Filter
import numpy as np
from typing import List, Dict, Optional, Tuple, Any
import hashlib
import torch
from transformers import AutoTokenizer, AutoModel
from dataclasses import dataclass
import re
import json


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì • ìƒìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_COLLECTION = "documents"
WEAVIATE_HOST = "192.168.0.79"
WEAVIATE_PORT = 8080

# ê²€ìƒ‰ í’ˆì§ˆ ì„¤ì •
DEFAULT_SIMILARITY_THRESHOLD = 0.35
HIGH_CONFIDENCE_THRESHOLD = 0.65
MEDIUM_CONFIDENCE_THRESHOLD = 0.45
MIN_RESULTS_BEFORE_FILTER = 1

# ì„ë² ë”© ëª¨ë¸ í•„í„°ë§ ê¸°ì¤€
MAX_EMBEDDING_DIM = 1024
MAX_MEMORY_MB = 1300

# ì „ì—­ ìºì‹œ
_client: Optional[weaviate.WeaviateClient] = None
_embed_models: Dict = {}
_device: Optional[str] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë‹¨ì¼ í•­ëª©"""
    text: str
    similarity: float
    metadata: Dict
    id: str
    confidence: str

    def to_dict(self) -> Dict:
        return {
            "text": self.text,
            "similarity": self.similarity,
            "metadata": self.metadata,
            "id": self.id,
            "confidence": self.confidence,
        }


@dataclass
class SearchResponse:
    """ê²€ìƒ‰ ì‘ë‹µ ì „ì²´"""
    results: List[SearchResult]
    query: str
    total_found: int
    filtered_count: int
    quality_summary: Dict

    def to_dict(self) -> Dict:
        return {
            "results": [r.to_dict() for r in self.results],
            "query": self.query,
            "total_found": self.total_found,
            "filtered_count": self.filtered_count,
            "quality_summary": self.quality_summary,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_device() -> str:
    global _device
    if _device is None:
        _device = "cuda" if torch.cuda.is_available() else "cpu"
    return _device


def get_client() -> weaviate.WeaviateClient:
    """Weaviate v4 persistent client"""
    global _client
    if _client is None:
        try:
            # v4: connect_to_local or connect_to_custom with ConnectionParams
            # ë¡œì»¬ë§ ì¥ë¹„ì´ë¯€ë¡œ connect_to_localì— hostë§Œ ì§€ì •í•´ë„ ì¶©ë¶„í•¨
            _client = weaviate.connect_to_local(
                host=WEAVIATE_HOST,
                port=WEAVIATE_PORT,
                grpc_port=50051
            )
            print(f"âœ… Weaviate v4 ì—°ê²° ì„±ê³µ ({WEAVIATE_HOST}:{WEAVIATE_PORT})")
        except Exception as e:
            print(f"âŒ Weaviate v4 ì—°ê²° ì‹¤íŒ¨ (ê¸°ë³¸ ì—°ê²° ì‹œë„): {e}")
            # í´ë°±: ì§ì ‘ ì£¼ì†Œë¡œ ì—°ê²°
            _client = weaviate.connect_to_local(host=WEAVIATE_HOST, port=WEAVIATE_PORT)
    
    # ì—°ê²° í™•ì¸ ë£¨í‹´
    if not _client.is_connected():
        _client.connect()
        
    return _client


def get_collection_name_for_model(base_name: str, model_name: str) -> str:
    """v4ì—ì„œë„ PascalCase ê¶Œì¥ë˜ë¯€ë¡œ ê·œì¹™ ìœ ì§€"""
    safe_base = re.sub(r'[^a-zA-Z0-9]', '', base_name)
    if not safe_base:
        safe_base = "Collection"
    
    model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]
    class_name = safe_base[0].upper() + safe_base[1:] + "V4" + model_hash
    return class_name


def calculate_confidence(similarity: float) -> str:
    """ìœ ì‚¬ë„ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
    if similarity >= HIGH_CONFIDENCE_THRESHOLD:
        return "high"
    elif similarity >= MEDIUM_CONFIDENCE_THRESHOLD:
        return "medium"
    return "low"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„ë² ë”© ëª¨ë¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_embedding_model(model_name: str = "intfloat/multilingual-e5-small"):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    global _embed_models
    if model_name in _embed_models:
        return _embed_models[model_name]

    print(f"ğŸ“¦ Loading embedding model: {model_name}...")
    device = get_device()
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)
    model.eval()

    _embed_models[model_name] = (tokenizer, model)
    return tokenizer, model


def embed_text(text: str, model_name: str = "intfloat/multilingual-e5-small") -> List[float]:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© (e5 prefix ì§€ì›)"""
    tokenizer, model = get_embedding_model(model_name)
    device = get_device()

    # e5 ëª¨ë¸ ê¶Œì¥ í”„ë¦¬í”½ìŠ¤ (ê²€ìƒ‰ ì¿¼ë¦¬ ì‹œ)
    # if "e5" in model_name.lower():
    #     text = f"query: {text}"

    if len(text) > 1500: text = text[:1500]

    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)

    mask = inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()
    sum_embeddings = torch.sum(outputs.last_hidden_state * mask, 1)
    sum_mask = torch.clamp(mask.sum(1), min=1e-9)
    embedding = (sum_embeddings / sum_mask).cpu().numpy()[0]
    return embedding.tolist()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì»¬ë ‰ì…˜ (v4) ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def list_collections() -> List[str]:
    """v4: client.collections.list_all() ì‚¬ìš©"""
    client = get_client()
    cols = client.collections.list_all()
    return list(cols.keys())


def get_collection_info(collection_name: str) -> Dict:
    """v4: í´ëŸ¬ìŠ¤í„° í†µê³„ ë˜ëŠ” aggregate ì‚¬ìš©"""
    try:
        client = get_client()
        collection = client.collections.get(collection_name)
        res = collection.aggregate.over_all(total_count=True)
        return {"name": collection_name, "count": res.total_count}
    except Exception as e:
        return {"name": collection_name, "count": 0, "error": str(e)}


def delete_collection(collection_name: str) -> bool:
    """v4: client.collections.delete()"""
    try:
        client = get_client()
        client.collections.delete(collection_name)
        return True
    except Exception:
        return False


def close_client():
    """Weaviate í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ"""
    global _client
    if _client is not None:
        try:
            _client.close()
            print("ğŸ›‘ Weaviate ì—°ê²° ì¢…ë£Œë¨")
        except Exception as e:
            print(f"âš ï¸ Weaviate ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            _client = None


def ensure_collection(client: weaviate.WeaviateClient, collection_name: str):
    """v4: ìŠ¤í‚¤ë§ˆ ìƒì„± (Properties + Config)"""
    if not client.collections.exists(collection_name):
        client.collections.create(
            name=collection_name,
            properties=[
                wvc.config.Property(name="text", data_type=wvc.config.DataType.TEXT),
                wvc.config.Property(name="metadata_json", data_type=wvc.config.DataType.TEXT),
                wvc.config.Property(name="doc_name", data_type=wvc.config.DataType.TEXT),
                wvc.config.Property(name="sop_id", data_type=wvc.config.DataType.TEXT),
                wvc.config.Property(name="model", data_type=wvc.config.DataType.TEXT),
            ],
            # v4.4+ ì—ì„œëŠ” vector_config ì‚¬ìš© ê¶Œì¥
            vector_config=wvc.config.Configure.Vector.none(
                vector_index_config=wvc.config.Configure.VectorIndex.hnsw(
                    distance_metric=wvc.config.VectorDistances.COSINE
                )
            )
        )
        print(f"ğŸ†• Weaviate v4 Collection ìƒì„±ë¨: {collection_name}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° ìƒì„±/ìˆ˜ì •/ì‚­ì œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def add_documents(
    texts: List[str],
    metadatas: List[Dict],
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "intfloat/multilingual-e5-small",
) -> Dict:
    """v4: collection.data.insert_many() í™œìš©"""
    actual_name = get_collection_name_for_model(collection_name, model_name)
    client = get_client()
    ensure_collection(client, actual_name)
    
    collection = client.collections.get(actual_name)
    
    data_objects = []
    for i, text in enumerate(texts):
        meta = metadatas[i]
        vector = embed_text(text, model_name)
        
        data_objects.append(
            wvc.data.DataObject(
                properties={
                    "text": text,
                    "metadata_json": json.dumps(meta),
                    "doc_name": str(meta.get("doc_name", "")),
                    "sop_id": str(meta.get("sop_id", "")),
                    "model": model_name
                },
                vector=vector
            )
        )
    
    # ë°°ì¹˜ ì‚½ì…
    res = collection.data.insert_many(data_objects)
    
    if res.has_errors:
        print(f"âš ï¸ ì¼ë¶€ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {res.errors}")

    return {
        "success": not res.has_errors,
        "added": len(texts) - len(res.errors),
        "collection": actual_name,
    }


def add_single_text(
    text: str,
    metadata: Dict,
    collection_name: str = DEFAULT_COLLECTION,
    model_name: str = "intfloat/multilingual-e5-small",
) -> Dict:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¶”ê°€"""
    return add_documents([text], [metadata], collection_name, model_name)


def search(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 5,
    model_name: str = "intfloat/multilingual-e5-small",
    filter_doc: Optional[str] = None,
    similarity_threshold: Optional[float] = None,
    return_low_confidence: bool = False,
) -> List[Dict]:
    """v4: collection.query.near_vector() í™œìš©"""
    actual_name = get_collection_name_for_model(collection_name, model_name)
    client = get_client()

    if not client.collections.exists(actual_name): return []
    
    vector = embed_text(query, model_name)
    collection = client.collections.get(actual_name)
    
    # í•„í„° êµ¬ì„±
    filters = None
    if filter_doc:
        filters = Filter.by_property("doc_name").equal(filter_doc)
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    res = collection.query.near_vector(
        near_vector=vector,
        limit=max(n_results * 2, 10),
        filters=filters,
        return_metadata=MetadataQuery(certainty=True, distance=True)
    )

    search_results = []
    threshold = similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
    
    for obj in res.objects:
        # v4 certaintyëŠ” 0~1 (Cosine ê¸°ì¤€ 1-distance/2 ë˜ëŠ” ìœ ì‚¬)
        certainty = obj.metadata.certainty if obj.metadata.certainty is not None else 0.0
        similarity = certainty # Weaviate v4ì˜ certaintyëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìŒ
        
        if not return_low_confidence and similarity < threshold: continue
        
        try:
            meta = json.loads(obj.properties.get('metadata_json', '{}'))
        except:
            meta = obj.properties
            
        search_results.append({
            "text": obj.properties.get('text', ""),
            "similarity": round(similarity, 4),
            "metadata": meta,
            "id": str(obj.uuid),
            "confidence": calculate_confidence(similarity),
        })

    # ìµœì†Œ ê²°ê³¼ ë³´ì¥ ë£¨í‹´
    if len(search_results) < MIN_RESULTS_BEFORE_FILTER and res.objects:
        search_results = []
        for obj in res.objects[:n_results]:
            similarity = obj.metadata.certainty or 0.0
            try: meta = json.loads(obj.properties.get('metadata_json', '{}'))
            except: meta = obj.properties
            search_results.append({
                "text": obj.properties.get('text', ""),
                "similarity": round(similarity, 4),
                "metadata": meta,
                "id": str(obj.uuid),
                "confidence": calculate_confidence(similarity),
            })

    return search_results[:n_results]


def search_advanced(
    query: str,
    collection_name: str = DEFAULT_COLLECTION,
    n_results: int = 5,
    model_name: str = "intfloat/multilingual-e5-small",
    filter_doc: Optional[str] = None,
    similarity_threshold: Optional[float] = None,
    return_low_confidence: bool = False,
) -> SearchResponse:
    """í™•ì¥ ê²€ìƒ‰ (SearchResponse ê°ì²´ ë°˜í™˜)"""
    results = search(
        query, collection_name, n_results, model_name, 
        filter_doc, similarity_threshold, return_low_confidence
    )
    
    # ë°ì´í„° í´ë˜ìŠ¤ ë³€í™˜
    structured_results = [SearchResult(**r) for r in results]
    
    return SearchResponse(
        results=structured_results,
        query=query,
        total_found=len(structured_results),
        filtered_count=0,
        quality_summary={
            "high": len([r for r in structured_results if r.confidence == "high"]),
            "medium": len([r for r in structured_results if r.confidence == "medium"]),
            "low": len([r for r in structured_results if r.confidence == "low"]),
        }
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í˜¸í™˜ì„±/ê´€ë¦¬ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def delete_by_doc_name(doc_name: str, collection_name: str = DEFAULT_COLLECTION, model_name: Optional[str] = None) -> Dict:
    """v4: collection.data.delete_many() í™œìš©"""
    actual_classes = [get_collection_name_for_model(collection_name, model_name)] if model_name else [c for c in list_collections() if c.startswith(collection_name)]
    
    client = get_client()
    deleted_total = 0
    
    for cls in actual_classes:
        col = client.collections.get(cls)
        res = col.data.delete_many(where=Filter.by_property("doc_name").equal(doc_name))
        deleted_total += res.successful
        
    return {"success": deleted_total > 0, "deleted": deleted_total}


def delete_all(collection_name: str = DEFAULT_COLLECTION, model_name: Optional[str] = None) -> Dict:
    """ì „ì²´ ì‚­ì œ"""
    actual_classes = [get_collection_name_for_model(collection_name, model_name)] if model_name else [c for c in list_collections() if c.startswith(collection_name)]
    deleted = []
    for cls in actual_classes:
        if delete_collection(cls): deleted.append(cls)
    return {"success": len(deleted) > 0, "deleted_collections": deleted}


def list_documents(collection_name: str = DEFAULT_COLLECTION, model_name: Optional[str] = None) -> List[Dict]:
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (v4 iterator í™œìš©)"""
    # v4ì—ì„œëŠ” PascalCaseë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë§¤ì¹­
    all_cols = list_collections()
    
    if model_name:
        actual_classes = [get_collection_name_for_model(collection_name, model_name)]
    else:
        # prefix ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        prefix = collection_name.lower()
        actual_classes = [c for c in all_cols if c.lower().startswith(prefix)]
    
    docs = {}
    client = get_client()
    
    for cls in actual_classes:
        col = client.collections.get(cls)
        # 1000ê°œë§Œ ìƒ˜í”Œë§í•˜ì—¬ ëª©ë¡í™”
        for obj in col.iterator(include_vector=False):
            doc_name = obj.properties.get('doc_name', 'unknown')
            model = obj.properties.get('model', 'unknown')
            key = f"{doc_name}|{model}"
            
            if key not in docs:
                try: meta = json.loads(obj.properties.get('metadata_json', '{}'))
                except: meta = {}
                docs[key] = {
                    "doc_name": doc_name,
                    "doc_title": meta.get('doc_title') or meta.get('title'),
                    "chunk_count": 0,
                    "model": model,
                    "collection": cls
                }
            docs[key]["chunk_count"] += 1
            if len(docs) > 500: break # ì„±ëŠ¥ ë°©ì–´
            
    return list(docs.values())


# search_advanced, search_with_context ë“±ì€ v3ì™€ ë¡œì§ì´ ìœ ì‚¬í•˜ë¯€ë¡œ ìƒëµí•˜ê±°ë‚˜ 
# search()ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ í™œìš©í•˜ë„ë¡ ìœ ì§€í•˜ë©´ ë©ë‹ˆë‹¤.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def search_with_context(query: str, collection_name: str = DEFAULT_COLLECTION, n_results: int = 3, model_name: str = "intfloat/multilingual-e5-small", filter_doc: Optional[str] = None, similarity_threshold: Optional[float] = None) -> Tuple[List[Dict], str]:
    results = search(query, collection_name, n_results, model_name, filter_doc, similarity_threshold)
    context_parts = []
    for i, r in enumerate(results):
        meta = r.get('metadata', {})
        header = f"[{meta.get('doc_name', 'Unknown')} > {meta.get('title', 'No Title')}] (ìœ ì‚¬ë„: {r['similarity']:.1%})"
        context_parts.append(f"{header}\n{r['text']}")
    return results, "\n\n---\n\n".join(context_parts)

EMBEDDING_MODEL_SPECS = {
    "intfloat/multilingual-e5-small": {
        "name": "multilingual-e5-small", "dim": 384, "memory_mb": 120, "lang": "multi",
    },
}


def filter_compatible_models() -> List[Dict]:
    """í˜¸í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return [
        {"path": path, **spec}
        for path, spec in EMBEDDING_MODEL_SPECS.items()
        if spec['dim'] <= MAX_EMBEDDING_DIM and spec['memory_mb'] <= MAX_MEMORY_MB
    ]
def get_embedding_model_info(model_path: str) -> Dict:
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    return EMBEDDING_MODEL_SPECS.get(model_path, {"name": "unknown", "dim": 0})

def is_model_compatible(model_path: str) -> bool:
    """ëª¨ë¸ì´ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸"""
    spec = EMBEDDING_MODEL_SPECS.get(model_path)
    if not spec: return False
    return spec['dim'] <= MAX_EMBEDDING_DIM and spec['memory_mb'] <= MAX_MEMORY_MB
