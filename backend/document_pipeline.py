"""
ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ v2.0 - PDF ì¡°í•­ ë‹¨ìœ„ íŒŒì‹±

ë‹¨ê³„:
1. PDF â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
2. ì¡°í•­ íŒŒì‹± (ì •ê·œì‹, evaluate_gmp_unified ë°©ì‹)
3. ì¡°í•­ë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (LLM)
4. ì²­í¬ ìƒì„±
"""

import os
import re
import json
from typing import List, Dict
from pathlib import Path
from io import BytesIO
from .llm import get_llm_response


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ë‹¨ê³„: PDF â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pdf_to_markdown(pdf_content: bytes) -> str:
    """
    PDFë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (ë‹¤ì¤‘ íŒŒì„œ í´ë°±)

    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
    """
    # 1ìˆœìœ„: pdfplumber (ê°€ì¥ ì•ˆì •ì )
    try:
        import pdfplumber
        md_lines = []
        with pdfplumber.open(BytesIO(pdf_content)) as pdf:
            for i, page in enumerate(pdf.pages):
                text = page.extract_text() or ''
                if text.strip():
                    md_lines.append(f"<!-- PAGE:{i + 1} -->")
                    md_lines.append(text)

        markdown = '\n'.join(md_lines)
        if len(markdown.strip()) > 100:
            print("    âœ“ pdfplumberë¡œ ë³€í™˜ ì™„ë£Œ")
            return markdown
    except Exception as e:
        print(f"    pdfplumber ì‹¤íŒ¨: {e}")

    # 2ìˆœìœ„: PyMuPDF
    try:
        import fitz
        pdf = fitz.open(stream=pdf_content, filetype="pdf")
        md_lines = []
        for page_num, page in enumerate(pdf):
            text = page.get_text()
            if text.strip():
                md_lines.append(f"<!-- PAGE:{page_num + 1} -->")
                md_lines.append(text)

        markdown = '\n'.join(md_lines)
        if len(markdown.strip()) > 100:
            print("    âœ“ PyMuPDFë¡œ ë³€í™˜ ì™„ë£Œ")
            return markdown
    except Exception as e:
        print(f"    PyMuPDF ì‹¤íŒ¨: {e}")

    raise Exception("PDF ë³€í™˜ ì‹¤íŒ¨ (ëª¨ë“  íŒŒì„œ ì‹¤íŒ¨)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°: PDF ë…¸ì´ì¦ˆ ì œê±°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _clean_noise_globally(markdown: str) -> str:
    """
    ì „ì²´ ë¬¸ì„œì—ì„œ ë°˜ë³µë˜ëŠ” í–‰(í—¤ë”/í‘¸í„°)ì„ ë™ì ìœ¼ë¡œ íƒì§€í•˜ê³  ì œê±°í•©ë‹ˆë‹¤.
    (í•˜ë“œì½”ë”©ëœ ì •ê·œì‹ì„ ìµœì†Œí™”í•˜ê³  ë¹ˆë„ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘)
    """
    if not markdown:
        return ""

    # 1. í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    # markersëŠ” ë³´ì¡´í•˜ë©´ì„œ ë¶„í• 
    parts = re.split(r'(<!-- PAGE:\d+ -->)', markdown)
    page_data = []
    for i in range(1, len(parts), 2):
        marker = parts[i]
        content = parts[i+1] if i+1 < len(parts) else ""
        page_num_match = re.search(r'\d+', marker)
        page_num = int(page_num_match.group()) if page_num_match else 0
        page_data.append({"marker": marker, "page": page_num, "content": content})

    if not page_data:
        # ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ìµœì†Œí•œì˜ í˜ê¹Œì§€ë§Œ ì²˜ë¦¬
        return re.sub(r'<!-- PAGE:\d+ -->', '', markdown).strip()

    # 2. í–‰ ë¹ˆë„ ë¶„ì„ (ìƒë‹¨/í•˜ë‹¨ 5í–‰ ì¤‘ì‹¬)
    # ìˆ«ìë¥¼ #ìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ "Page 1 of 10"ê³¼ "Page 2 of 10"ì„ ë™ì¼í•œ í…œí”Œë¦¿ìœ¼ë¡œ ì¸ì‹
    line_templates = {}
    for p in page_data:
        lines = [l.strip() for l in p["content"].split('\n') if l.strip()]
        candidates = set(lines[:5] + lines[-5:])
        for line in candidates:
            # íŠ¹ìˆ˜ ê¸°í˜¸ë‚˜ ìˆ«ìê°€ í¬í•¨ëœ ë¬¸íŒ¨í„´ì„ í…œí”Œë¦¿í™”
            template = re.sub(r'\d+', '#', line)
            line_templates[template] = line_templates.get(template, 0) + 1

    # 3. 30% ì´ìƒì˜ í˜ì´ì§€ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í…œí”Œë¦¿ì„ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼
    total_pages = len(page_data)
    noise_threshold = max(2, total_pages * 0.3)
    noise_templates = {t for t, count in line_templates.items() if count >= noise_threshold}

    # ë””ë²„ê·¸: ë…¸ì´ì¦ˆë¡œ íŒì •ëœ í…œí”Œë¦¿ ì¶œë ¥ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # if noise_templates:
    #     print(f"    [ë…¸ì´ì¦ˆ ì œê±°] {len(noise_templates)}ê°œ í…œí”Œë¦¿ ì œê±° (ì„ê³„ê°’: {noise_threshold:.1f}í˜ì´ì§€)")
    #     for template in list(noise_templates)[:10]:  # ì²˜ìŒ 10ê°œ
    #         print(f"      - {template[:70]}")

    # 4. ë…¸ì´ì¦ˆ ì œê±° ë° ë³¸ë¬¸ ì¬êµ¬ì„±
    # ì¡°í•­ íŒ¨í„´ì„ ê°€ì§„ ë¼ì¸ì€ ë³´ì¡´ (ë‹¨, "1 of 11" ê°™ì€ í˜ì´ì§€ ë²ˆí˜¸ëŠ” ì œì™¸)
    # "1." (ì„¹ì…˜ í—¤ë”), "1.1" (ì¡°í•­), ì¡°í•­ë§Œ ìˆëŠ” ê²½ìš° ëª¨ë‘ ë§¤ì¹­
    clause_pattern = re.compile(r'^\s*(\d+(?:\.\d+)*\.?)(?:\s+(?!of\s+\d)|$)')

    preserved_clauses = 0
    removed_lines = 0

    cleaned_parts = []
    for p in page_data:
        lines = p["content"].split('\n')
        cleaned_lines = []
        for l in lines:
            stripped = l.strip()
            if not stripped:
                cleaned_lines.append("")
                continue

            # ì¡°í•­ íŒ¨í„´ì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë³´ì¡´
            if clause_pattern.match(stripped):
                cleaned_lines.append(l)
                preserved_clauses += 1
                continue

            template = re.sub(r'\d+', '#', stripped)
            if template in noise_templates:
                removed_lines += 1
                continue
            cleaned_lines.append(l)

        # í˜ì´ì§€ ë§ˆì»¤ëŠ” ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ ì¼ë‹¨ ìœ ì§€
        cleaned_parts.append(p["marker"] + "\n" + "\n".join(cleaned_lines))

    # ë””ë²„ê·¸ ë¡œê¹… (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # print(f"    [ë…¸ì´ì¦ˆ í•„í„°] ì¡°í•­ ë³´ì¡´: {preserved_clauses}ê°œ, ë…¸ì´ì¦ˆ ì œê±°: {removed_lines}ê°œ")

    result = "\n".join(cleaned_parts)
    
    # 5. ì „í˜•ì ì¸ ë¬¸ì„œ ì¢…ë£Œ ë§ˆì»¤ ë“± ìµœì†Œí•œì˜ ì •ì  ì •ì œ
    result = re.sub(r'\*{3,}\s*END OF DOCUMENT\s*\*{3,}', '', result, flags=re.IGNORECASE)
    
    return result.strip()


def _normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”: ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°, í•œì˜ ë¬¸ë‹¨ êµ¬ë¶„ ìœ ì§€"""

    # 1. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ -> 2ê°œ)
    text = re.sub(r'\n{3,}', '\n\n', text)

    # 2. ë¬¸ë‹¨ë³„ ì²˜ë¦¬
    paragraphs = text.split('\n\n')
    normalized_paragraphs = []

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # í•œê¸€ ë¹„ìœ¨ ì²´í¬ (í•œê¸€ ë¬¸ë‹¨ vs ì˜ì–´ ë¬¸ë‹¨ êµ¬ë¶„)
        korean_chars = len(re.findall(r'[ê°€-í£]', para))
        total_chars = len(re.sub(r'\s', '', para))
        is_korean = korean_chars > total_chars * 0.3  # 30% ì´ìƒì´ í•œê¸€

        # ê°™ì€ ì–¸ì–´ ë¬¸ë‹¨ ë‚´ì—ì„œë§Œ ì¤„ë°”ê¿ˆ ì œê±°
        # ë‹¨, ë¬¸ì¥ ë(ë§ˆì¹¨í‘œ ë“±)ì€ ìœ ì§€
        if '\n' in para:
            # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
            normalized = para.replace('\n', ' ')
            # ì—°ì† ê³µë°± ì œê±°
            normalized = re.sub(r'\s+', ' ', normalized)
            normalized_paragraphs.append(normalized.strip())
        else:
            normalized_paragraphs.append(para)

    # 3. ë¬¸ë‹¨ ì¬ê²°í•© (í•œì˜ êµ¬ë¶„ì€ \n\n ìœ ì§€)
    return '\n\n'.join(normalized_paragraphs)


def _split_recursive(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë¶„í•  (ë¬¸ì¥, ì¤„ë°”ê¿ˆ ê¸°ì¤€)"""
    if len(text) <= chunk_size:
        return [text]

    separators = ["\n\n", "\n", ". ", " ", ""]
    chunks = []

    parts = []
    for sep in separators:
        if sep in text:
            parts = text.split(sep)
            break

    if not parts:
        return [text[:chunk_size]]

    current_chunk = ""
    for p in parts:
        if len(current_chunk) + len(p) < chunk_size:
            current_chunk += p + (sep if sep != "" else "")
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = p + (sep if sep != "" else "")

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ë‹¨ê³„: ì¡°í•­ íŒŒì‹± (evaluate_gmp_unified ë°©ì‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_clauses(markdown: str, max_level: int = None) -> List[Dict]:
    """
    ë§ˆí¬ë‹¤ìš´ì—ì„œ ì¡°í•­ ì¶”ì¶œ (evaluate_gmp_unified.pyì˜ load_pdf_by_clause ë°©ì‹)

    ì¡°í•­ íŒ¨í„´: 5.1.2 í˜•ì‹ì˜ ìˆ«ì ë²ˆí˜¸

    Args:
        markdown: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        max_level: ìµœëŒ€ ì¡°í•­ ê¹Šì´ (Noneì´ë©´ ëª¨ë“  ë ˆë²¨ í¬í•¨)

    Returns:
        List[Dict]: ì¡°í•­ ë¦¬ìŠ¤íŠ¸
            - clause: ì¡°í•­ ë²ˆí˜¸ (ì˜ˆ: "5.1.2")
            - title: ì¡°í•­ ì œëª©
            - content: ì¡°í•­ ë‚´ìš©
            - level: ê¹Šì´ (0ë¶€í„° ì‹œì‘)
    """
    # ë³¼ë“œ ë§ˆì»¤ ì œê±° (ë…¸ì´ì¦ˆ ë¶„ì„ ì „ì— ë¨¼ì € ì‹¤í–‰)
    # **1** **ëª©ì  Purpose** â†’ 1 ëª©ì  Purpose
    # ì´ë ‡ê²Œ í•˜ë©´ ë…¸ì´ì¦ˆ í…œí”Œë¦¿ ë§¤ì¹­ì—ì„œ ì„¹ì…˜ í—¤ë”ê°€ ì œê±°ë˜ì§€ ì•ŠìŒ
    markdown = re.sub(r'\*\*', '', markdown)  # ëª¨ë“  ë³¼ë“œ ë§ˆì»¤ ì œê±°

    # ë™ì  ë…¸ì´ì¦ˆ ì œê±° (ë¹ˆë„ ë¶„ì„ ê¸°ë°˜)
    markdown = _clean_noise_globally(markdown)

    # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±° (ì¡°í•­ìœ¼ë¡œ ì˜ëª» ì¸ì‹ë˜ëŠ” ê²ƒ ë°©ì§€)
    # ì˜ˆ: "1 of 11", "2 of 11", "Page 1", "í˜ì´ì§€ 1" ë“±
    page_number_patterns = [
        r'(?m)^\s*(\d+)\s+of\s+\d+.*?$',  # "1 of 11 Number: ..." í˜•ì‹
        r'(?m)^\s*(?:Page|í˜ì´ì§€)\s+\d+.*?$',  # "Page 1", "í˜ì´ì§€ 1" í˜•ì‹
        r'(?m)^\s*\d+\s*/\s*\d+.*?$',  # "1/11" í˜•ì‹
    ]
    for pattern in page_number_patterns:
        markdown = re.sub(pattern, '', markdown, flags=re.MULTILINE)

    # ê°œì •ì´ë ¥/ë³€ê²½ì´ë ¥ ì„¹ì…˜ ì œê±° (ì œëª© í–‰ë§Œ ì œê±°)
    # ì£¼ì˜: ì´ì „ì— re.DOTALLì„ ì‚¬ìš©í•˜ë©´ ë¬¸ì„œ ëê¹Œì§€ ëª¨ë‘ ì œê±°ë˜ë¯€ë¡œ ì œê±°í•¨
    revision_patterns = [
        r'(?im)^(ê°œì •\s*ì´ë ¥|ë³€ê²½\s*ì´ë ¥|Revision\s+History|Change\s+History)[^\n]*\n?',
        r'(?im)^(\d+\.\d+\s+ì „ì²´\s+ë³€ê²½ê´€ë¦¬)[^\n]*\n?'
    ]
    for pattern in revision_patterns:
        markdown = re.sub(pattern, '', markdown)

    # ì¡°í•­ ë²ˆí˜¸ íŒ¨í„´ (ë§¤ìš° ê´€ëŒ€í•˜ê²Œ)
    # "1." (ì„¹ì…˜ í—¤ë”) ë° "1.1" (ì¡°í•­) ëª¨ë‘ ë§¤ì¹­í•˜ë„ë¡ trailing dot í—ˆìš©
    pattern = r'(?:^|\n)\s*(\d+(?:\.\d+)*\.?)\s+'
    all_matches = list(re.finditer(pattern, markdown))

    print(f"    ë””ë²„ê·¸: {len(all_matches)}ê°œ íŒ¨í„´ ë°œê²¬")

    clauses = []
    filtered_count = 0
    filtered_details = []  # ì œì™¸ëœ ì¡°í•­ ìƒì„¸ ì •ë³´

    for i, m in enumerate(all_matches):
        clause_num = m.group(1)
        level = clause_num.count(".")

        # í•´ë‹¹ ì¡°í•­ì´ ìœ„ì¹˜í•œ í˜ì´ì§€ ì°¾ê¸°
        page_num = 1
        page_marker_before = re.findall(r'<!-- PAGE:(\d+) -->', markdown[:m.start()])
        if page_marker_before:
            page_num = int(page_marker_before[-1])

        # ë ˆë²¨ í•„í„°ë§
        if max_level is not None and level > max_level:
            filtered_count += 1
            filtered_details.append(f"{clause_num}: ë ˆë²¨ ì´ˆê³¼")
            continue

        # ë‚´ìš© ì¶”ì¶œ
        start = m.end()
        end = all_matches[i+1].start() if i+1 < len(all_matches) else len(markdown)
        content = markdown[start:end]

        # ë³¸ë¬¸ ë‚´ì˜ í˜ì´ì§€ ë§ˆì»¤ ì œê±°
        content = re.sub(r'<!-- PAGE:\d+ -->', '', content).strip()

        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(content) < 5:
            filtered_count += 1
            filtered_details.append(f"{clause_num}: ë‚´ìš© ë„ˆë¬´ ì§§ìŒ ({len(content)}ì)")
            continue

        # ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬
        # v8.5: ì œëª©ì´ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° ëŒ€ë¹„ - ì²« ë¬¸ì¥ ë˜ëŠ” 200ìê¹Œì§€

        # 1. ë§ˆì¹¨í‘œë‚˜ ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì²« ë¬¸ì¥ ì°¾ê¸°
        sentence_ends = ['. ', '.\n', ': ', ':\n']
        title_end = -1

        for end_marker in sentence_ends:
            pos = content.find(end_marker)
            if pos > 0 and pos < 300:  # 300ì ì´ë‚´ì˜ ë§ˆì¹¨í‘œ/ì½œë¡ 
                title_end = pos + 1
                break

        if title_end > 0:
            # ë§ˆì¹¨í‘œ/ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì²« ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ
            title = content[:title_end].replace('\n', ' ').strip()
            body = content[title_end:].strip()
        else:
            # ë§ˆì¹¨í‘œê°€ ì—†ìœ¼ë©´ ì²« 200ìë¥¼ ì œëª©ìœ¼ë¡œ
            first_newline = content.find('\n\n')  # ë¬¸ë‹¨ êµ¬ë¶„
            if 0 < first_newline < 200:
                title = content[:first_newline].replace('\n', ' ').strip()
                body = content[first_newline:].strip()
            else:
                title = content[:200].replace('\n', ' ').strip()
                body = content[200:].strip() if len(content) > 200 else ""

        # ë³¸ë¬¸ì—ì„œ ë…¸ì´ì¦ˆ ì œê±°
        # body = _clean_pdf_noise(body) # ì „ì—­ ì •ì œë¡œ ëŒ€ì²´ë¨

        # í•„í„°ë§: ì˜ëª»ëœ ë§¤ì¹­ ì œê±°
        skip = False
        skip_reason = ""

        # 1. ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸
        if len(title) < 2:
            filtered_count += 1
            skip = True
            skip_reason = f"ì œëª© ë„ˆë¬´ ì§§ìŒ: '{title}'"

        # 2. ë²„ì „ ë²ˆí˜¸ (1.0 ì „ì²´ ë³€ê²½...) ì œì™¸ - ë” ì •ë°€í•œ ì¡°ê±´
        elif re.match(r'^\d+\.\d+$', clause_num) and level == 1:
            # ì œëª©ì´ "ë²„ì „", "ê°œì • ë‚´ì—­", "ë³€ê²½ ì´ë ¥" ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë§Œ ì œì™¸
            if re.match(r'^(ë²„ì „|ê°œì •\s*ë‚´ì—­|ë³€ê²½\s*ì´ë ¥|Revision\s+History|Change\s+Log|Version)\b', title, re.IGNORECASE):
                filtered_count += 1
                skip = True
                skip_reason = f"ë²„ì „ ë²ˆí˜¸ íŒ¨í„´: '{title}'"

        # 3. ê´„í˜¸ë¡œë§Œ êµ¬ì„±ëœ ì œëª© ì œì™¸
        elif re.match(r'^\([^\)]+\)\s*$', title):
            filtered_count += 1
            skip = True
            skip_reason = f"ê´„í˜¸ë§Œ: '{title}'"

        # 4. ìˆ«ìì™€ ê¸°í˜¸ë§Œ ìˆëŠ” ì œëª© ì œì™¸
        elif re.match(r'^[\d\s\(\)\-\.,]+$', title):
            filtered_count += 1
            skip = True
            skip_reason = f"ìˆ«ì/ê¸°í˜¸ë§Œ: '{title}'"

        # 5. "characters", "numbers" ê°™ì€ ì„¤ëª… í…ìŠ¤íŠ¸ ì œì™¸
        elif re.match(r'^(characters|numbers|digits|letters|Level)\b', title, re.IGNORECASE):
            filtered_count += 1
            skip = True
            skip_reason = f"ì„¤ëª… í…ìŠ¤íŠ¸: '{title}'"

        # 6. "of" ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì œì™¸
        elif re.match(r'^(of\s+\d+|page\s+\d+)', title, re.IGNORECASE):
            filtered_count += 1
            skip = True
            skip_reason = f"í˜ì´ì§€ ë²ˆí˜¸: '{title}'"

        if skip:
            filtered_details.append(f"{clause_num}: {skip_reason}")
            continue

        clauses.append({
            "clause": clause_num,
            "title": title,
            "content": body,
            "level": level,
            "page": page_num
        })

    print(f"    âœ“ {len(clauses)}ê°œ ì¡°í•­ ì¶”ì¶œ")

    if filtered_count > 0:
        print(f"    âš  {filtered_count}ê°œ ì¡°í•­ ì œì™¸ë¨:")
        for detail in filtered_details[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
            print(f"      - {detail}")
        if len(filtered_details) > 10:
            print(f"      ... ì™¸ {len(filtered_details) - 10}ê°œ")

    return clauses


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (evaluate_gmp_unified ë°©ì‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_metadata(content: str, clause_id: str, title: str,
                     embed_model=None) -> Dict:
    """
    ì¡°í•­ë³„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (LLM ì‚¬ìš©)

    Returns:
        Dict: ë©”íƒ€ë°ì´í„°
            - content_type, main_topic, actors, actions, ...
            - intent_summary, intent_embedding
    """
    if len(content.strip()) < 30:
        return _default_metadata()

    prompt = f"""ë‹¹ì‹ ì€ GMP ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì„œ ì²­í¬ë¥¼ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ì²­í¬ ì •ë³´
- ì¡°í•­ë²ˆí˜¸: {clause_id}
- ì œëª©: {title}
- ë‚´ìš©:
{content[:1000]}

## ì¶”ì¶œí•  ë©”íƒ€ë°ì´í„°
ë‹¤ìŒ í•­ëª©ë“¤ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:

1. content_type: ì´ ì²­í¬ê°€ ì„¤ëª…í•˜ëŠ” ë‚´ìš©ì˜ ìœ í˜• (ì˜ˆ: ëª©ì , ì •ì˜, ì±…ì„, ì ˆì°¨, ê¸°ì¤€, ê¸°ë¡, ì°¸ê³ ë¬¸í—Œ ë“±)
2. main_topic: ì´ ì²­í¬ì˜ í•µì‹¬ ì£¼ì œ
3. sub_topics: ê´€ë ¨ ì„¸ë¶€ ì£¼ì œë“¤ (ë¦¬ìŠ¤íŠ¸, ìµœëŒ€ 3ê°œ)
4. actors: ì–¸ê¸‰ëœ ì—­í• ì/ë‹´ë‹¹ì (ë¦¬ìŠ¤íŠ¸)
5. actions: ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” í–‰ìœ„/ì ˆì°¨ (ë¦¬ìŠ¤íŠ¸, ìµœëŒ€ 3ê°œ)
6. conditions: íŠ¹ìˆ˜ ì¡°ê±´ì´ë‚˜ ìƒí™© (ë¦¬ìŠ¤íŠ¸)
7. summary: í•œ ë¬¸ì¥ ìš”ì•½ (30ì ì´ë‚´)
8. intent_scope: ì´ ì¡°í•­ì´ ë‹¤ë£¨ëŠ” ê´€ë¦¬ ì˜ì—­ (ë‹¤ìŒ ì¤‘ 1ê°œë§Œ ì„ íƒ)
   - user_account: ì‚¬ìš©ì ê³„ì •Â·ê¶Œí•œÂ·ì—­í•  ê´€ë¦¬
   - document_lifecycle: ë¬¸ì„œì˜ ìˆ˜ëª…ì£¼ê¸° (ì‘ì„±, ìŠ¹ì¸, ê°œì •, íê¸° ë“±)
   - system_configuration: ì‹œìŠ¤í…œ ì„¤ì • or êµ¬ì¡° ë³€ê²½
   - audit_evidence: ê°ì‚¬ëŒ€ì‘ì— ê´€ë ¨í•œ ìë£Œ
   - training: êµìœ¡, í›ˆë ¨, ìê²©
9. intent_summary: ì´ ì¡°í•­ì´ ì–´ë–¤ ì§ˆë¬¸ì— ë‹µí•˜ëŠ”ì§€ë¥¼ ì˜ì–´ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (ì˜ˆ: "What is the purpose of Level 1 quality manual?")
10. language: ì´ ì²­í¬ì˜ ì£¼ìš” ì–¸ì–´ ("ko" ë˜ëŠ” "en")

## ì¶œë ¥
JSONë§Œ ì¶œë ¥:
{{"content_type": "...", "main_topic": "...", "sub_topics": [...], "actors": [...], "actions": [...], "conditions": [...], "summary": "...", "intent_scope": "...", "intent_summary": "...", "language": "..."}}
"""

    try:
        response = get_llm_response(prompt, llm_model="gpt-4o-mini", llm_backend="openai", max_tokens=4096, temperature=0)
        result = response.strip()

        # JSON ë¸”ë¡ íŒŒì‹±
        if "```" in result:
            json_str = result.split("```")[1]
            if json_str.startswith("json"):
                json_str = json_str[4:]
            result = json_str.strip()

        metadata = json.loads(result)

        # intent_embedding ìƒì„±
        if embed_model and metadata.get("intent_summary"):
            try:
                embedding = embed_model.encode([metadata["intent_summary"]])[0].tolist()
                metadata["intent_embedding"] = embedding
            except:
                metadata["intent_embedding"] = []
        else:
            metadata["intent_embedding"] = []

        return metadata

    except Exception as e:
        print(f"    ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return _default_metadata()


def _default_metadata() -> Dict:
    """ê¸°ë³¸ ë©”íƒ€ë°ì´í„°"""
    return {
        "content_type": "",
        "main_topic": "",
        "sub_topics": [],
        "actors": [],
        "actions": [],
        "conditions": [],
        "summary": "",
        "intent_scope": "",
        "intent_summary": "",
        "language": "ko",
        "intent_embedding": []
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5ë‹¨ê³„: ì²­í¬ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_chunks(clauses: List[Dict], doc_id: str, doc_title: str,
                 use_llm_metadata: bool = False, embed_model=None,
                 chunk_size: int = 800, chunk_overlap: int = 100) -> List[Dict]:
    """
    ì¡°í•­ì„ ì²­í¬ë¡œ ë³€í™˜ (ê¸´ ì¡°í•­ì€ ì¬ë¶„í• )

    Returns:
        List[Dict]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    global_idx = 0

    if use_llm_metadata:
        print(f"    LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: gpt-4o-mini")

    for idx, clause in enumerate(clauses):
        clause_num = clause["clause"]
        title = clause["title"]
        content = clause["content"]
        level = clause["level"]

        # ì¡°í•­ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ê¸°ë°˜ ëŒ€ì²´
        clause_id = clause_num or f"ì„¹ì…˜_{idx+1}"

        # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì¡°í•­ë³„ë¡œ í•œ ë²ˆ ìˆ˜í–‰)
        if use_llm_metadata:
            print(f"    [{idx+1}/{len(clauses)}] ë¶„ì„: {clause_id} {title[:20]}...")
            meta = extract_metadata(content, clause_id, title, embed_model)
        else:
            print(f"    [{idx+1}/{len(clauses)}] ì €ì¥: {clause_id} {title[:20]}...")
            meta = _default_metadata()

        # 2. ê¸´ ì¡°í•­ ì¬ë¶„í•  (ë²¡í„° ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        # v8.7: contentê°€ 500ì ì´ìƒì´ë©´ ë¶„í• , ê° ì²­í¬ì— title í¬í•¨

        # contentê°€ 500ìë¥¼ ë„˜ìœ¼ë©´ ì—¬ëŸ¬ íŒŒíŠ¸ë¡œ ë¶„í• 
        if len(content) > 500:
            content_parts = _split_recursive(content, chunk_size=500, chunk_overlap=50)
        else:
            content_parts = [content]

        # ê° content íŒŒíŠ¸ë§ˆë‹¤ titleì„ ë¶™ì—¬ì„œ ì²­í¬ ìƒì„±
        for p_idx, content_part in enumerate(content_parts):
            # ê° ì²­í¬ = title + content_part
            chunk_text = f"{title}\n\n{content_part}"
            
            # ìµœì¢… ë©”íƒ€ë°ì´í„°
            full_meta = {
                "doc_id": doc_id,
                "doc_title": doc_title,
                "clause_id": clause_id,
                "title": title,
                "clause_level": level,
                "main_section": clause_id.split('.')[0] if '.' in str(clause_id) else clause_id,
                "page": clause.get("page", 1),
                "chunk_part": p_idx + 1 if len(content_parts) > 1 else None,
                **meta
            }

            chunks.append({
                "text": chunk_text.strip(),
                "index": global_idx,
                "metadata": full_meta
            })
            global_idx += 1

    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_document(
    file_path: str,
    content: bytes = None,
    doc_id: str = None,
    max_clause_level: int = None,
    use_llm_metadata: bool = False,
    embed_model=None
) -> Dict:
    """
    PDF ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜

    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        content: PDF ë°”ì´ë„ˆë¦¬ (ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ì½ìŒ)
        doc_id: ë¬¸ì„œ ID (ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ)
        max_clause_level: ìµœëŒ€ ì¡°í•­ ë ˆë²¨ (Noneì´ë©´ ëª¨ë“  ë ˆë²¨ í¬í•¨)
        use_llm_metadata: LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì—¬ë¶€
        embed_model: ì„ë² ë”© ëª¨ë¸ (intent_embeddingìš©)

    Returns:
        Dict: ì²˜ë¦¬ ê²°ê³¼
            - success: ì„±ê³µ ì—¬ë¶€
            - chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            - errors: ì—ëŸ¬ ë©”ì‹œì§€
    """
    print(f"\n{'='*60}")
    print(f"ë¬¸ì„œ ì²˜ë¦¬: {Path(file_path).name}")
    print(f"{'='*60}")

    try:
        # íŒŒì¼ ì½ê¸°
        if content is None:
            with open(file_path, 'rb') as f:
                content = f.read()

        # ë¬¸ì„œ ID ë° ë²„ì „ ê²°ì •
        if doc_id is None:
            # ID ì¶”ì¶œ: EQ-SOP-00004 í˜•ì‹ (ëŒ€ë¬¸ì-ëŒ€ë¬¸ì-ìˆ«ì)
            id_match = re.search(r'[A-Z]+-[A-Z]+-\d+', file_path)
            doc_id = id_match.group() if id_match else Path(file_path).stem
            
        # 1ì°¨ ë²„ì „ ì¶”ì¶œ: íŒŒì¼ëª… íŒ¨í„´ (v1.0, _v2.0 ë“±)
        version = None
        version_match = re.search(r'[vV_-]?(\d+\.\d+)', Path(file_path).name)
        if version_match:
            version = version_match.group(1)
            print(f"     [ì¶”ì¶œ] íŒŒì¼ëª…ì—ì„œ ë²„ì „ ê°ì§€: {version}")

        doc_title = Path(file_path).stem

        # 1. ë¬¸ì„œ ë³€í™˜ (PDF â†’ ë§ˆí¬ë‹¤ìš´, í…ìŠ¤íŠ¸ëŠ” ì§ì ‘ ì‚¬ìš©)
        file_ext = Path(file_path).suffix.lower()
        if file_ext in ['.md', '.markdown', '.txt']:
            print(f"\n[1/4] í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ ê°ì§€ ({file_ext}): ë³€í™˜ ìƒëµ ë° ì§ì ‘ ë¡œë“œ")
            if isinstance(content, bytes):
                markdown = content.decode('utf-8')
            else:
                markdown = content
        else:
            print("\n[1/4] PDF â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜")
            markdown = pdf_to_markdown(content)

        # 2. ì¡°í•­ íŒŒì‹±
        print("\n[2/4] ì¡°í•­ íŒŒì‹±")
        clauses = parse_clauses(markdown, max_level=max_clause_level)

        if not clauses:
            return {
                "success": False,
                "chunks": [],
                "errors": ["ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            }
            
        # 2.5 ì§€ëŠ¥í˜• ë²„ì „ ì¶”ì¶œ (íŒŒì¼ëª…ì— ì—†ì„ ê²½ìš° ë³¸ë¬¸ ë¶„ì„)
        if not version and use_llm_metadata:
            print("     [ì¶”ì¶œ] íŒŒì¼ëª…ì— ë²„ì „ ì •ë³´ê°€ ì—†ì–´ ë³¸ë¬¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            try:
                from backend.agent import get_openai_client
                client = get_openai_client()
                # ì•ë’¤ 2000ì ë¶„ì„
                sample_text = markdown[:2000] + "\n...\n" + markdown[-2000:]
                prompt = f"""ë‹¤ìŒ GMP ë¬¸ì„œ ë‚´ìš©(ê°œì • ì´ë ¥ í¬í•¨)ì„ ë³´ê³  ì´ ë¬¸ì„œì˜ 'í˜„ì¬ ë²„ì „'ì„ ì°¾ì•„ë‚´ì„¸ìš”.
                - í˜•ì‹ì€ 1.0, 2.0 ë“± ìˆ«ì.ìˆ«ì ì—¬ì•¼ í•©ë‹ˆë‹¤.
                - ê°€ì¥ ìµœê·¼(ë§ˆì§€ë§‰) ê°œì •ì´ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ì„¸ìš”.
                - ë²„ì „ ë²ˆí˜¸ë§Œ ë‹µë³€í•˜ì„¸ìš” (ì˜ˆ: 2.0). ì°¾ì§€ ëª»í•˜ë©´ '1.0'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
                
                [ë¬¸ì„œ ë‚´ìš©]
                {sample_text}
                """
                res = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}]
                )
                extracted_v = res.choices[0].message.content.strip()
                v_match_llm = re.search(r'(\d+\.\d+)', extracted_v)
                if v_match_llm:
                    version = v_match_llm.group(1)
                    print(f"     [ì¶”ì¶œ] ë³¸ë¬¸ ë¶„ì„ì„ í†µí•´ ë²„ì „ ê°ì§€: {version}")
            except Exception as ve:
                print(f"     [ì¶”ì¶œ] ë³¸ë¬¸ ë²„ì „ ì¶”ì¶œ ì‹¤íŒ¨: {ve}")

        # 3. ì²­í¬ ìƒì„±
        print(f"\n[3/4] ì²­í¬ ìƒì„± {'(LLM ë©”íƒ€ë°ì´í„° í¬í•¨)' if use_llm_metadata else ''}")
        chunks = create_chunks(clauses, doc_id, doc_title, use_llm_metadata, embed_model)

        # 4. ì™„ë£Œ
        print(f"\n[4/4] ì™„ë£Œ!")
        print(f"  - ì¡°í•­: {len(clauses)}ê°œ")
        print(f"  - ì²­í¬: {len(chunks)}ê°œ")

        return {
            "success": True,
            "chunks": chunks,
            "doc_id": doc_id,
            "doc_title": doc_title,
            "version": version or "1.0",
            "total_clauses": len(clauses),
            "markdown": markdown,  # ì›ë³¸ ë§ˆí¬ë‹¤ìš´ ì¶”ê°€
            "clauses": clauses,  # ì¡°í•­ êµ¬ì¡° ì¶”ê°€
            "errors": []
        }

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "chunks": [],
            "errors": [str(e)]
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python document_pipeline.py <PDFíŒŒì¼> [--llm-meta]")
        sys.exit(1)

    pdf_path = sys.argv[1]
    use_llm = "--llm-meta" in sys.argv

    result = process_document(
        file_path=pdf_path,
        use_llm_metadata=use_llm
    )

    if result["success"]:
        print(f"\nğŸŸ¢ ì„±ê³µ!")
    else:
        print(f"\nğŸ”´ ì‹¤íŒ¨: {result['errors']}")
