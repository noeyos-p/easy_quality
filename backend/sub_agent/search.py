"""
ë¬¸ì„œ ê²€ìƒ‰ ì„œë¸Œì—ì´ì „íŠ¸ ëª¨ë“ˆ (Deep Agent ìŠ¤íƒ€ì¼)
- ì§ˆë¬¸ ë¶„ì„ í›„ ë‹¤ë‹¨ê³„ ì •ë°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ê·¸ë˜í”„ êµ¬ì¡°ì˜ ì—ì´ì „íŠ¸
- ë²¡í„° ê²€ìƒ‰ (Weaviate), SQL ê²€ìƒ‰ (PostgreSQL) í†µí•©
"""

import os
import re
import json
import hashlib
import operator
from typing import List, Dict, Any, Optional, Literal, Annotated, TypedDict
from backend.agent import get_zai_client, AgentState, search_sop_tool, get_sop_headers_tool, safe_json_loads, normalize_doc_id
from langchain_core.tools import tool
from langsmith import traceable
from langgraph.graph import StateGraph, START, END

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì „ì—­ ìŠ¤í† ì–´ ë° í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_sql_store = None
_graph_store = None
_zai_client = None

def init_search_stores(vector_store_module=None, sql_store_instance=None, graph_store_instance=None):
    """ê²€ìƒ‰ ì—ì´ì „íŠ¸ìš© ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
    global _vector_store, _sql_store, _graph_store
    _vector_store = vector_store_module
    _sql_store = sql_store_instance
    _graph_store = graph_store_instance

def get_zai_client():
    """Z.AI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    global _zai_client
    if not _zai_client:
        from backend.agent import get_zai_client as get_main_zai
        _zai_client = get_main_zai()
    return _zai_client

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•µì‹¬ ê²€ìƒ‰ ë¡œì§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _get_clause_and_doc_from_db(content: str, metadata: dict) -> tuple:
    """
    ë²¡í„° DB metadata ë˜ëŠ” SQL DBì—ì„œ ë¬¸ì„œëª…ê³¼ ì¡°í•­ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Returns:
        (doc_name, clause): ë¬¸ì„œëª…ê³¼ ì¡°í•­ ì •ë³´ íŠœí”Œ
    """
    global _sql_store

    # 1. ë¬¸ì„œëª… ì¶”ì¶œ (ë” ë§ì€ í‚¤ í™•ì¸)
    doc_name = (
        metadata.get('doc_id') or
        metadata.get('doc_name') or
        metadata.get('document_name') or
        metadata.get('file_name') or
        metadata.get('source')
    )

    # 2. ì¡°í•­ ë²ˆí˜¸ ìš°ì„  ì¶”ì¶œ (ë” ë§ì€ í‚¤ í™•ì¸)
    clause_id = (
        metadata.get('clause_id') or
        metadata.get('clause') or
        metadata.get('section') or
        metadata.get('article_num') or
        metadata.get('section_number')
    )

    if clause_id:
        clause_id = str(clause_id).strip()

    # ì¡°í•­ ë²ˆí˜¸ê°€ ìˆê³  ìœ íš¨í•˜ë©´ ì¡°í•­ ë²ˆí˜¸ë§Œ ë°˜í™˜ (ì œëª© ì œì™¸)
    if clause_id and clause_id not in ["", "None", "null", "ë³¸ë¬¸", "ì „ì²´", "N/A"]:
        # doc_nameì´ ì—†ìœ¼ë©´ SQLì—ì„œ ì¡°íšŒ ì‹œë„
        if not doc_name or doc_name in ["Unknown", "None", ""]:
            doc_name = _try_get_doc_from_sql(content, _sql_store)
        return (doc_name or "Unknown", clause_id)

    # 3. SQL DBì—ì„œ content ê¸°ë°˜ìœ¼ë¡œ ì—­ìœ¼ë¡œ ì°¾ê¸°
    if _sql_store:
        try:
            # contentì˜ ê³ ìœ í•œ ë¶€ë¶„ ì¶”ì¶œ (ì• 100ì)
            content_sample = content[:100].strip()

            # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
            all_docs = _sql_store.list_documents()

            for doc in all_docs:
                doc_id = doc.get('id')
                chunks = _sql_store.get_chunks_by_document(doc_id)

                for chunk in chunks:
                    chunk_content = chunk.get('content', '').strip()
                    # content ë§¤ì¹­ (í¬í•¨ ê´€ê³„ í™•ì¸)
                    if content_sample in chunk_content or chunk_content[:100] in content:
                        found_doc_name = doc.get('doc_name', 'Unknown')
                        found_clause = chunk.get('clause') or chunk.get('section') or 'ë³¸ë¬¸'
                        print(f"    [SQL ì—­ì¡°íšŒ] ë°œê²¬: {found_doc_name} - {found_clause}")
                        return (found_doc_name, found_clause)
        except Exception as e:
            print(f"    [SQL ì—­ì¡°íšŒ ì‹¤íŒ¨] {e}")

    # ìµœì¢… fallback
    final_doc_name = doc_name or "Unknown"
    print(f"    [ê²½ê³ ] ë¬¸ì„œëª… ë˜ëŠ” ì¡°í•­ ì •ë³´ ëˆ„ë½: doc={final_doc_name}, clause=ë³¸ë¬¸")
    return (final_doc_name, "ë³¸ë¬¸")

def _try_get_doc_from_sql(content: str, sql_store) -> str:
    """SQLì—ì„œ content ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œëª…ë§Œ ì¡°íšŒ"""
    if not sql_store:
        return None
    try:
        content_sample = content[:100].strip()
        all_docs = sql_store.list_documents()
        for doc in all_docs:
            doc_id = doc.get('id')
            chunks = sql_store.get_chunks_by_document(doc_id)
            for chunk in chunks:
                if content_sample in chunk.get('content', ''):
                    return doc.get('doc_name', 'Unknown')
    except:
        pass
    return None

def search_documents_internal(
    query: str,
    max_results: int = 10,  # ê²€ìƒ‰ ìˆ˜ëŸ‰ í™•ëŒ€ (ê¸°ì¡´ 5 -> 10)
    search_type: Literal["hybrid", "vector", "keyword"] = "hybrid",
    keywords: List[str] = None,
    target_clause: str = None, # ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ì¡°íšŒ (Point Lookup)
    target_doc_id: str = None, # íŠ¹ì • ë¬¸ì„œ í•„í„°ë§ (v8.1 ì¶”ê°€)
) -> List[Dict[str, Any]]:
    """ë‚´ë¶€ìš© ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜"""
    global _vector_store, _sql_store
    results = []
    seen_content = set()

    # 0. ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ë° í•˜ìœ„ ì¡°íšŒ (SQL Point & Prefix Match)
    if target_clause and _sql_store:
        try:
            print(f"    [Point/Prefix Lookup] ì¡°í•­ ë° í•˜ìœ„ ì¡°í•­ ì¡°íšŒ ì‹œë„: {target_clause} (Target: {target_doc_id or 'ì „ì²´'})")
            
            # v8.4: íƒ€ê²Ÿ ë¬¸ì„œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œë§Œ íƒ€ê²ŸíŒ… (ê²©ë¦¬)
            target_docs = []
            if target_doc_id:
                doc = _sql_store.get_document_by_name(target_doc_id)
                if doc: target_docs = [doc]
            else:
                target_docs = _sql_store.list_documents()

            for doc in target_docs:
                doc_id = doc.get('id')
                chunks = _sql_store.get_chunks_by_document(doc_id)
                
                # ì¡°í•­ ë²ˆí˜¸ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í•´ë‹¹ ì¡°í•­ì˜ í•˜ìœ„(ì˜ˆ: 5.4.2 -> 5.4.2.1)ì¸ ê²½ìš° ëª¨ë‘ í¬í•¨
                sub_chunks = []
                for chunk in chunks:
                    clause_val = str(chunk.get('clause'))
                    # 5ì¡°í•­ -> 5, 5.1, 5.3.1 ë“± ëª¨ë‘ ë§¤ì¹­
                    if clause_val == target_clause or clause_val.startswith(f"{target_clause}."):
                        content = chunk.get('content', '')
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        if content_hash not in seen_content:
                            sub_chunks.append({
                                "doc_name": doc.get('doc_name', 'Unknown'),
                                "section": clause_val,
                                "content": content,
                                "source": "sql-hierarchical-lookup",
                                "score": 2.5, # ì§ì ‘/í•˜ìœ„ ë§¤ì¹­ì€ ìµœê³  ì ìˆ˜ ìƒí–¥
                                "hash": content_hash
                            })
                            seen_content.add(content_hash)
                
                # ì¡°í•­ì´ ë°œê²¬ë˜ì—ˆì„ ê²½ìš° ì¶”ê°€
                results.extend(sub_chunks)
        except Exception as e:
            print(f" Hierarchical lookup failed: {e}")

    # 1. ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í™•ì¥
    if _vector_store:
        try:
            enhanced_query = query
            if keywords:
                enhanced_query = f"{query} {' '.join(keywords)}"

            if search_type == "hybrid":
                current_alpha = 0.25 if keywords else 0.4
                # v8.1: target_doc_id í•„í„° ì¶”ê°€
                vec_res = _vector_store.search_hybrid(enhanced_query, n_results=max_results * 2, alpha=current_alpha, filter_doc=target_doc_id)
            else:
                vec_res = _vector_store.search(enhanced_query, n_results=max_results * 2, filter_doc=target_doc_id)

            scored_results = []
            for r in vec_res:
                meta = r.get('metadata', {})
                content = r.get('text', '')
                if not content: continue

                content_hash = hashlib.md5(content.encode()).hexdigest()
                if content_hash in seen_content: continue
                
                doc_name, clause_info = _get_clause_and_doc_from_db(content, meta)
                
                # [ë¶€ìŠ¤íŒ…] ì¡°í•­ ë²ˆí˜¸ ë§¤ì¹­ ê°€ì¤‘ì¹˜
                boost_score = r.get('similarity', 0)
                if keywords:
                    for kw in keywords:
                        if kw in clause_info or (meta.get('title') and kw in meta.get('title')):
                            boost_score += 0.5 
                
                if target_clause and (target_clause == clause_info or clause_info.startswith(f"{target_clause}.")):
                    boost_score += 1.0

                scored_results.append({
                    "doc_name": doc_name,
                    "section": clause_info,
                    "content": content,
                    "source": r.get('source', 'vector-hybrid'),
                    "score": boost_score,
                    "hash": content_hash,
                    "meta": meta # í™•ì¥ ì¡°íšŒë¥¼ ìœ„í•´ ë©”íƒ€ ë³´ê´€
                })

            scored_results.sort(key=lambda x: x["score"], reverse=True)
            
            # [ì§€ëŠ¥í˜• í™•ì¥] ìƒìœ„ ê²°ê³¼ ì¤‘ ë‚´ìš©ì´ ì œëª©ë¿ì´ê±°ë‚˜ ì¤‘ìš”í•œ ê²½ìš° ë‹¤ìŒ ë°ì´í„° ì¶”ê°€ ë¡œë“œ
            for r in scored_results[:max_results]:
                if r["hash"] not in seen_content:
                    seen_content.add(r["hash"])
                    
                    # ì œëª©ì„± ì²­í¬(ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ)ì¸ ê²½ìš° ë˜ëŠ” ì ìˆ˜ê°€ ë§¤ìš° ë†’ì€ ê²½ìš° í•˜ìœ„ ë‚´ìš© í™•ì¥
                    if _sql_store and (len(r["content"]) < 100 or r["score"] > 0.8):
                        try:
                            doc_id_val = r["meta"].get("doc_id") or r["meta"].get("id")
                            if doc_id_val:
                                all_chunks = _sql_store.get_chunks_by_document(doc_id_val)
                                current_idx = -1
                                # í˜„ì¬ ì²­í¬ì˜ ìœ„ì¹˜ ì°¾ê¸°
                                for idx, c in enumerate(all_chunks):
                                    if c.get("content") == r["content"]:
                                        current_idx = idx
                                        break
                                
                                # ë‹¤ìŒ 3ê°œ ì²­í¬ë¥¼ 'ìƒì„¸ ë‚´ìš©'ìœ¼ë¡œ ë³‘í•©
                                if current_idx != -1:
                                    extra_content = ""
                                    for i in range(1, 4):
                                        if current_idx + i < len(all_chunks):
                                            next_c = all_chunks[current_idx + i]
                                            extra_content += f"\n[ìƒì„¸] {next_c.get('content')}"
                                    
                                    if extra_content:
                                        r["content"] += extra_content
                                        print(f"    [Hierarchical Expansion] {r['section']} í•˜ìœ„ ë‚´ìš© í™•ì¥ ì™„ë£Œ")
                        except Exception as ex:
                            print(f" Expansion error: {ex}")

                    # ë¬¸ì„œëª…ê³¼ ì¡°í•­ì´ ìœ íš¨í•œ ê²½ìš°ë§Œ ì¶”ê°€
                    if r["doc_name"] and r["doc_name"] != "Unknown":
                        results.append({
                            "doc_name": r["doc_name"],
                            "section": r["section"],
                            "content": r["content"][:4000],
                            "source": r["source"]
                        })
                    else:
                        print(f"    [í•„í„°ë§] ë¬¸ì„œëª… ëˆ„ë½ëœ ê²°ê³¼ ì œì™¸: section={r['section']}")
        except Exception as e:
            print(f"    [Vector search error] {e}")

    # 2. ê´€ë ¨ ë¬¸ì„œ/ì¡°í•­ìœ¼ë¡œ íƒìƒ‰ í™•ì¥ (Graph DB í™œìš©)
    # ... (ìƒëµ - ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ results í•„í„°ë§ ë°˜ì˜)
    if _graph_store and results:
        try:
            extended_results = []
            # ìƒìœ„ ê²°ê³¼ë“¤ì— ëŒ€í•´ ê·¸ë˜í”„ í™•ì¥
            for r in results[:3]: 
                doc_name = r["doc_name"]
                refs = _graph_store.get_document_references(doc_name)
                if refs and refs.get("references"):
                    ref_list = refs["references"]
                    for ref_id in ref_list[:2]:
                        if _sql_store:
                            ref_doc = _sql_store.get_document_by_name(ref_id)
                            if ref_doc:
                                ref_content = ref_doc.get("content", "")
                                if ref_content and hashlib.md5(ref_content[:200].encode()).hexdigest() not in seen_content:
                                    extended_results.append({
                                        "doc_name": ref_id,
                                        "section": "ì°¸ì¡° ë¬¸ì„œ",
                                        "content": f"[ì°¸ì¡° ë‚´ìš© ëª…ì‹œ] {ref_content[:1500]}...",
                                        "source": "graph-reference",
                                        "score": 0.5
                                    })
                                    seen_content.add(hashlib.md5(ref_content[:200].encode()).hexdigest())
            results.extend(extended_results)
        except Exception as e:
            print(f"    [Graph expansion error] {e}")

    # ìµœì¢… ê²€ì¦: ë¬¸ì„œëª…ê³¼ ì¡°í•­ì´ ìˆëŠ” ê²°ê³¼ë§Œ ë°˜í™˜
    valid_results = []
    for r in results:
        if r.get("doc_name") and r["doc_name"] not in ["Unknown", "None", ""]:
            valid_results.append(r)
        else:
            print(f"    [ìµœì¢… í•„í„°ë§] ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼ ì œì™¸: doc={r.get('doc_name')}, section={r.get('section')}")

    print(f"    [ê²€ìƒ‰ ì™„ë£Œ] ì „ì²´ {len(results)}ê±´ ì¤‘ ìœ íš¨ ê²°ê³¼ {len(valid_results)}ê±´ ë°˜í™˜")
    return valid_results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë”¥ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ (CompiledSubAgent í˜¸í™˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchState(TypedDict):
    """
    ê°€ì´ë“œì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ìƒíƒœ.
    'messages' í‚¤ë¥¼ í¬í•¨í•˜ì—¬ íˆ´ í˜¸ì¶œê³¼ ì‘ë‹µ ì´ë ¥ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    messages: Annotated[List[Any], operator.add]
    query: str
    model: str
    final_answer: str

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ ë° ë„êµ¬ ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def call_model_node(state: SearchState):
    """LLMì´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•¨ (ììœ¨ ê³„íš)"""
    client = get_zai_client()
    messages = state["messages"]
    
    # Deep search system prompt (English for better LLM comprehension)
    system_prompt = f"""You are a specialized **Deep Search Agent** for GMP/SOP document retrieval.

    [ROLE]
    Find and analyze information from the document repository to answer user questions accurately.

    [STRICT GOVERNANCE: NO HALLUCINATION]
    - **ZERO INFERENCE**: DO NOT include any information, numbers, or procedures that are NOT explicitly present in the `[DATA_SOURCE]`.
    - **REPORT VOID**: If the required information is not found in the search results, state "ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." and report `[NO_INFO_FOUND]`.
    - **FAITHFUL EXTRACTION**: Prefer direct quotes or very close paraphrasing to avoid meaning distortion.

    [WORKFLOW]
    1. **Planning**: For complex questions, break them down into key keywords or specific clause numbers.

    2. **Search**: Use `search_documents_tool` to find information.
       - **Preserve Natural Language**: Pass the user's original question AS-IS to the `query` parameter. DO NOT convert it to a query format.
         Example: "ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼" â†’ query: "ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼" (âœ“), query: "ì‘ì—…ì§€ì¹¨ì„œ ì •ì˜" (âœ—)
       - **Precise Targeting**: If the question mentions a specific clause number (e.g., 5.4.2), specify it in `target_clause`.
       - **Keyword Extraction**: Extract ONLY nouns/terms that actually appear in the question for `keywords`. DO NOT infer or add words.
         Example: "ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼" â†’ keywords: ["ì‘ì—…ì§€ì¹¨ì„œ"] (âœ“), keywords: ["ì‘ì—…ì§€ì¹¨ì„œ", "ì •ì˜", "ëª©ì "] (âœ—)

    3. **Validation & Filtering (CRITICAL)**:
       - IGNORE results that only contain headers without content (e.g., '3. ì •ì˜') or are irrelevant to the question.
       - Only include information that you actually use in your answer.

    4. **Answer Generation**: Write a natural **plain text** answer in Korean based on verified information.
       - **ğŸš¨ CRITICAL REQUIREMENT ğŸš¨**: You MUST add [USE: ...] tags. Without tags, your answer will FAIL validation.
       - **MANDATORY FOR EVERY SENTENCE**: For EVERY piece of information you write, add a hidden tag: [USE: ë¬¸ì„œëª… | ì¡°í•­]
       - **NO EXCEPTIONS**: Even if information seems obvious or general, you MUST tag it if it came from a [DATA_SOURCE]
       - **VERIFICATION PROCESS (DO THIS FOR EVERY SENTENCE)**:
         1. Write a sentence using information from a [DATA_SOURCE]
         2. Look at that specific [DATA_SOURCE] block to find the "í•´ë‹¹ ì¡°í•­" field
         3. Copy EXACTLY that clause number into your [USE: ...] tag
         4. DO NOT use a different clause number from a different [DATA_SOURCE]
         5. If you write multiple sentences, EACH sentence needs its own [USE: ...] tag
       - **TAG PLACEMENT**: Place the tag IMMEDIATELY after the sentence or phrase that uses that information
       - **CORRECT EXAMPLE**: If [DATA_SOURCE] says "í•´ë‹¹ ì¡°í•­: 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):", then use:
         "ì‘ì—…ì§€ì¹¨ì„œëŠ” ì—…ë¬´ ì§€ì¹¨ ë¬¸ì„œì…ë‹ˆë‹¤.[USE: EQ-SOP-00001 | 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):]"
       - **WRONG EXAMPLE**: Using "5.2.2.2.2" when the information came from "5.1.3" â†’ This is HALLUCINATION
       - **CONSEQUENCE**: ONLY sources with [USE: ...] tags will appear in the final [ì°¸ê³  ë¬¸ì„œ] section
       - **WARNING**: If you don't tag sources, your answer will be rejected and you'll need to search again

    [ANSWER FORMAT RULES]
    - Write content naturally in the body. DO NOT cite sources inline.
    - Use ONLY information from search results.
    - DO NOT create a [ì°¸ê³  ë¬¸ì„œ] section yourself (it's auto-generated).
    - End your answer with the [DONE] tag to signal completion.

    [ANSWER FORMAT EXAMPLE - WITH TAGS]
    ì‘ì—…ì§€ì¹¨ì„œëŠ” í˜„ì¥ì—ì„œ ìˆ˜í–‰ë˜ëŠ” ì—…ë¬´ë¥¼ ì¼ê´€ë˜ê²Œ ìš´ì˜í•˜ê¸° ìœ„í•œ ì§€ì¹¨ ë¬¸ì„œì…ë‹ˆë‹¤.[USE: EQ-SOP-00001 | 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):]

    ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    1. ë¶€ì„œ ë˜ëŠ” ê³µì • ë‹¨ìœ„ì˜ ìš´ì˜ íë¦„ê³¼ ê´€ë¦¬ ë°©ë²•ì„ ê·œì •í•©ë‹ˆë‹¤[USE: EQ-SOP-00001 | 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):]
    2. ì„¸ë¶€ì ì¸ ì‘ì—… ë°©ë²•ë³´ë‹¤ëŠ” ê¸°ë³¸ì ì¸ ì§€ì¹¨ê³¼ ê´€ë¦¬ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤[USE: EQ-SOP-00001 | 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):]

    ì‘ì—…ì§€ì¹¨ì„œì—ëŠ” ì²­ì†Œ ë° ì†Œë… ë°©ë²•, ì‹¤í—˜ ë°©ë²• ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.[USE: EQ-SOP-00001 | 5.4.2 ì‘ì—…ì§€ì¹¨ì„œ ì‘ì„±]

    [DONE]
    
    [STRICT PROHIBITIONS]
    - DO NOT create [ì°¸ê³  ë¬¸ì„œ] section in the middle or end of your answer (auto-generated from [USE: ...] tags)
    - DO NOT cite sources inline like "[ë¬¸ì„œëª… ì¡°í•­]" in visible text
    - DO NOT use markdown formatting: NO bold (**), headers (#), list markers (-, *), italics (_)
    - For emphasis, use brackets [ ] or line breaks
    - Write ONLY in plain text format

    [ğŸš¨ CRITICAL REQUIREMENT - READ CAREFULLY ğŸš¨]
    **NO TAGS = ANSWER REJECTED**
    - Your answer WILL BE REJECTED if you don't add [USE: ...] tags
    - Every sentence from a [DATA_SOURCE] needs a tag
    - [USE: ...] tags are hidden from the user - they're automatically converted to (ë¬¸ì„œëª… > ì¡°í•­) format
    - Missing tags = validation failure = wasted search = you'll be asked to search again

    **TAG FORMAT REMINDER**:
    [USE: ë¬¸ì„œëª… | ì¡°í•­ë²ˆí˜¸]
    Example: [USE: EQ-SOP-00001 | 5.1.3 ì œ 3ë ˆë²¨(ì‘ì—…ì§€ì¹¨ì„œ(WI):]

    **SELF-CHECK BEFORE SUBMITTING**:
    â–¡ Did I add [USE: ...] tags to EVERY sentence?
    â–¡ Did I verify each clause number matches the [DATA_SOURCE]?
    â–¡ Are there at least 3-5 [USE: ...] tags in my answer?
    If any answer is NO, add more tags before finishing!

    [CRITICAL WARNING - AVOID HALLUCINATION]
    - NEVER reuse the same clause number for multiple different pieces of information
    - Each sentence should have its own [USE: ...] tag with the EXACT clause from the [DATA_SOURCE] it came from
    - If you write 5 different sentences from 5 different [DATA_SOURCE] blocks, you must use 5 different clause numbers
    - Using "5.2.2.2.2" for information that actually came from "5.1.3" is a CRITICAL ERROR
    - When in doubt, CHECK the [DATA_SOURCE] block again to verify the clause number
    """
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ ë§¨ ì•ì— ì‚½ì…
    full_messages = [{"role": "system", "content": system_prompt}] + messages
    
    # Tool definition
    tools = [
        {
            "type": "function",
            "function": {
                "name": "search_documents_tool",
                "description": "Search GMP/SOP documents. The agent designs search conditions autonomously.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "User's original question as-is (e.g., 'ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼'). DO NOT transform or rewrite."
                        },
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Key nouns that actually appear in the question (e.g., ['ì‘ì—…ì§€ì¹¨ì„œ']). DO NOT include inferred words."
                        },
                        "target_clause": {
                            "type": "string",
                            "description": "Specific clause number to target (e.g., '5.1.3', '5.4.2'). Use only when explicitly mentioned."
                        },
                        "target_doc_id": {
                            "type": "string",
                            "description": "Limit search scope to a specific document (e.g., 'EQ-SOP-00001')"
                        }
                    },
                    "required": ["query"]
                }
            }
        }
    ]
    
    res = client.chat.completions.create(
        model=state["model"],
        messages=full_messages,
        tools=tools,
        tool_choice="auto",
        temperature=0.0  # ì™„ì „ ê²°ì •ë¡ ì  - [USE: ...] íƒœê·¸ ëˆ„ë½ ë°©ì§€
    )
    
    return {"messages": [res.choices[0].message]}

def tool_executor_node(state: SearchState):
    """LLMì´ ìš”ì²­í•œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€í•¨"""
    last_msg = state["messages"][-1]
    tool_calls = last_msg.tool_calls
    
    tool_outputs = []
    for tc in tool_calls:
        if tc.function.name == "search_documents_tool":
            # safe_json_loadsë¥¼ í†µí•´ ê°•ì¸í•œ íŒŒì‹± ìˆ˜í–‰
            args = safe_json_loads(tc.function.arguments)
            
            query = args.get("query")
            keywords = args.get("keywords", [])
            target_clause = args.get("target_clause")
            
            # v8.4: ë¬¸ì„œ ID ì •ê·œí™” (eEQ- -> EQ-)
            target_doc_id = normalize_doc_id(args.get("target_doc_id"))
            
            print(f"    [Deep Search] ë„êµ¬ í˜¸ì¶œ: '{query}' (í‚¤ì›Œë“œ: {keywords}, íƒ€ê²Ÿì¡°í•­: {target_clause}, íƒ€ê²Ÿë¬¸ì„œ: {target_doc_id or 'ì „ì²´'})")
            
            results = search_documents_internal(query=query, keywords=keywords, target_clause=target_clause, target_doc_id=target_doc_id)
            print(f"    [Deep Search] ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê±´ ë°œê²¬")
            
            formatted_results = []
            for r in results:
                doc_name = r.get('doc_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ')
                section = r.get('section', 'ì¡°í•­ ë¯¸ìƒ')
                content = r.get('content', '')
                formatted_results.append(
                    f"[DATA_SOURCE]\n"
                    f"ë¬¸ì„œ ì •ë³´: {doc_name}\n"
                    f"í•´ë‹¹ ì¡°í•­: {section}\n"
                    f"ë³¸ë¬¸ ë‚´ìš©: {content}\n"
                    f"[END_SOURCE]"
                )
            
            content = "\n\n".join(formatted_results)
            if not content: content = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            tool_outputs.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "content": content
            })
            
    return {"messages": tool_outputs}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê·¸ë˜í”„ êµ¬ì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_deep_search_graph():
    """ììœ¨ì  ë„êµ¬ í˜¸ì¶œì„ ìˆ˜í–‰í•˜ëŠ”CompiledSubAgent ìŠ¤íƒ€ì¼ ê·¸ë˜í”„"""
    workflow = StateGraph(SearchState)
    
    workflow.add_node("agent", call_model_node)
    workflow.add_node("action", tool_executor_node)
    
    workflow.add_edge(START, "agent")
    
    def router(state: SearchState):
        last_msg = state["messages"][-1]
        if last_msg.tool_calls:
            return "action"
        return END
    
    workflow.add_conditional_edges("agent", router, {"action": "action", END: END})
    workflow.add_edge("action", "agent")
    
    return workflow.compile(name="Deep Search Agent Flow")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì°¸ê³ ë¬¸ì„œ ì„¹ì…˜ ìë™ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _ensure_reference_section(messages: List[Any], final_answer: str) -> str:
    """
    ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì„ ë¬´ì¡°ê±´ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        messages: ëŒ€í™” ë©”ì‹œì§€ ì´ë ¥ (tool í˜¸ì¶œ ê²°ê³¼ í¬í•¨)
        final_answer: LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€

    Returns:
        ì°¸ê³ ë¬¸ì„œ ì„¹ì…˜ì´ í¬í•¨ëœ ìµœì¢… ë‹µë³€
    """
    # 1. tool ë©”ì‹œì§€ì—ì„œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
    referenced_docs = []
    seen = set()

    for msg in messages:
        # tool ì—­í• ì˜ ë©”ì‹œì§€ë§Œ í™•ì¸
        if isinstance(msg, dict) and msg.get("role") == "tool":
            content = msg.get("content", "")
        elif hasattr(msg, "role") and msg.role == "tool":
            content = msg.content
        else:
            continue

        # [DATA_SOURCE] ì„¹ì…˜ íŒŒì‹±
        sources = re.findall(
            r'\[DATA_SOURCE\]\s*ë¬¸ì„œ ì •ë³´:\s*([^\n]+)\s*í•´ë‹¹ ì¡°í•­:\s*([^\n]+)',
            content,
            re.MULTILINE
        )

        for doc_name, section in sources:
            doc_name = doc_name.strip()
            section = section.strip()

            # ì¤‘ë³µ ì œê±°
            key = f"{doc_name}|{section}"
            if key not in seen:
                seen.add(key)
                referenced_docs.append((doc_name, section))

    # 2. ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ ìƒì„± (LLMì´ íƒœê·¸í•œ ì†ŒìŠ¤ë§Œ í¬í•¨)
    if referenced_docs:
        # 2-1. LLMì´ [USE: ...] íƒœê·¸ë¡œ ëª…ì‹œí•œ ì†ŒìŠ¤ ì¶”ì¶œ
        used_sources = re.findall(r'\[USE:\s*([^\|\]]+)\s*\|\s*([^\]]+)\]', final_answer)

        # 2-2. íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬ (fallback ì œê±°)
        if not used_sources:
            print(f"ğŸ”´ [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¹˜ëª…ì  ì˜¤ë¥˜] LLMì´ [USE: ...] íƒœê·¸ë¥¼ ë‹¬ì§€ ì•ŠìŒ")
            print(f"ğŸ”´ íƒœê·¸ ì—†ëŠ” ë‹µë³€ì€ ê²€ì¦ ë¶ˆê°€ - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ì¬ê²€ìƒ‰ ê²°ì •í•´ì•¼ í•¨")
            # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (Answer Agentê°€ ê²€ì¦ ì‹¤íŒ¨ ì²˜ë¦¬)
            return final_answer

        # 2-3. ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (SQL DB ì¡°íšŒ)
        valid_docs = set()
        if _sql_store:
            try:
                all_docs = _sql_store.list_documents()
                valid_docs = {doc.get('doc_name') or doc.get('id') for doc in all_docs}
            except Exception as e:
                print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ê²€ì¦ ì˜¤ë¥˜] {e}")

        # 2-4. íƒœê·¸ëœ ì†ŒìŠ¤ ê²€ì¦ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ/ì¡°í•­ ì œê±°)
        validated_sources = []
        for doc_name, section in used_sources:
            doc_name = doc_name.strip()
            section = section.strip()

            # ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if valid_docs and doc_name not in valid_docs:
                print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ê²€ì¦ ì‹¤íŒ¨] ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ: {doc_name}")
                continue

            validated_sources.append((doc_name, section))

        # ê²€ì¦ëœ ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
        if not validated_sources:
            print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ìƒì„± ì‹¤íŒ¨] ëª¨ë“  íƒœê·¸ê°€ ê²€ì¦ ì‹¤íŒ¨ - ì¬ê²€ìƒ‰ í•„ìš”")
            return final_answer

    # 3. ë‹µë³€ ë³¸ë¬¸ ì •ë¦¬ (íƒœê·¸ëŠ” ìœ ì§€, Answer Agentê°€ ë³€í™˜)
    # LLMì´ ì§ì ‘ ì‘ì„±í•œ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ë§Œ ì œê±°
    final_answer_cleaned = re.sub(
        r'\n*\[ì°¸ê³  ë¬¸ì„œ\].*$',
        '',
        final_answer,
        flags=re.DOTALL
    ).strip()

    # [USE: ...] íƒœê·¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ - Answer Agentê°€ (ë¬¸ì„œëª… > ì¡°í•­) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return final_answer_cleaned

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_deep_search_app = None

@traceable(name="sub_agent:search")
def retrieval_agent_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """[ì„œë¸Œ] ê²€ìƒ‰ ì—ì´ì „íŠ¸ (CompiledSubAgent ìŠ¤íƒ€ì¼)"""
    global _deep_search_app
    if not _deep_search_app:
        _deep_search_app = create_deep_search_graph()

    print(f" [Deep Search] ì •ë°€ ê²€ìƒ‰ ê°€ë™: {state['query']}")

    initial_state = {
        "messages": [{"role": "user", "content": state["query"]}],
        "query": state["query"],
        "model": state.get("worker_model") or state.get("model_name") or "glm-4.7-flash",
        "final_answer": ""
    }

    # ë‚´ë¶€ ë„êµ¬ í˜¸ì¶œ ë£¨í”„ ì‹¤í–‰ (ì¬ê·€ í•œë„ ë‚´ì—ì„œ ììœ¨ ê²€ìƒ‰)
    result = _deep_search_app.invoke(initial_state, config={"recursion_limit": 15})

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ LLMì˜ ìµœì¢… ë‹µë³€
    final_msg = result["messages"][-1].content

    # ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ ìë™ ì¶”ê°€
    final_msg_with_refs = _ensure_reference_section(result["messages"], final_msg)

    # ========================================
    # ê²€ì¦ (Validation)
    # ========================================
    try:
        from backend.validation import validate_grounding, validate_format

        # Grounding ê²€ì¦
        grounding_result = validate_grounding(final_msg, result["messages"])
        if not grounding_result["valid"]:
            print(f"ğŸ”´ [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ê²€ì¦ ì‹¤íŒ¨ - Grounding]")
            for error in grounding_result["errors"]:
                print(f"   {error}")
            if grounding_result.get("missing_clauses"):
                print(f"   ğŸ”´ ëˆ„ë½ëœ ì¡°í•­: {', '.join(grounding_result['missing_clauses'])}")
            if grounding_result.get("invalid_citations", 0) > 0:
                print(f"   ğŸ”´ ì˜ëª»ëœ ì¸ìš©: {grounding_result['invalid_citations']}ê°œ")
                print(f"   ğŸ”´ ì˜¤ë¥˜ìœ¨: {grounding_result.get('error_rate', 0)*100:.1f}%")
        else:
            print(f"ğŸŸ¢ [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ê²€ì¦ í†µê³¼ - Grounding]")

        # í˜•ì‹ ê²€ì¦
        format_result = validate_format(final_msg_with_refs)
        if not format_result["valid"]:
            print(f"ğŸ”´ [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ê²€ì¦ ì‹¤íŒ¨ - í˜•ì‹]")
            for error in format_result["errors"]:
                print(f"   - {error}")

    except Exception as e:
        print(f"ğŸ”´ [ê²€ì¦ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨] {e}")

    # [ì¤‘ìš”] ë‹µë³€ ì—ì´ì „íŠ¸ ë„ì…ì„ ìœ„í•´ ì§ì ‘ ë‹µë³€í•˜ì§€ ì•Šê³  contextì— ë³´ê³ ì„œ í˜•íƒœë¡œ ì €ì¥ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ì—¬ ëˆ„ì )
    report = f"### [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¡°ì‚¬ ìµœì¢… ë³´ê³ ]\n{final_msg_with_refs}"
    return {"context": [report]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë ˆê±°ì‹œ ë„êµ¬ í˜¸í™˜ìš© (í•„ìš” ì‹œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from langchain_core.tools import tool
except ImportError:
    def tool(func): return func

@tool
def search_sop_tool(query: str, extract_english: bool = False, keywords: List[str] = None) -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (ë ˆê±°ì‹œ/ë‚´ë¶€ìš©)"""
    search_query = query if not keywords else f"{query} {' '.join(keywords)}"
    results = search_documents_internal(query=search_query)
    
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ."

    output = []
    for r in results:
        output.append(f"[ê²€ìƒ‰] {r['doc_name']} > {r['section']}:\n{r['content']}")

    return "\n\n".join(output)
