"""
ë¬¸ì„œ ê²€ìƒ‰ ì„œë¸Œì—ì´ì „íŠ¸ ëª¨ë“ˆ (Deep Agent ìŠ¤íƒ€ì¼)
- ì§ˆë¬¸ ë¶„ì„ í›„ ë‹¤ë‹¨ê³„ ì •ë°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ê·¸ë˜í”„ êµ¬ì¡°ì˜ ì—ì´ì „íŠ¸
- ë²¡í„° ê²€ìƒ‰ (Weaviate), SQL ê²€ìƒ‰ (PostgreSQL) í†µí•©
"""

import os
import re
import json
import hashlib
import operator
from typing import List, Dict, Any, Optional, Literal, Annotated, TypedDict
from backend.agent import get_openai_client, AgentState, search_sop_tool, get_sop_headers_tool, safe_json_loads, normalize_doc_id
from langsmith import traceable
from langchain_core.tools import tool
from langsmith import traceable
from langgraph.graph import StateGraph, START, END

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì „ì—­ ìŠ¤í† ì–´ ë° í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_sql_store = None
_graph_store = None
_openai_client = None

def init_search_stores(vector_store_module=None, sql_store_instance=None, graph_store_instance=None):
    """ê²€ìƒ‰ ì—ì´ì „íŠ¸ìš© ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
    global _vector_store, _sql_store, _graph_store
    _vector_store = vector_store_module
    _sql_store = sql_store_instance
    _graph_store = graph_store_instance

def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    global _openai_client
    if not _openai_client:
        from backend.agent import get_openai_client as get_main_openai
        _openai_client = get_main_openai()
    return _openai_client

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í•µì‹¬ ê²€ìƒ‰ ë¡œì§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _get_clause_and_doc_from_db(content: str, metadata: dict) -> tuple:
    """
    ë²¡í„° DB metadata ë˜ëŠ” SQL DBì—ì„œ ë¬¸ì„œëª…ê³¼ ì¡°í•­ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Returns:
        (doc_name, clause): ë¬¸ì„œëª…ê³¼ ì¡°í•­ ì •ë³´ íŠœí”Œ
    """
    global _sql_store

    # 1. ë¬¸ì„œëª… ì¶”ì¶œ (ë” ë§ì€ í‚¤ í™•ì¸)
    doc_name = (
        metadata.get('doc_id') or
        metadata.get('doc_name') or
        metadata.get('document_name') or
        metadata.get('file_name') or
        metadata.get('source')
    )

    # 2. ì¡°í•­ ë²ˆí˜¸ ìš°ì„  ì¶”ì¶œ (ë” ë§ì€ í‚¤ í™•ì¸)
    clause_id = (
        metadata.get('clause_id') or
        metadata.get('clause') or
        metadata.get('section') or
        metadata.get('article_num') or
        metadata.get('section_number')
    )

    if clause_id:
        clause_id = str(clause_id).strip()

    # ì¡°í•­ ë²ˆí˜¸ê°€ ìˆê³  ìœ íš¨í•˜ë©´ ì¡°í•­ ë²ˆí˜¸ë§Œ ë°˜í™˜ (ì œëª© ì œì™¸)
    if clause_id and clause_id not in ["", "None", "null", "ë³¸ë¬¸", "ì „ì²´", "N/A"]:
        # doc_nameì´ ì—†ìœ¼ë©´ SQLì—ì„œ ì¡°íšŒ ì‹œë„
        if not doc_name or doc_name in ["Unknown", "None", ""]:
            doc_name = _try_get_doc_from_sql(content, _sql_store)
        return (doc_name or "Unknown", clause_id)

    # 3. SQL DBì—ì„œ content ê¸°ë°˜ìœ¼ë¡œ ì—­ìœ¼ë¡œ ì°¾ê¸°
    if _sql_store:
        try:
            # contentì˜ ê³ ìœ í•œ ë¶€ë¶„ ì¶”ì¶œ (ì• 100ì)
            content_sample = content[:100].strip()

            # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
            all_docs = _sql_store.list_documents()

            for doc in all_docs:
                doc_id = doc.get('id')
                chunks = _sql_store.get_chunks_by_document(doc_id)

                for chunk in chunks:
                    chunk_content = chunk.get('content', '').strip()
                    # content ë§¤ì¹­ (í¬í•¨ ê´€ê³„ í™•ì¸)
                    if content_sample in chunk_content or chunk_content[:100] in content:
                        found_doc_name = doc.get('doc_name', 'Unknown')
                        found_clause = chunk.get('clause') or chunk.get('section') or 'ë³¸ë¬¸'
                        print(f"    [SQL ì—­ì¡°íšŒ] ë°œê²¬: {found_doc_name} - {found_clause}")
                        return (found_doc_name, found_clause)
        except Exception as e:
            print(f"    [SQL ì—­ì¡°íšŒ ì‹¤íŒ¨] {e}")

    # ìµœì¢… fallback
    final_doc_name = doc_name or "Unknown"
    print(f"    [ê²½ê³ ] ë¬¸ì„œëª… ë˜ëŠ” ì¡°í•­ ì •ë³´ ëˆ„ë½: doc={final_doc_name}, clause=ë³¸ë¬¸")
    return (final_doc_name, "ë³¸ë¬¸")

def _try_get_doc_from_sql(content: str, sql_store) -> str:
    """SQLì—ì„œ content ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œëª…ë§Œ ì¡°íšŒ"""
    if not sql_store:
        return None
    try:
        content_sample = content[:100].strip()
        all_docs = sql_store.list_documents()
        for doc in all_docs:
            doc_id = doc.get('id')
            chunks = sql_store.get_chunks_by_document(doc_id)
            for chunk in chunks:
                if content_sample in chunk.get('content', ''):
                    return doc.get('doc_name', 'Unknown')
    except:
        pass
    return None

def search_documents_internal(
    query: str,
    max_results: int = 100,  # ì—ì´ì „íŠ¸ ë¶„ì„ìš© ë²¡í„° ê²€ìƒ‰ ìˆ˜ëŸ‰ í™•ëŒ€
    search_type: Literal["hybrid", "vector", "keyword"] = "hybrid",
    keywords: List[str] = None,
    target_clause: str = None, # ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ì¡°íšŒ (Point Lookup)
    target_doc_id: str = None, # íŠ¹ì • ë¬¸ì„œ í•„í„°ë§ (v8.1 ì¶”ê°€)
) -> List[Dict[str, Any]]:
    """ë‚´ë¶€ìš© ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜"""
    global _vector_store, _sql_store
    results = []
    seen_content = set()

    # 0. ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ë° í•˜ìœ„ ì¡°íšŒ (SQL Point & Prefix Match)
    if target_clause and _sql_store:
        try:
            print(f"    [Point/Prefix Lookup] ì¡°í•­ ë° í•˜ìœ„ ì¡°í•­ ì¡°íšŒ ì‹œë„: {target_clause} (Target: {target_doc_id or 'ì „ì²´'})")
            
            # v8.4: íƒ€ê²Ÿ ë¬¸ì„œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œë§Œ íƒ€ê²ŸíŒ… (ê²©ë¦¬)
            target_docs = []
            if target_doc_id:
                doc = _sql_store.get_document_by_name(target_doc_id)
                if doc: target_docs = [doc]
            else:
                target_docs = _sql_store.list_documents()

            for doc in target_docs:
                doc_id = doc.get('id')
                chunks = _sql_store.get_chunks_by_document(doc_id)
                
                # ì¡°í•­ ë²ˆí˜¸ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í•´ë‹¹ ì¡°í•­ì˜ í•˜ìœ„(ì˜ˆ: 5.4.2 -> 5.4.2.1)ì¸ ê²½ìš° ëª¨ë‘ í¬í•¨
                sub_chunks = []
                for chunk in chunks:
                    clause_val = str(chunk.get('clause'))
                    # 5ì¡°í•­ -> 5, 5.1, 5.3.1 ë“± ëª¨ë‘ ë§¤ì¹­
                    if clause_val == target_clause or clause_val.startswith(f"{target_clause}."):
                        content = chunk.get('content', '')
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        if content_hash not in seen_content:
                            sub_chunks.append({
                                "doc_name": doc.get('doc_name', 'Unknown'),
                                "section": clause_val,
                                "content": content,
                                "source": "sql-hierarchical-lookup",
                                "score": 2.5, # ì§ì ‘/í•˜ìœ„ ë§¤ì¹­ì€ ìµœê³  ì ìˆ˜ ìƒí–¥
                                "hash": content_hash
                            })
                            seen_content.add(content_hash)
                
                # ì¡°í•­ì´ ë°œê²¬ë˜ì—ˆì„ ê²½ìš° ì¶”ê°€
                results.extend(sub_chunks)
        except Exception as e:
            print(f" Hierarchical lookup failed: {e}")

    # 1. ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í™•ì¥
    if _vector_store:
        try:
            enhanced_query = query
            if keywords:
                enhanced_query = f"{query} {' '.join(keywords)}"

            if search_type == "hybrid":
                current_alpha = 0.25 if keywords else 0.4
                # v8.1: target_doc_id í•„í„° ì¶”ê°€
                vec_res = _vector_store.search_hybrid(
                    enhanced_query,
                    n_results=max_results,
                    alpha=current_alpha,
                    filter_doc=target_doc_id
                )
            else:
                vec_res = _vector_store.search(
                    enhanced_query,
                    n_results=max_results,
                    filter_doc=target_doc_id
                )

            scored_results = []
            for r in vec_res:
                meta = r.get('metadata', {})
                content = r.get('text', '')
                if not content: continue

                content_hash = hashlib.md5(content.encode()).hexdigest()
                if content_hash in seen_content: continue
                
                doc_name, clause_info = _get_clause_and_doc_from_db(content, meta)
                
                # [ë¶€ìŠ¤íŒ…] ì¡°í•­ ë²ˆí˜¸ ë§¤ì¹­ ê°€ì¤‘ì¹˜
                boost_score = r.get('similarity', 0)
                if keywords:
                    for kw in keywords:
                        if kw in clause_info or (meta.get('title') and kw in meta.get('title')):
                            boost_score += 0.5 
                
                if target_clause and (target_clause == clause_info or clause_info.startswith(f"{target_clause}.")):
                    boost_score += 1.0

                scored_results.append({
                    "doc_name": doc_name,
                    "section": clause_info,
                    "content": content,
                    "source": r.get('source', 'vector-hybrid'),
                    "score": boost_score,
                    "hash": content_hash,
                    "meta": meta # í™•ì¥ ì¡°íšŒë¥¼ ìœ„í•´ ë©”íƒ€ ë³´ê´€
                })

            scored_results.sort(key=lambda x: x["score"], reverse=True)
            
            # [ì§€ëŠ¥í˜• í™•ì¥] ìƒìœ„ ê²°ê³¼ ì¤‘ ë‚´ìš©ì´ ì œëª©ë¿ì´ê±°ë‚˜ ì¤‘ìš”í•œ ê²½ìš° ë‹¤ìŒ ë°ì´í„° ì¶”ê°€ ë¡œë“œ
            for r in scored_results[:max_results]:
                if r["hash"] not in seen_content:
                    seen_content.add(r["hash"])
                    
                    # ì œëª©ì„± ì²­í¬(ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ)ì¸ ê²½ìš° ë˜ëŠ” ì ìˆ˜ê°€ ë§¤ìš° ë†’ì€ ê²½ìš° í•˜ìœ„ ë‚´ìš© í™•ì¥
                    if _sql_store and (len(r["content"]) < 100 or r["score"] > 0.8):
                        try:
                            doc_id_val = r["meta"].get("doc_id") or r["meta"].get("id")
                            if doc_id_val:
                                all_chunks = _sql_store.get_chunks_by_document(doc_id_val)
                                current_idx = -1
                                # í˜„ì¬ ì²­í¬ì˜ ìœ„ì¹˜ ì°¾ê¸°
                                for idx, c in enumerate(all_chunks):
                                    if c.get("content") == r["content"]:
                                        current_idx = idx
                                        break
                                
                                # ë‹¤ìŒ 3ê°œ ì²­í¬ë¥¼ 'ìƒì„¸ ë‚´ìš©'ìœ¼ë¡œ ë³‘í•©
                                if current_idx != -1:
                                    extra_content = ""
                                    for i in range(1, 4):
                                        if current_idx + i < len(all_chunks):
                                            next_c = all_chunks[current_idx + i]
                                            extra_content += f"\n[ìƒì„¸] {next_c.get('content')}"
                                    
                                    if extra_content:
                                        r["content"] += extra_content
                                        print(f"    [Hierarchical Expansion] {r['section']} í•˜ìœ„ ë‚´ìš© í™•ì¥ ì™„ë£Œ")
                        except Exception as ex:
                            print(f" Expansion error: {ex}")

                    # ë¬¸ì„œëª…ê³¼ ì¡°í•­ì´ ìœ íš¨í•œ ê²½ìš°ë§Œ ì¶”ê°€
                    if r["doc_name"] and r["doc_name"] != "Unknown":
                        results.append({
                            "doc_name": r["doc_name"],
                            "section": r["section"],
                            "content": r["content"][:4000],
                            "source": r["source"]
                        })
                    else:
                        print(f"    [í•„í„°ë§] ë¬¸ì„œëª… ëˆ„ë½ëœ ê²°ê³¼ ì œì™¸: section={r['section']}")
        except Exception as e:
            print(f"    [Vector search error] {e}")

    # 2. ê´€ë ¨ ë¬¸ì„œ/ì¡°í•­ìœ¼ë¡œ íƒìƒ‰ í™•ì¥ (Graph DB í™œìš©)
    # ... (ìƒëµ - ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ results í•„í„°ë§ ë°˜ì˜)
    if _graph_store and results:
        try:
            extended_results = []
            # ìƒìœ„ ê²°ê³¼ë“¤ì— ëŒ€í•´ ê·¸ë˜í”„ í™•ì¥
            for r in results[:3]: 
                doc_name = r["doc_name"]
                refs = _graph_store.get_document_references(doc_name)
                if refs and refs.get("references"):
                    ref_list = refs["references"]
                    for ref_id in ref_list[:2]:
                        if _sql_store:
                            ref_doc = _sql_store.get_document_by_name(ref_id)
                            if ref_doc:
                                ref_content = ref_doc.get("content", "")
                                if ref_content and hashlib.md5(ref_content[:200].encode()).hexdigest() not in seen_content:
                                    extended_results.append({
                                        "doc_name": ref_id,
                                        "section": "ì°¸ì¡° ë¬¸ì„œ",
                                        "content": f"[ì°¸ì¡° ë‚´ìš© ëª…ì‹œ] {ref_content[:1500]}...",
                                        "source": "graph-reference",
                                        "score": 0.5
                                    })
                                    seen_content.add(hashlib.md5(ref_content[:200].encode()).hexdigest())
            results.extend(extended_results)
        except Exception as e:
            print(f"    [Graph expansion error] {e}")

    # ìµœì¢… ê²€ì¦: ë¬¸ì„œëª…ê³¼ ì¡°í•­ì´ ìˆëŠ” ê²°ê³¼ë§Œ ë°˜í™˜
    valid_results = []
    for r in results:
        if r.get("doc_name") and r["doc_name"] not in ["Unknown", "None", ""]:
            valid_results.append(r)
        else:
            print(f"    [ìµœì¢… í•„í„°ë§] ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼ ì œì™¸: doc={r.get('doc_name')}, section={r.get('section')}")

    print(f"    [ê²€ìƒ‰ ì™„ë£Œ] ì „ì²´ {len(results)}ê±´ ì¤‘ ìœ íš¨ ê²°ê³¼ {len(valid_results)}ê±´ ë°˜í™˜")
    return valid_results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë”¥ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ (CompiledSubAgent í˜¸í™˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchState(TypedDict):
    """
    ê°€ì´ë“œì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ìƒíƒœ.
    'messages' í‚¤ë¥¼ í¬í•¨í•˜ì—¬ íˆ´ í˜¸ì¶œê³¼ ì‘ë‹µ ì´ë ¥ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    messages: Annotated[List[Any], operator.add]
    query: str
    model: str
    final_answer: str
    detected_doc_id: Optional[str] # v8.5 ì¶”ê°€

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ ë° ë„êµ¬ ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@traceable(name="search_agent_llm_call", run_type="llm")
def call_model_node(state: SearchState):
    """LLMì´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•¨ (ììœ¨ ê³„íš)"""
    client = get_openai_client()
    messages = state["messages"]
    
    system_prompt = f"""You are a specialized agent for GMP/SOP document retrieval.
Use the search tool to accurately answer user questions.
{f"**Priority search target**: {state.get('detected_doc_id')}" if state.get('detected_doc_id') else ""}

## Principles

- **No information without a source**: Never write content that is not in [DATA_SOURCE].
- **Specify document name**: Always use the document ID (e.g., EQ-SOP-00001) as the subject instead of pronouns like "this document" or "this regulation."
- **On search failure**: Return "No relevant information found within the searched documents." + [NO_INFO_FOUND].

## Search Method (search_documents_tool)

- `query`: Pass the user question **as-is in its original form**. Do not convert it into a query format.
  - O "What is a work instruction?" -> query: "What is a work instruction?"
  - X query: "work instruction definition"
- `target_clause`: Specify if a particular clause number is mentioned (e.g., "5.4.2").
- `keywords`: Extract only **nouns that actually appear** in the question. Do not infer or add additional terms.
  - O "What is a work instruction?" -> ["work instruction"]
  - X ["work instruction", "definition", "purpose"]

## Answer Writing Rules

**Format**: Korean plain text. No markdown (**, #, -, *). Use [ ] or line breaks for emphasis.

**Source tagging ([USE: ...] tags)**:
- Every sentence sourced from [DATA_SOURCE] must end with a `[USE: document name | clause]` tag.
- The clause number must be an **exact copy** of the "applicable clause" field from the corresponding [DATA_SOURCE].
- Information from different [DATA_SOURCE] entries must use their respective clause numbers.
- Answers without tags will be treated as verification failures.
- The [Reference Documents] section is auto-generated from tags; do not write it manually.

**Example**:
A work instruction is a guidance document for consistently operating on-site tasks.[USE: EQ-SOP-00001 | 5.1.3 Level 3 (Work Instruction (WI):]
The key characteristics are as follows.
1. It defines the operational flow and management methods at the department or process level.[USE: EQ-SOP-00001 | 5.1.3 Level 3 (Work Instruction (WI):]
2. It includes cleaning and disinfection methods, testing methods, etc.[USE: EQ-SOP-00001 | 5.4.2 Work Instruction Writing]
[DONE]

## Pre-submission Checklist

- Does every sentence have a [USE: ...] tag?
- Does each tag's clause number match the "applicable clause" of the corresponding [DATA_SOURCE]?
- Was the document ID used instead of pronouns like "this document"?
- Was the [Reference Documents] section not written manually?
- Is [DONE] appended at the end of the answer?"""
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ ë§¨ ì•ì— ì‚½ì…
    full_messages = [{"role": "system", "content": system_prompt}] + messages
    
    # Tool definition
    tools = [
        {
            "type": "function",
            "function": {
                "name": "search_documents_tool",
                "description": "Search GMP/SOP documents. The agent designs search conditions autonomously.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "User's original question as-is (e.g., 'ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼'). DO NOT transform or rewrite."
                        },
                        "keywords": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Key nouns that actually appear in the question (e.g., ['ì‘ì—…ì§€ì¹¨ì„œ']). DO NOT include inferred words."
                        },
                        "target_clause": {
                            "type": "string",
                            "description": "Specific clause number to target (e.g., '5.1.3', '5.4.2'). Use only when explicitly mentioned."
                        },
                        "target_doc_id": {
                            "type": "string",
                            "description": "Limit search scope to a specific document (e.g., 'EQ-SOP-00001')"
                        }
                    },
                    "required": ["query"]
                }
            }
        }
    ]
    
    # ë””ë²„ê¹…: ëª¨ë¸ í™•ì¸
    model_to_use = state["model"]
    print(f"[DEBUG call_model_node] Using model: {model_to_use}")

    # LangChain ChatOpenAI ì‚¬ìš© (tools ë°”ì¸ë”©) - LangSmith ìë™ ì¶”ì 
    from backend.agent import get_langchain_llm
    llm = get_langchain_llm(model=model_to_use, temperature=0.0)
    llm_with_tools = llm.bind(tools=tools, tool_choice="auto")

    res = llm_with_tools.invoke(full_messages)

    return {"messages": [res]}

def tool_executor_node(state: SearchState):
    """LLMì´ ìš”ì²­í•œ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€í•¨"""
    last_msg = state["messages"][-1]
    tool_calls = last_msg.tool_calls
    
    tool_outputs = []
    for tc in tool_calls:
        # LangChainê³¼ OpenAI API í˜¸í™˜ì„± ì²˜ë¦¬
        if isinstance(tc, dict):
            # LangChain í˜•ì‹ (dict)
            tool_name = tc.get("name")
            tool_args = tc.get("args", {})
            tool_id = tc.get("id")
        else:
            # OpenAI API í˜•ì‹ (ê°ì²´)
            tool_name = tc.function.name
            tool_args = safe_json_loads(tc.function.arguments)
            tool_id = tc.id

        if tool_name == "search_documents_tool":
            query = tool_args.get("query")
            keywords = tool_args.get("keywords", [])
            target_clause = tool_args.get("target_clause")

            # v8.4: ë¬¸ì„œ ID ì •ê·œí™” (eEQ- -> EQ-)
            target_doc_id = normalize_doc_id(tool_args.get("target_doc_id"))
            # LLMì´ target_doc_idë¥¼ ëˆ„ë½í•´ë„, ì§ˆë¬¸ì—ì„œ ê°ì§€ëœ IDê°€ ìˆìœ¼ë©´ ê°•ì œ ì£¼ì…
            if not target_doc_id and state.get("detected_doc_id"):
                target_doc_id = normalize_doc_id(state.get("detected_doc_id"))
                print(f"    [Deep Search] ê°ì§€ëœ ë¬¸ì„œ ID ê°•ì œ ì ìš©: {target_doc_id}")

            print(f"    [Deep Search] ë„êµ¬ í˜¸ì¶œ: '{query}' (í‚¤ì›Œë“œ: {keywords}, íƒ€ê²Ÿì¡°í•­: {target_clause}, íƒ€ê²Ÿë¬¸ì„œ: {target_doc_id or 'ì „ì²´'})")

            results = search_documents_internal(query=query, keywords=keywords, target_clause=target_clause, target_doc_id=target_doc_id)
            print(f"    [Deep Search] ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê±´ ë°œê²¬")

            formatted_results = []
            for r in results:
                doc_name = r.get('doc_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ')
                section = r.get('section', 'ì¡°í•­ ë¯¸ìƒ')
                content = r.get('content', '')
                formatted_results.append(
                    f"[DATA_SOURCE]\n"
                    f"ë¬¸ì„œ ì •ë³´: {doc_name}\n"
                    f"í•´ë‹¹ ì¡°í•­: {section}\n"
                    f"ë³¸ë¬¸ ë‚´ìš©: {content}\n"
                    f"[END_SOURCE]"
                )

            content = "\n\n".join(formatted_results)
            if not content: content = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

            tool_outputs.append({
                "role": "tool",
                "tool_call_id": tool_id,
                "content": content
            })
            
    return {"messages": tool_outputs}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê·¸ë˜í”„ êµ¬ì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_deep_search_graph():
    """ììœ¨ì  ë„êµ¬ í˜¸ì¶œì„ ìˆ˜í–‰í•˜ëŠ”CompiledSubAgent ìŠ¤íƒ€ì¼ ê·¸ë˜í”„"""
    workflow = StateGraph(SearchState)
    
    workflow.add_node("agent", call_model_node)
    workflow.add_node("action", tool_executor_node)
    
    workflow.add_edge(START, "agent")
    
    def router(state: SearchState):
        last_msg = state["messages"][-1]
        if last_msg.tool_calls:
            return "action"
        return END
    
    workflow.add_conditional_edges("agent", router, {"action": "action", END: END})
    workflow.add_edge("action", "agent")
    
    return workflow.compile(name="Deep Search Agent Flow")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì°¸ê³ ë¬¸ì„œ ì„¹ì…˜ ìë™ ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _ensure_reference_section(messages: List[Any], final_answer: str) -> str:
    """
    ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì„ ë¬´ì¡°ê±´ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        messages: ëŒ€í™” ë©”ì‹œì§€ ì´ë ¥ (tool í˜¸ì¶œ ê²°ê³¼ í¬í•¨)
        final_answer: LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€

    Returns:
        ì°¸ê³ ë¬¸ì„œ ì„¹ì…˜ì´ í¬í•¨ëœ ìµœì¢… ë‹µë³€
    """
    # 1. tool ë©”ì‹œì§€ì—ì„œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
    referenced_docs = []
    seen = set()

    for msg in messages:
        # tool ì—­í• ì˜ ë©”ì‹œì§€ë§Œ í™•ì¸
        if isinstance(msg, dict) and msg.get("role") == "tool":
            content = msg.get("content", "")
        elif hasattr(msg, "role") and msg.role == "tool":
            content = msg.content
        else:
            continue

        # [DATA_SOURCE] ì„¹ì…˜ íŒŒì‹±
        sources = re.findall(
            r'\[DATA_SOURCE\]\s*ë¬¸ì„œ ì •ë³´:\s*([^\n]+)\s*í•´ë‹¹ ì¡°í•­:\s*([^\n]+)',
            content,
            re.MULTILINE
        )

        for doc_name, section in sources:
            doc_name = doc_name.strip()
            section = section.strip()

            # ì¤‘ë³µ ì œê±°
            key = f"{doc_name}|{section}"
            if key not in seen:
                seen.add(key)
                referenced_docs.append((doc_name, section))

    # 2. ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ ìƒì„± (LLMì´ íƒœê·¸í•œ ì†ŒìŠ¤ë§Œ í¬í•¨)
    if referenced_docs:
        # 2-1. LLMì´ [USE: ...] íƒœê·¸ë¡œ ëª…ì‹œí•œ ì†ŒìŠ¤ ì¶”ì¶œ
        used_sources = re.findall(r'\[USE:\s*([^\|\]]+)\s*\|\s*([^\]]+)\]', final_answer)

        # 2-2. íƒœê·¸ê°€ ì—†ìœ¼ë©´ ìµœì†Œ íƒœê·¸ë¥¼ ìë™ ì£¼ì…í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ë‹¨ì ˆ ë°©ì§€
        if not used_sources:
            print(f"ğŸ”´ [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¹˜ëª…ì  ì˜¤ë¥˜] LLMì´ [USE: ...] íƒœê·¸ë¥¼ ë‹¬ì§€ ì•ŠìŒ")
            print(f"ğŸŸ¡ ê²€ìƒ‰ëœ DATA_SOURCE ê¸°ë°˜ìœ¼ë¡œ [USE] íƒœê·¸ ìë™ ë³´ê°•")
            fallback_tags = " ".join(
                [f"[USE: {doc} | {section}]" for doc, section in referenced_docs[:3]]
            )
            return f"{final_answer}\n{fallback_tags}".strip()

        # 2-3. ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (SQL DB ì¡°íšŒ)
        valid_docs = set()
        if _sql_store:
            try:
                all_docs = _sql_store.list_documents()
                valid_docs = {doc.get('doc_name') or doc.get('id') for doc in all_docs}
            except Exception as e:
                print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ê²€ì¦ ì˜¤ë¥˜] {e}")

        # 2-4. íƒœê·¸ëœ ì†ŒìŠ¤ ê²€ì¦ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ/ì¡°í•­ ì œê±°)
        validated_sources = []
        for doc_name, section in used_sources:
            doc_name = doc_name.strip()
            section = section.strip()

            # ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if valid_docs and doc_name not in valid_docs:
                print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ê²€ì¦ ì‹¤íŒ¨] ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ: {doc_name}")
                continue

            validated_sources.append((doc_name, section))

        # ê²€ì¦ëœ ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨
        if not validated_sources:
            print(f"ğŸ”´ [ì°¸ê³ ë¬¸í—Œ ìƒì„± ì‹¤íŒ¨] ëª¨ë“  íƒœê·¸ê°€ ê²€ì¦ ì‹¤íŒ¨ - ì¬ê²€ìƒ‰ í•„ìš”")
            return final_answer

    # 3. ë‹µë³€ ë³¸ë¬¸ ì •ë¦¬ (íƒœê·¸ëŠ” ìœ ì§€, Answer Agentê°€ ë³€í™˜)
    # LLMì´ ì§ì ‘ ì‘ì„±í•œ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ë§Œ ì œê±°
    final_answer_cleaned = re.sub(
        r'\n*\[ì°¸ê³  ë¬¸ì„œ\].*$',
        '',
        final_answer,
        flags=re.DOTALL
    ).strip()

    # [USE: ...] íƒœê·¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ - Answer Agentê°€ (ë¬¸ì„œëª… > ì¡°í•­) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return final_answer_cleaned

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_deep_search_app = None

@traceable(name="sub_agent:search")
def retrieval_agent_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """[ì„œë¸Œ] ê²€ìƒ‰ ì—ì´ì „íŠ¸ (CompiledSubAgent ìŠ¤íƒ€ì¼)"""
    global _deep_search_app
    if not _deep_search_app:
        _deep_search_app = create_deep_search_graph()

    print(f" [Deep Search] ì •ë°€ ê²€ìƒ‰ ê°€ë™: {state['query']}")

    # v8.5: ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ SOP ID ìë™ ê°ì§€ (ì •ê·œì‹ ì‚¬ìš©)
    # ì˜ˆ: "EQ-SOP-00001 ëª©ì ì´ ë­ì•¼?" -> target_doc_id="EQ-SOP-00001"
    auto_doc_id = None
    sop_pattern = r'(EQ-(?:SOP|WI|FRM)-\d+)'
    match = re.search(sop_pattern, state['query'], re.IGNORECASE)
    if match:
        auto_doc_id = match.group(1).upper()
        print(f"    [Deep Search] ì§ˆë¬¸ì—ì„œ ë¬¸ì„œ ID ê°ì§€: {auto_doc_id}")

    # ëª¨ë¸ ì •ë³´ í™•ì¸
    worker_model = state.get("worker_model")
    model_name = state.get("model_name")
    final_model = worker_model or model_name or "gpt-4o"
    print(f"[DEBUG retrieval] model={final_model}")

    initial_state = {
        "messages": [{"role": "user", "content": state["query"]}],
        "query": state["query"],
        "model": final_model,
        "final_answer": ""
    }

    # ë§Œì•½ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ë„˜ê²¨ì¤€ íƒ€ê²Ÿì´ ì—†ê³  ë³¸ë¬¸ì—ì„œ ê°ì§€ë˜ì—ˆë‹¤ë©´ ì£¼ì…
    # (ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
    if auto_doc_id and not state.get("target_doc_id"):
        # ì´ˆê¸° ë©”ì‹œì§€ì— íŒíŠ¸ë¥¼ ì£¼ì–´ LLMì´ tool í˜¸ì¶œ ì‹œ í•´ë‹¹ IDë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„í•˜ê±°ë‚˜, 
        # ì§ì ‘ SearchStateì— ë°˜ì˜ (ì—¬ê¸°ì„œëŠ” search_documents_internal í˜¸ì¶œ ì‹œ ë°˜ì˜ë˜ë„ë¡ model_node í”„ë¡¬í”„íŠ¸ ë³´ê°• ê³ ë ¤ ê°€ëŠ¥)
        # í•˜ì§€ë§Œ ë” í™•ì‹¤í•œ ë°©ë²•ì€ call_model_nodeì˜ í”„ë¡¬í”„íŠ¸ì— ê°ì§€ëœ IDë¥¼ ëª…ì‹œí•˜ëŠ” ê²ƒì„
        initial_state["detected_doc_id"] = auto_doc_id

    # ë‚´ë¶€ ë„êµ¬ í˜¸ì¶œ ë£¨í”„ ì‹¤í–‰ (ì¬ê·€ í•œë„ ë‚´ì—ì„œ ììœ¨ ê²€ìƒ‰)
    result = _deep_search_app.invoke(initial_state, config={"recursion_limit": 15})

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ LLMì˜ ìµœì¢… ë‹µë³€
    final_msg = result["messages"][-1].content

    # ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ ìë™ ì¶”ê°€
    final_msg_with_refs = _ensure_reference_section(result["messages"], final_msg)

    # ë³µêµ¬ ë¡œì§:
    # LLMì´ NO_INFOë¥¼ ë°˜í™˜í–ˆë”ë¼ë„, tool ê²°ê³¼ì— [DATA_SOURCE]ê°€ ìˆìœ¼ë©´
    # ê²€ìƒ‰ëœ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì†Œ ë‹µë³€ì„ ìƒì„±í•´ íŒŒì´í”„ë¼ì¸ ë‹¨ì ˆì„ ë°©ì§€
    has_no_info = (
        "No relevant information found within the searched documents." in final_msg_with_refs
        or "[NO_INFO_FOUND]" in final_msg_with_refs
        or "ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in final_msg_with_refs
    )
    if has_no_info:
        recovered_sources = []
        for msg in result["messages"]:
            if isinstance(msg, dict) and msg.get("role") == "tool":
                tool_content = msg.get("content", "")
            elif hasattr(msg, "role") and msg.role == "tool":
                tool_content = getattr(msg, "content", "")
            else:
                continue

            hits = re.findall(
                r'\[DATA_SOURCE\]\s*ë¬¸ì„œ ì •ë³´:\s*([^\n]+)\s*í•´ë‹¹ ì¡°í•­:\s*([^\n]+)\s*ë³¸ë¬¸ ë‚´ìš©:\s*(.*?)\s*\[END_SOURCE\]',
                tool_content,
                re.MULTILINE | re.DOTALL
            )
            for doc_name, clause, body in hits[:5]:
                body = re.sub(r'\s+', ' ', (body or '').strip())
                if not body:
                    continue
                recovered_sources.append((doc_name.strip(), clause.strip(), body[:300]))

        if recovered_sources:
            lines = []
            for doc_name, clause, body in recovered_sources[:3]:
                lines.append(f"{doc_name}ì˜ {clause}ì— ë”°ë¥´ë©´ {body}[USE: {doc_name} | {clause}]")
            lines.append("[DONE]")
            final_msg_with_refs = "\n".join(lines)
            print(f"ğŸŸ¡ [Deep Search] NO_INFO ë³µêµ¬ ì ìš©: {len(recovered_sources)}ê±´ ì†ŒìŠ¤ ê¸°ë°˜ ìµœì†Œ ë‹µë³€ ìƒì„±")

    # [ì¤‘ìš”] ë‹µë³€ ì—ì´ì „íŠ¸ ë„ì…ì„ ìœ„í•´ ì§ì ‘ ë‹µë³€í•˜ì§€ ì•Šê³  contextì— ë³´ê³ ì„œ í˜•íƒœë¡œ ì €ì¥ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ì—¬ ëˆ„ì )
    report = f"### [ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì¡°ì‚¬ ìµœì¢… ë³´ê³ ]\n{final_msg_with_refs}"
    return {"context": [report]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë ˆê±°ì‹œ ë„êµ¬ í˜¸í™˜ìš© (í•„ìš” ì‹œ)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from langchain_core.tools import tool
except ImportError:
    def tool(func): return func

@tool
def search_sop_tool(query: str, extract_english: bool = False, keywords: List[str] = None) -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ (ë ˆê±°ì‹œ/ë‚´ë¶€ìš©)"""
    search_query = query if not keywords else f"{query} {' '.join(keywords)}"
    results = search_documents_internal(query=search_query)
    
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ."

    output = []
    for r in results:
        output.append(f"[ê²€ìƒ‰] {r['doc_name']} > {r['section']}:\n{r['content']}")

    return "\n\n".join(output)
