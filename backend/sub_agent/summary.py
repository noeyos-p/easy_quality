import json
import re
import operator
from typing import Any, Dict, List, Optional, Annotated, TypedDict, Literal
from backend.agent import get_zai_client, search_sop_tool, get_sop_headers_tool, AgentState
from langgraph.graph import StateGraph, START, END

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë”¥ ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ (SummaryState)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SummaryState(TypedDict):
    query: str
    doc_id: Optional[str]
    full_context: Annotated[List[str], operator.add]
    summary_mode: Literal["global", "section"]
    plan: List[str] # ìš”ì•½í•  ì¡°í•­/ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
    current_step: int
    final_report: str
    model: str

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ ì •ì˜ (Nodes)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def planner_node(state: SummaryState):
    """[Planner] ì§ˆë¬¸ ì˜ë„ì™€ ë¬¸ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ìš”ì•½ ê³„íš ìˆ˜ë¦½"""
    client = get_zai_client()
    query = state["query"]
    
    # 1. ë¬¸ì„œ ID ì¶”ì¶œ ë° ì‹¤ì œ ëª©ì°¨ ì¡°íšŒ
    id_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ë¶„ì„ ëŒ€ìƒì´ ë˜ëŠ” ë¬¸ì„œ ID(ì˜ˆ: EQ-SOP-00001)ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. ì§ˆë¬¸: {query}"
    id_res = client.chat.completions.create(model=state["model"], messages=[{"role": "user", "content": id_prompt}])
    doc_id = re.search(r'([A-Z]{2}-SOP-\d+)', id_res.choices[0].message.content.upper())
    doc_id = doc_id.group(1) if doc_id else None
    
    actual_headers = ""
    if doc_id:
        actual_headers = get_sop_headers_tool.invoke({"sop_id": doc_id})
        print(f"   ğŸ“‘ [Deep Summary] ì‹¤ì œ ëª©ì°¨ íŒŒì•… ì„±ê³µ: {doc_id}")

    # 2. ìš”ì•½ ëª¨ë“œ ê²°ì • ë° ê³„íš ìˆ˜ë¦½
    prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ ê³„íšì„ ì„¸ìš°ì„¸ìš”.
    ì§ˆë¬¸: {query}
    ë¬¸ì„œ ID: {doc_id}
    ì‹¤ì œ ì¡°í•­ ëª©ë¡:
    {actual_headers}
    
    [ì‘ì—…]
    1. ìš”ì•½ ëª¨ë“œ ê²°ì • (global: ì „ì²´ í•µì‹¬, section: ì¡°í•­ë³„ ìƒì„¸)
    2. ë°œê²¬ëœ 'ì‹¤ì œ ì¡°í•­ ëª©ë¡' ì¤‘ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆê±°ë‚˜ ìš”ì•½í•´ì•¼ í•  ì¡°í•­ ë²ˆí˜¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”.
    3. **ì ˆëŒ€ ì¡°í•­ ë²ˆí˜¸ë¥¼ ì§€ì–´ë‚´ì§€ ë§ê³ , ìœ„ì˜ ëª©ë¡ì— ìˆëŠ” ë²ˆí˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.**
    
    ë°˜ë“œì‹œ JSONìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”: 
    {{"doc_id": "{doc_id}", "mode": "global|section", "plan": ["1.1", "2.1", "5.4"]}}"""
    
    try:
        res = client.chat.completions.create(
            model=state["model"],
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        decision = json.loads(res.choices[0].message.content)
        return {
            "doc_id": decision.get("doc_id"),
            "summary_mode": decision.get("mode", "global"),
            "plan": decision.get("plan", []),
            "current_step": 0
        }
    except:
        return {"summary_mode": "global", "plan": [], "current_step": 0}

def worker_node(state: SummaryState):
    """[Worker] ê³„íšëœ ì¡°í•­ë³„ë¡œ ì •ë°€ ê²€ìƒ‰ ìˆ˜í–‰ (ìºì‹± íš¨ê³¼)"""
    query = state["query"]
    doc_id = state["doc_id"]
    plan = state["plan"]
    step = state["current_step"]
    
    # ê³„íšì´ ì—†ê±°ë‚˜ global ëª¨ë“œë©´ ì¼ë°˜ ê²€ìƒ‰
    if not plan or state["summary_mode"] == "global":
        search_res = search_sop_tool.invoke({
            "query": f"{doc_id} {query}",
            "target_sop_id": doc_id # íŠ¹ì • ë¬¸ì„œë¡œ í•œì •
        })
        return {"full_context": [search_res], "current_step": step + 1}
    
    # ì¡°í•­ë³„ ê²€ìƒ‰ (í˜„ì¬ ìŠ¤í…ì˜ ì¡°í•­) - ì •ë°€ íƒ€ê²©
    target_clause = plan[step]
    print(f"   ğŸ” [Deep Summary] {doc_id} {target_clause}ì¡° ë³¸ë¬¸ íƒ€ê²© ì¤‘...")
    
    search_query = f"{target_clause}"
    search_res = search_sop_tool.invoke({
        "query": search_query, 
        "target_sop_id": doc_id, # ë‹¤ë¥¸ ë¬¸ì„œ ë…¸ì´ì¦ˆ ì°¨ë‹¨
        "keywords": [target_clause]
    })
    
    return {
        "full_context": [f"### [ì œ{target_clause}ì¡° ì‹¤ì œ ë³¸ë¬¸ ë°ì´í„°]\n{search_res}"],
        "current_step": step + 1
    }

def finalizer_node(state: SummaryState):
    """[Finalizer] ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì·¨í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
    client = get_zai_client()
    query = state["query"]
    contexts = "\n\n".join(state["full_context"])
    mode = state["summary_mode"]
    
    if mode == "section":
        system_prompt = """ë‹¹ì‹ ì€ SOP ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ì¡°í•­ë³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‚¬ìš©ìì—ê²Œ ë°”ë¡œ ì „ë‹¬í•  ìµœì¢… ë³´ê³ ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        - ê° ì¡°í•­ë³„ë¡œ í•µì‹¬ ë‚´ìš©ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
        - ëˆ„ë½ëœ ì¡°í•­ì´ ìˆë‹¤ë©´ ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ ì •ë¦¬í•˜ë˜, ê°€ê¸‰ì  ìˆ˜ì§‘ëœ ë°ì´í„°ì— ì¶©ì‹¤í•˜ì„¸ìš”.
        - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""
    else:
        system_prompt = """ë‹¹ì‹ ì€ SOP ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì„œ ì „ì²´ì˜ í•µì‹¬ì„ ìš”ì•½í•˜ì—¬ **ìµœì¢… ë‹µë³€**ì„ ì‘ì„±í•˜ì„¸ìš”.
        - 5~8ê°œì˜ í•µì‹¬ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
        - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""
        
    res = client.chat.completions.create(
        model=state["model"],
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸: {query}\n\n[ìˆ˜ì§‘ëœ ë°ì´í„°]\n{contexts}"}
        ]
    )
    
    report_tag = "[ë”¥ ì—ì´ì „íŠ¸ - ì¡°í•­ë³„ ì •ë°€ ë¶„ì„]" if mode == "section" else "[ë”¥ ì—ì´ì „íŠ¸ - ì „ì²´ í•µì‹¬ ìš”ì•½]"
    return {"final_report": f"{report_tag}\n{res.choices[0].message.content}\n\n[DONE]"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê·¸ë˜í”„ êµ¬ì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_deep_summary_graph():
    workflow = StateGraph(SummaryState)
    
    workflow.add_node("planner", planner_node)
    workflow.add_node("worker", worker_node)
    workflow.add_node("finalizer", finalizer_node)
    
    workflow.add_edge(START, "planner")
    workflow.add_edge("planner", "worker")
    
    def should_continue(state: SummaryState):
        # ê³„íšëœ ëª¨ë“  ì¡°í•­ì„ ë‹¤ ì½ì—ˆê±°ë‚˜, global ëª¨ë“œë©´ ì¢…ë£Œ ë‹¨ê³„ë¡œ
        if state["summary_mode"] == "global" or state["current_step"] >= len(state["plan"]):
            return "finalizer"
        # ë” ì½ì–´ì•¼ í•  ì¡°í•­ì´ ë‚¨ì•˜ë‹¤ë©´ Worker ë°˜ë³µ
        return "worker"
    
    workflow.add_conditional_edges(
        "worker",
        should_continue,
        {
            "worker": "worker",
            "finalizer": "finalizer"
        }
    )
    
    workflow.add_edge("finalizer", END)
    return workflow.compile()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_deep_summary_app = None

def summary_agent_node(state: AgentState):
    """[ì„œë¶€] ìš”ì•½ ì—ì´ì „íŠ¸ (Deep Agent ë²„ì „)
    - ë‚´ë¶€ ê·¸ë˜í”„ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ê³„íšì„ ì„¸ìš°ê³  ì¡°í•­ë³„ë¡œ ì •ë°€í•˜ê²Œ ì½ìŠµë‹ˆë‹¤.
    """
    global _deep_summary_app
    if not _deep_summary_app:
        _deep_summary_app = create_deep_summary_graph()
        
    print(f"ğŸš€ [Deep Summary] ë”¥ ì—ì´ì „íŠ¸ ê°€ë™ ì‹œì‘: {state['query']}")
    
    initial_summary_state = {
        "query": state["query"],
        "doc_id": None,
        "full_context": [],
        "summary_mode": "global",
        "plan": [],
        "current_step": 0,
        "model": state.get("worker_model") or state.get("model_name") or "glm-4.7-flash",
        "final_report": ""
    }
    
    # ë‚´ë¶€ ë”¥ ë£¨í”„ ì‹¤í–‰ (ìµœëŒ€ 15ë‹¨ê³„ ì œí•œ)
    result = _deep_summary_app.invoke(initial_summary_state, config={"recursion_limit": 15})
    
    return {"messages": [{"role": "assistant", "content": result["final_report"]}]}
