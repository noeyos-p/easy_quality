"""
LLM as a Judge - ì—ì´ì „íŠ¸ ì‘ë‹µ í‰ê°€ ëª¨ë“ˆ

í‰ê°€ ë©”íŠ¸ë¦­:
- faithfulness: ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ (í™˜ê° ë°©ì§€)
- groundness: ë‹µë³€ì´ ì‹¤ì œ ê·¼ê±°ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€
- relevancy: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€
- correctness: ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€
"""

import os
import json
from typing import Dict, List, Optional
from openai import OpenAI


class AgentEvaluator:
    """LLM as a Judgeë¥¼ ì‚¬ìš©í•œ ì—ì´ì „íŠ¸ ì‘ë‹µ í‰ê°€ê¸°"""

    def __init__(self, judge_model: str = "gpt-4o-mini"):
        """
        Args:
            judge_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸: gpt-4o-mini)
        """
        self.judge_model = judge_model
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.client = OpenAI(api_key=api_key)

    def _call_judge(self, prompt: str) -> Dict[str, any]:
        """Judge LLM í˜¸ì¶œ"""
        try:
            response = self.client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"ğŸ”´ Judge í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"score": 0, "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"}

    def evaluate_faithfulness(self, question: str, answer: str, context: str) -> Dict[str, any]:
        """
        ì¶©ì‹¤ì„± í‰ê°€: ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ëŠ”ì§€ í‰ê°€
        + ì¸ìš©ëœ ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦

        Returns:
            {"score": 1-5, "reasoning": "í‰ê°€ ì´ìœ "}
        """
        prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ì¶©ì‹¤í•˜ê²Œ ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ **ë§¤ìš° ì—„ê²©í•˜ê²Œ** í‰ê°€í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ì œê³µëœ ì»¨í…ìŠ¤íŠ¸]
{context}

[ë‹µë³€]
{answer}

[í‰ê°€ ì ˆì°¨ - ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰]
1ë‹¨ê³„: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
2ë‹¨ê³„: [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì˜ ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦
   - ë‹µë³€ ëì˜ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì„ ì°¾ìœ¼ì„¸ìš”
   - í˜•ì‹: "EQ-SOP-00001(5.1.3, 5.4.2)"
   - ê° ë¬¸ì„œëª…ê³¼ ì¡°í•­ì„ ì¶”ì¶œí•˜ê³ 
   - ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ í•´ë‹¹ ì¡°í•­ì„ ì°¾ìœ¼ì„¸ìš”
   - ì—†ë‹¤ë©´ í• ë£¨ì‹œë„¤ì´ì…˜ì…ë‹ˆë‹¤
3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ì˜ ì¼ë°˜ë¡ ì´ë‚˜ ì¶”ë¡ ì´ ìˆëŠ”ì§€ í™•ì¸
4ë‹¨ê³„: ì¢…í•© í‰ê°€

[í‰ê°€ ê¸°ì¤€ - ë§¤ìš° ì—„ê²©í•˜ê²Œ ì ìš©]
5ì :
  - ë‹µë³€ì˜ ëª¨ë“  ë‹¨ì–´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ê°€ëŠ¥
  - ì¸ìš©ëœ ëª¨ë“  ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ê³  ì •í™•íˆ ì¼ì¹˜
  - ì¼ë°˜ ìƒì‹ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ 0%

4ì :
  - ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ë‚˜ ë‹¨ì–´ ìˆœì„œ ë³€ê²½ì´ë‚˜ ë§¤ìš° ê°„ë‹¨í•œ ìš”ì•½ í¬í•¨
  - ëª¨ë“  ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¡´ì¬
  - ì™¸ë¶€ ì§€ì‹ 5% ì´í•˜

3ì :
  - ì»¨í…ìŠ¤íŠ¸ ë‚´ìš© + ì¼ë°˜ì ì¸ ì—°ê²°ì–´ë‚˜ ì„¤ëª… ì¶”ê°€
  - ì¸ìš©ëœ ì¡°í•­ ì¤‘ 1-2ê°œê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ ë¶ˆê°€
  - ì™¸ë¶€ ì§€ì‹ 10% ì´í•˜

2ì :
  - ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ì˜ ì¶”ë¡ , í•´ì„, ì¼ë°˜ë¡ ì´ 20% ì´ìƒ
  - ì¸ìš©ëœ ì¡°í•­ì˜ 30% ì´ìƒì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ
  - í• ë£¨ì‹œë„¤ì´ì…˜ ì˜ì‹¬

1ì :
  - ì»¨í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•œ ë‚´ìš© 30% ì´ìƒ
  - ì¸ìš©ëœ ì¡°í•­ì˜ 50% ì´ìƒì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒ
  - ëª…í™•í•œ í• ë£¨ì‹œë„¤ì´ì…˜ ë°œê²¬

**ì¤‘ìš”**:
- ì¡°ê¸ˆì´ë¼ë„ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ê°ì í•˜ì„¸ìš”
- [ì°¸ê³  ë¬¸ì„œ]ì— ë‚˜ì—´ëœ ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì¦‰ì‹œ ëŒ€í­ ê°ì í•˜ì„¸ìš”
- **[ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ ìì²´ê°€ ì—†ìœ¼ë©´ ê²€ì¦ ë¶ˆê°€ì´ë¯€ë¡œ ë‚®ì€ ì ìˆ˜ë¥¼ ì¤˜ì•¼ í•©ë‹ˆë‹¤**

**CRITICAL ì²´í¬**:
- ë‹µë³€ ëì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆëŠ”ê°€?
- ì—†ìœ¼ë©´ 3ì  ì´ìƒ ì¤„ ìˆ˜ ì—†ìŒ

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{"score": 1-5, "reasoning": "í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…. [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ëª…ì‹œ. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì¡°í•­ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì "}}"""

        return self._call_judge(prompt)

    def evaluate_groundness(self, question: str, answer: str, context: str) -> Dict[str, any]:
        """
        ê·¼ê±°ì„± í‰ê°€: ë‹µë³€ì˜ ëª¨ë“  ì£¼ì¥ì´ ëª…í™•í•œ ê·¼ê±°ë¥¼ ê°€ì§€ëŠ”ì§€ í‰ê°€
        + [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆëŠ”ì§€ ê²€ì¦

        Returns:
            {"score": 1-5, "reasoning": "í‰ê°€ ì´ìœ "}
        """
        # ë””ë²„ê¹…: [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_ref = "[ì°¸ê³  ë¬¸ì„œ]" in answer
        print(f"ğŸ” [í‰ê°€ ë””ë²„ê¹…] ë‹µë³€ì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜: {'ìˆìŒ âœ…' if has_ref else 'ì—†ìŒ âŒ'}")
        if has_ref:
            # [ì°¸ê³  ë¬¸ì„œ] ì´í›„ ë‚´ìš© ì¶œë ¥
            ref_index = answer.find("[ì°¸ê³  ë¬¸ì„œ]")
            ref_content = answer[ref_index:ref_index+200]
            print(f"   [ì°¸ê³  ë¬¸ì„œ] ë‚´ìš©: {ref_content}")

        prompt = f"""ë‹¤ìŒ ë‹µë³€ì˜ ê·¼ê±°ì„±ì„ **ë§¤ìš° ì—„ê²©í•˜ê²Œ** í‰ê°€í•˜ì„¸ìš”.

**ğŸš¨ CRITICAL: ë‹µë³€ ì „ì²´ë¥¼ ëê¹Œì§€ ì½ìœ¼ì„¸ìš”!**
- [DONE] íƒœê·¸ê°€ ìˆì–´ë„ ê·¸ ì´í›„ì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë‹µë³€ í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ì—¬ "[ì°¸ê³  ë¬¸ì„œ]" ë¬¸ìì—´ì´ ìˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”

[ì§ˆë¬¸]
{question}

[ì œê³µëœ ì»¨í…ìŠ¤íŠ¸]
{context}

[ë‹µë³€]
{answer}

[í‰ê°€ ì ˆì°¨ - ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰]
1ë‹¨ê³„: ë‹µë³€ í…ìŠ¤íŠ¸ì—ì„œ "[ì°¸ê³  ë¬¸ì„œ]" ë¬¸ìì—´ì„ ê²€ìƒ‰í•˜ì„¸ìš”
   - ë‹µë³€ ì–´ë””ë“ ì§€ "[ì°¸ê³  ë¬¸ì„œ]" ë¬¸ìì—´ì´ ìˆëŠ”ì§€ ì°¾ìœ¼ì„¸ìš”
   - ì°¾ì•˜ë‹¤ë©´: ê·¸ ì•„ë˜ ì¤„ì— "ë¬¸ì„œëª…(ì¡°í•­1, ì¡°í•­2)" í˜•ì‹ì´ ìˆëŠ”ì§€ í™•ì¸
   - ì˜ˆì‹œ í˜•ì‹:
     ```
     [ì°¸ê³  ë¬¸ì„œ]
     EQ-SOP-00001(5.1.3, 5.4.2)
     ```
   ë˜ëŠ”
     ```
     [ì°¸ê³  ë¬¸ì„œ]
     EQ-SOP-00001(5.1.3, 5.4.2)

     [DONE]
     ```

2ë‹¨ê³„: [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì˜ ì¡°í•­ ê²€ì¦:
   - ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ í•´ë‹¹ ì¡°í•­ ë‚´ìš©ì„ ì°¾ìœ¼ì„¸ìš”
   - ë‹µë³€ ë‚´ìš©ì´ ì‹¤ì œë¡œ ê·¸ ì¡°í•­ë“¤ì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
   - ì¡°í•­ ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆë‹¤ë©´ ì¦‰ì‹œ ê°ì í•˜ì„¸ìš”

3ë‹¨ê³„: ì „ì²´ ë‹µë³€ì˜ ê·¼ê±°ì„±ì„ ì¢…í•© í‰ê°€í•˜ì„¸ìš”

[í‰ê°€ ê¸°ì¤€ - ë§¤ìš° ì—„ê²©í•˜ê²Œ ì ìš©]
**CRITICAL**: [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì ˆëŒ€ 3ì  ì´ìƒ ì¤„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!

5ì :
  - ë‹µë³€ ëì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆìŒ
  - [ì°¸ê³  ë¬¸ì„œ]ì— ë‚˜ì—´ëœ ëª¨ë“  ì¡°í•­ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ê³  ë‚´ìš©ì´ ì¼ì¹˜í•¨
  - ì¡°í•­ ë²ˆí˜¸ ì˜¤ë¥˜ 0ê°œ
  - ë‹µë³€ ë‚´ìš©ì´ ì°¸ê³ í•œ ì¡°í•­ë“¤ì˜ ë‚´ìš©ê³¼ ì¼ì¹˜í•¨

4ì :
  - [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆìŒ
  - ì¡°í•­ ëŒ€ë¶€ë¶„ ì •í™•í•˜ë‚˜ 1ê°œ ì •ë„ ë¶ˆëª…í™•
  - ì¡°í•­ ë²ˆí˜¸ ì˜¤ë¥˜ 0-1ê°œ

3ì :
  - [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆìœ¼ë‚˜ ì¡°í•­ ë²ˆí˜¸ê°€ ì‹¤ì œ ë‚´ìš©ê³¼ ë§ì§€ ì•ŠëŠ” ê²½ìš° 2-3ê°œ
  - ë˜ëŠ” ë‹µë³€ì— ê·¼ê±° ì—†ëŠ” í‘œí˜„("ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ")ì´ í¬í•¨ë¨

2ì :
  - [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆìœ¼ë‚˜ ì¡°í•­ ë²ˆí˜¸ì˜ 50% ì´ìƒì´ ì‹¤ì œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜
  - ê·¼ê±° ì—†ëŠ” ì£¼ì¥ ë‹¤ìˆ˜

1ì :
  - **[ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì „í˜€ ì—†ìŒ** â† ì´ ê²½ìš° ë¬´ì¡°ê±´ 1ì 
  - [ì°¸ê³  ë¬¸ì„œ]ê°€ ìˆì–´ë„ ëª¨ë“  ì¡°í•­ì´ í‹€ë¦¼
  - ëŒ€ë¶€ë¶„ ì¶”ì¸¡ì´ê±°ë‚˜ ì¼ë°˜ë¡ 
  - ì»¨í…ìŠ¤íŠ¸ì™€ ì—°ê²° ë¶ˆê°€

**í•µì‹¬ ê²€ì¦ ì‚¬í•­**:
- ë‹µë³€ ëì— "[ì°¸ê³  ë¬¸ì„œ]" ì„¹ì…˜ì´ ìˆëŠ”ê°€?
- ìˆë‹¤ë©´: EQ-SOP-00001(5.1.3, 5.4.2) ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë¬¸ì„œì™€ ì¡°í•­ì´ ë‚˜ì—´ë˜ì–´ ìˆëŠ”ê°€?
- ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì¡°í•­ë“¤ì„ ì°¾ì•„ ë‹µë³€ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸

**CRITICAL ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë°˜ë“œì‹œ í™•ì¸)**:
1. ë‹µë³€ ëì— [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ìˆëŠ”ê°€? (ìˆìœ¼ë©´ â—‹, ì—†ìœ¼ë©´ âœ—)
2. âœ—ë¼ë©´ ë¬´ì¡°ê±´ 1ì 

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{"score": 1-5, "reasoning": "í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…. [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë©´ 'ì¸ìš© ì—†ìŒ, 1ì ' ëª…ì‹œ. ì¡°í•­ ë²ˆí˜¸ ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì "}}"""

        return self._call_judge(prompt)

    def evaluate_relevancy(self, question: str, answer: str, context: str = None) -> Dict[str, any]:
        """
        ê´€ë ¨ì„± í‰ê°€: ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ”ì§€ í‰ê°€

        Returns:
            {"score": 1-5, "reasoning": "í‰ê°€ ì´ìœ "}
        """
        prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì§ì ‘ì ì´ê³  ê´€ë ¨ì„± ìˆê²Œ ë‹µí•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
{answer}

[í‰ê°€ ê¸°ì¤€]
5ì : ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€. ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ìŒ
4ì : ì§ˆë¬¸ì— ì˜ ë‹µí•˜ë‚˜, ì¼ë¶€ ë¶€ê°€ ì •ë³´ í¬í•¨
3ì : ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìœ¼ë‚˜, ìš°íšŒì ì´ê±°ë‚˜ ì¼ë¶€ ë¬´ê´€í•œ ë‚´ìš© í¬í•¨
2ì : ì§ˆë¬¸ê³¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ë ¨. ì£¼ìš” ì§ˆë¬¸ ë‚´ìš© ëˆ„ë½
1ì : ì§ˆë¬¸ê³¼ ê±°ì˜ ë¬´ê´€í•œ ë‹µë³€

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{"score": 1-5, "reasoning": "í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…"}}"""

        return self._call_judge(prompt)

    def evaluate_correctness(self, question: str, answer: str, context: str, reference_answer: str = None) -> Dict[str, any]:
        """
        ì •í™•ì„± í‰ê°€: ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€ í‰ê°€
        + ì¸ìš©ëœ ì¡°í•­ ë‚´ìš©ì´ ì‹¤ì œë¡œ ì •í™•í•œì§€ ê²€ì¦

        Args:
            reference_answer: ì°¸ì¡° ë‹µë³€ (ìˆì„ ê²½ìš°)

        Returns:
            {"score": 1-5, "reasoning": "í‰ê°€ ì´ìœ "}
        """
        ref_section = f"\n[ì°¸ì¡° ë‹µë³€]\n{reference_answer}\n" if reference_answer else ""

        prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€ **ë§¤ìš° ì—„ê²©í•˜ê²Œ** í‰ê°€í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ì œê³µëœ ì»¨í…ìŠ¤íŠ¸]
{context}
{ref_section}
[ë‹µë³€]
{answer}

[í‰ê°€ ì ˆì°¨ - ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰]
1ë‹¨ê³„: ë‹µë³€ì˜ ê° ë¬¸ì¥ì„ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€ì¡°
2ë‹¨ê³„: [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì˜ ì¡°í•­ ë²ˆí˜¸ê°€ ì‹¤ì œ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦
   - ë‹µë³€ ëì˜ [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ í™•ì¸: "EQ-SOP-00001(5.1.3, 5.4.2)"
   - ê²€ì¦: ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì¡°í•­ë“¤ì„ ì°¾ì•„ ì‹¤ì œë¡œ ë‹µë³€ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
   - ì¡°í•­ ë²ˆí˜¸ê°€ í‹€ë ¸ë‹¤ë©´ "ì‹¬ê°í•œ ì˜¤ë¥˜"ë¡œ ê°„ì£¼
3ë‹¨ê³„: í•„ìš”í•œ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ í™•ì¸
4ë‹¨ê³„: ì¢…í•© í‰ê°€

[í‰ê°€ ê¸°ì¤€ - ë§¤ìš° ì—„ê²©í•˜ê²Œ ì ìš©]
5ì :
  - ëª¨ë“  ë‚´ìš©ì´ ì •í™•í•¨
  - ì¸ìš©ëœ ëª¨ë“  ì¡°í•­ ë²ˆí˜¸ê°€ ì‹¤ì œ ë‚´ìš©ê³¼ ì •í™•íˆ ì¼ì¹˜
  - í•„ìš”í•œ ëª¨ë“  ì •ë³´ í¬í•¨
  - ì˜¤ë¥˜ 0ê°œ

4ì :
  - ëŒ€ì²´ë¡œ ì •í™•
  - ì¡°í•­ ë²ˆí˜¸ 1ê°œ ì •ë„ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ ëˆ„ë½
  - ê²½ë¯¸í•œ ë¶€ì •í™•í•¨ 1-2ê°œ

3ì :
  - í•µì‹¬ì€ ë§ìœ¼ë‚˜ ì¡°í•­ ë²ˆí˜¸ 2-3ê°œê°€ ì‹¤ì œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜
  - ë˜ëŠ” ì¤‘ìš”í•œ ì •ë³´ ëˆ„ë½
  - ì¼ë¶€ ë¶€ì •í™•í•¨

2ì :
  - ì¸ìš©ëœ ì¡°í•­ì˜ 50% ì´ìƒì´ ì‹¤ì œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜ (ì‹¬ê°í•œ í• ë£¨ì‹œë„¤ì´ì…˜)
  - ë˜ëŠ” ì—¬ëŸ¬ ì˜¤ë¥˜
  - ì£¼ìš” ì •ë³´ ëˆ„ë½

1ì :
  - ëŒ€ë¶€ë¶„ì˜ ì¡°í•­ ë²ˆí˜¸ê°€ í‹€ë¦¼
  - ì‹¬ê°í•œ ì˜¤ë¥˜ ë‹¤ìˆ˜
  - ëŒ€ë¶€ë¶„ì˜ ì •ë³´ ëˆ„ë½ ë˜ëŠ” ì˜ëª»ë¨

**ì¤‘ìš”**:
- ì¡°í•­ ë²ˆí˜¸ê°€ í‹€ë¦° ê²ƒì€ ë‹¨ìˆœ ì‹¤ìˆ˜ê°€ ì•„ë‹Œ ì‹¬ê°í•œ í• ë£¨ì‹œë„¤ì´ì…˜ì…ë‹ˆë‹¤
- [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ëŠ” ê²ƒì€ ê·¼ê±° ì—†ëŠ” ë‹µë³€ì´ë¯€ë¡œ ì‹¬ê°í•œ ë¬¸ì œì…ë‹ˆë‹¤

**CRITICAL ë…¼ë¦¬ ì˜¤ë¥˜ ë°©ì§€**:
- "[ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë¯€ë¡œ ì˜¤ë¥˜ê°€ ì—†ë‹¤" â† ì´ê²ƒì€ ì˜ëª»ëœ ë…¼ë¦¬ì…ë‹ˆë‹¤!
- [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì˜¤íˆë ¤ í° ë¬¸ì œì´ë©° ë‚®ì€ ì ìˆ˜ë¥¼ ì¤˜ì•¼ í•©ë‹ˆë‹¤

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
{{"score": 1-5, "reasoning": "í‰ê°€ ê·¼ê±°ë¥¼ í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…. [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì´ ì—†ìœ¼ë©´ 'ê·¼ê±° ì—†ìŒ'ì„ ëª…ì‹œí•˜ê³  ê°ì . ì¡°í•­ ë²ˆí˜¸ ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ê³  ëª‡ ê°œë‚˜ í‹€ë ¸ëŠ”ì§€ ëª…ì‹œ"}}"""

        return self._call_judge(prompt)

    def evaluate_single(
        self,
        question: str,
        answer: str,
        context: str = "",
        metrics: List[str] = None,
        reference_answer: str = None
    ) -> Dict[str, Dict[str, any]]:
        """
        ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€ ìŒì— ëŒ€í•œ ì¢…í•© í‰ê°€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            answer: ì—ì´ì „íŠ¸ ë‹µë³€
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (ìˆì„ ê²½ìš°)
            metrics: í‰ê°€í•  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ì „ì²´)
            reference_answer: ì°¸ì¡° ë‹µë³€ (ìˆì„ ê²½ìš°)

        Returns:
            {"faithfulness": {...}, "groundness": {...}, ...}
        """
        if metrics is None:
            metrics = ["faithfulness", "groundness", "relevancy", "correctness"]

        results = {}

        if "faithfulness" in metrics and context:
            results["faithfulness"] = self.evaluate_faithfulness(question, answer, context)

        if "groundness" in metrics and context:
            results["groundness"] = self.evaluate_groundness(question, answer, context)

        if "relevancy" in metrics:
            results["relevancy"] = self.evaluate_relevancy(question, answer, context)

        if "correctness" in metrics and context:
            results["correctness"] = self.evaluate_correctness(
                question, answer, context, reference_answer
            )

        return results

    def evaluate_batch(
        self,
        qa_pairs: List[Dict[str, str]],
        metrics: List[str] = None
    ) -> List[Dict[str, any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸-ë‹µë³€ ìŒì— ëŒ€í•œ ë°°ì¹˜ í‰ê°€

        Args:
            qa_pairs: [{"question": "...", "answer": "...", "context": "..."}, ...]
            metrics: í‰ê°€í•  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸

        Returns:
            [{"question": "...", "scores": {...}}, ...]
        """
        results = []

        for i, qa in enumerate(qa_pairs):
            print(f"ğŸŸ¢ í‰ê°€ ì§„í–‰ ì¤‘: {i+1}/{len(qa_pairs)}")

            scores = self.evaluate_single(
                question=qa.get("question", ""),
                answer=qa.get("answer", ""),
                context=qa.get("context", ""),
                metrics=metrics,
                reference_answer=qa.get("reference_answer")
            )

            results.append({
                "question": qa.get("question"),
                "answer": qa.get("answer"),
                "scores": scores
            })

        return results

    def get_average_scores(self, batch_results: List[Dict[str, any]]) -> Dict[str, float]:
        """
        ë°°ì¹˜ í‰ê°€ ê²°ê³¼ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°

        Returns:
            {"faithfulness": í‰ê· ì ìˆ˜, "groundness": í‰ê· ì ìˆ˜, ...}
        """
        metric_sums = {}
        metric_counts = {}

        for result in batch_results:
            scores = result.get("scores", {})
            for metric, score_data in scores.items():
                score = score_data.get("score", 0)
                metric_sums[metric] = metric_sums.get(metric, 0) + score
                metric_counts[metric] = metric_counts.get(metric, 0) + 1

        averages = {}
        for metric in metric_sums:
            averages[metric] = metric_sums[metric] / metric_counts[metric] if metric_counts[metric] > 0 else 0

        return averages


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    evaluator = AgentEvaluator(judge_model="gpt-4o-mini")

    # ë‹¨ì¼ í‰ê°€ ì˜ˆì‹œ
    result = evaluator.evaluate_single(
        question="ì‘ì—…ì§€ì¹¨ì„œê°€ ë­ì•¼?",
        answer="ì‘ì—…ì§€ì¹¨ì„œëŠ” í˜„ì¥ì—ì„œ ìˆ˜í–‰ë˜ëŠ” ì—…ë¬´ë¥¼ ì¼ê´€ë˜ê²Œ ìš´ì˜í•˜ê¸° ìœ„í•œ ì§€ì¹¨ ë¬¸ì„œì…ë‹ˆë‹¤.",
        context="[ê²€ìƒ‰] EQ-SOP-00001 > 5.1.3: ì‘ì—…ì§€ì¹¨ì„œ(WI)ëŠ” ë¶€ì„œ ë˜ëŠ” ê³µì • ë‹¨ìœ„ì˜ ìš´ì˜ íë¦„ê³¼ ê´€ë¦¬ ë°©ë²•ì„ ê·œì •í•˜ëŠ” ë¬¸ì„œì…ë‹ˆë‹¤.",
        metrics=["faithfulness", "relevancy"]
    )

    print(json.dumps(result, ensure_ascii=False, indent=2))
