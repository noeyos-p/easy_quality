"""
LLM as a Judge - ì—ì´ì „íŠ¸ ì‘ë‹µ í‰ê°€ ëª¨ë“ˆ

í‰ê°€ ë©”íŠ¸ë¦­:
- faithfulness: ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ (í™˜ê° ë°©ì§€)
- groundness: ë‹µë³€ì´ ì‹¤ì œ ê·¼ê±°ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€
- relevancy: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€
- correctness: ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€

íŠ¹ì§•:
- **ë¬´ì¡°ê±´ RDB(PostgreSQL)ì—ì„œ ì‹¤ì œ ë¬¸ì„œë¥¼ ì¡°íšŒí•˜ì—¬ ì¸ìš© ì •í™•ì„± ê²€ì¦**
- [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜ì˜ ì¡°í•­ ë²ˆí˜¸ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ DBì—ì„œ í™•ì¸
- ì¸ë¼ì¸ ì¸ìš© (ë¬¸ì„œëª… > ì¡°í•­)ë„ ê²€ì¦
"""

import os
import json
import re
from typing import Dict, List, Optional, Tuple
from openai import OpenAI


class AgentEvaluator:
    """LLM as a Judgeë¥¼ ì‚¬ìš©í•œ ì—ì´ì „íŠ¸ ì‘ë‹µ í‰ê°€ê¸° (RDB ê²€ì¦ í•„ìˆ˜)"""

    def __init__(self, judge_model: str = "gpt-4o-mini", sql_store=None):
        """
        Args:
            judge_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸
            sql_store: SQL ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í† ì–´ (ì‹¤ì œ ë¬¸ì„œ ê²€ì¦ìš© - í•„ìˆ˜!)
        """
        if not sql_store:
            raise ValueError("sql_storeëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. RDBì—ì„œ ë¬¸ì„œë¥¼ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.")

        self.judge_model = judge_model
        self.sql_store = sql_store

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = OpenAI(api_key=api_key)

    def _call_judge(self, prompt: str) -> Dict:
        """Judge LLM í˜¸ì¶œ"""
        try:
            response = self.client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"ğŸ”´ Judge í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"score": 0, "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"}

    def _extract_all_citations(self, answer: str) -> List[Tuple[str, str]]:
        """
        ë‹µë³€ì—ì„œ ëª¨ë“  ì¸ìš© ì¶”ì¶œ (RDB ê²€ì¦ìš©)

        Returns:
            [(doc_id, clause_num), ...]
            ì˜ˆ: [("EQ-SOP-00001", "5.1.3"), ("EQ-SOP-00002", "3.2")]
        """
        citations = []

        # 1. [ì°¸ê³  ë¬¸ì„œ] ì„¹ì…˜: EQ-SOP-00001(5.1.3, 5.4.2)
        ref_pattern = r'\[ì°¸ê³  ë¬¸ì„œ\](.*?)(?:\[DONE\]|$)'
        ref_match = re.search(ref_pattern, answer, re.DOTALL)

        if ref_match:
            ref_section = ref_match.group(1).strip()
            # ë¬¸ì„œ IDì™€ ì¡°í•­ íŒŒì‹±
            doc_pattern = r'([A-Z]+-[A-Z]+-\d+)\s*\(([^)]+)\)'
            for match in re.finditer(doc_pattern, ref_section):
                doc_id = match.group(1)
                clauses_str = match.group(2)
                clauses = [c.strip() for c in clauses_str.split(',')]
                for clause in clauses:
                    # ì¡°í•­ ë²ˆí˜¸ë§Œ ì¶”ì¶œ (ìˆ«ì.ìˆ«ì í˜•ì‹)
                    clause_match = re.match(r'([\d\.]+)', clause)
                    if clause_match:
                        citations.append((doc_id, clause_match.group(1)))

        # 2. ì¸ë¼ì¸ ì¸ìš©: (EQ-SOP-00001 > 3.1)
        inline_pattern = r'\(([A-Z]+-[A-Z]+-\d+)\s*>\s*([\d\.]+)'
        for match in re.finditer(inline_pattern, answer):
            doc_id = match.group(1)
            clause_num = match.group(2)
            citations.append((doc_id, clause_num))

        # 3. ë§ˆí¬ë‹¤ìš´ ê°•ì¡° ì¸ìš©: **[EQ-SOP-00001 > 3.1]**
        markdown_pattern = r'\*\*\[([A-Z]+-[A-Z]+-\d+)\s*>\s*([\d\.]+)'
        for match in re.finditer(markdown_pattern, answer):
            doc_id = match.group(1)
            clause_num = match.group(2)
            citations.append((doc_id, clause_num))

        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        seen = set()
        unique_citations = []
        for doc_id, clause_num in citations:
            key = (doc_id, clause_num)
            if key not in seen:
                seen.add(key)
                unique_citations.append((doc_id, clause_num))

        return unique_citations

    def _verify_against_rdb(self, answer: str) -> Dict:
        """
        **í•µì‹¬: RDBì—ì„œ ì‹¤ì œ ë¬¸ì„œë¥¼ ì¡°íšŒí•˜ì—¬ ë‹µë³€ ê²€ì¦**

        Returns:
            {
                "has_citations": bool,
                "total_citations": int,
                "verified_citations": int,
                "incorrect_citations": List[str],
                "accuracy_rate": float,
                "verification_details": str
            }
        """
        # ë‹µë³€ì—ì„œ ëª¨ë“  ì¸ìš© ì¶”ì¶œ
        citations = self._extract_all_citations(answer)

        if not citations:
            return {
                "has_citations": False,
                "total_citations": 0,
                "verified_citations": 0,
                "incorrect_citations": [],
                "accuracy_rate": 0.0,
                "verification_details": "âŒ ì¸ìš© ì—†ìŒ - ë‹µë³€ì— ë¬¸ì„œ ê·¼ê±°ê°€ ëª…ì‹œë˜ì§€ ì•ŠìŒ"
            }

        total_citations = len(citations)
        verified_citations = 0
        incorrect_citations = []
        details = []

        # ê° ì¸ìš©ì„ RDBì—ì„œ ê²€ì¦
        for doc_id, clause_num in citations:
            try:
                # RDBì—ì„œ ë¬¸ì„œ ì¡°íšŒ (ìµœì‹  ë²„ì „)
                doc = self.sql_store.get_document_by_name(doc_id)

                if not doc:
                    incorrect_citations.append(f"{doc_id}:{clause_num}")
                    details.append(f"âŒ {doc_id}:{clause_num} - ë¬¸ì„œê°€ RDBì— ì—†ìŒ")
                    continue

                # ë¬¸ì„œì˜ ì²­í¬ ì¡°íšŒ
                chunks = self.sql_store.get_chunks_by_document(doc['id'])

                # í•´ë‹¹ ì¡°í•­ ë²ˆí˜¸ì˜ ì²­í¬ ì°¾ê¸°
                found = False
                for chunk in chunks:
                    chunk_clause = chunk.get('clause', '')
                    # ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™” ë¹„êµ (7.5 == 7.5.0)
                    if chunk_clause == clause_num or chunk_clause.startswith(clause_num + '.'):
                        found = True
                        break

                if found:
                    verified_citations += 1
                    details.append(f"âœ… {doc_id}:{clause_num}")
                else:
                    incorrect_citations.append(f"{doc_id}:{clause_num}")
                    details.append(f"âŒ {doc_id}:{clause_num} - ì¡°í•­ì´ ë¬¸ì„œì— ì—†ìŒ")

            except Exception as e:
                incorrect_citations.append(f"{doc_id}:{clause_num}")
                details.append(f"âš ï¸ {doc_id}:{clause_num} - RDB ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")

        accuracy_rate = (verified_citations / total_citations * 100) if total_citations > 0 else 0.0

        return {
            "has_citations": True,
            "total_citations": total_citations,
            "verified_citations": verified_citations,
            "incorrect_citations": incorrect_citations,
            "accuracy_rate": round(accuracy_rate, 1),
            "verification_details": "\\n".join(details)
        }

    def evaluate_faithfulness(self, question: str, answer: str, context: str) -> Dict:
        """
        ì¶©ì‹¤ì„± í‰ê°€: ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê¸°ë°˜í•˜ëŠ”ì§€ í‰ê°€
        **RDB ê²€ì¦ ê²°ê³¼ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤**
        """
        # RDB ê²€ì¦ (í•„ìˆ˜)
        rdb_verification = self._verify_against_rdb(answer)

        rdb_info = f"""
[RDB ê²€ì¦ ê²°ê³¼ - ìµœìš°ì„  ê³ ë ¤!]
- ì¸ìš© ì¡´ì¬: {'ì˜ˆ' if rdb_verification['has_citations'] else 'ì•„ë‹ˆì˜¤'}
- ê²€ì¦ëœ ì¸ìš©: {rdb_verification['verified_citations']}/{rdb_verification['total_citations']}
- ì •í™•ë„: {rdb_verification['accuracy_rate']}%
- í‹€ë¦° ì¸ìš©: {len(rdb_verification['incorrect_citations'])}ê°œ

ê²€ì¦ ìƒì„¸:
{rdb_verification['verification_details']}
"""

        prompt = f"""ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ì¶©ì‹¤í•˜ê²Œ ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

**RDB ê²€ì¦ ê²°ê³¼ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”!**
{rdb_info}

[ì§ˆë¬¸] {question}
[ì»¨í…ìŠ¤íŠ¸] {context[:2000]}...
[ë‹µë³€] {answer[:1500]}...

[í‰ê°€ ê¸°ì¤€ - RDB ê²€ì¦ ê²°ê³¼ ì¤‘ì‹¬]
5ì : RDB ì •í™•ë„ 100%, ëª¨ë“  ì¸ìš©ì´ ì‹¤ì œ ë¬¸ì„œì— ì¡´ì¬, ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
4ì : RDB ì •í™•ë„ 90% ì´ìƒ, 1ê°œ ì¸ìš©ë§Œ ë¶ˆì¼ì¹˜
3ì : RDB ì •í™•ë„ 70-89%, 2-3ê°œ ì¸ìš© ë¶ˆì¼ì¹˜
2ì : RDB ì •í™•ë„ 50-69%, 30% ì´ìƒ ë¶ˆì¼ì¹˜
1ì : RDB ì •í™•ë„ 50% ë¯¸ë§Œ ë˜ëŠ” ì¸ìš© ì—†ìŒ

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{"score": 1-5, "reasoning": "RDB ê²€ì¦ ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€ ê·¼ê±° ì„¤ëª…"}}"""

        result = self._call_judge(prompt)
        result["rdb_verification"] = rdb_verification
        return result

    def evaluate_groundness(self, question: str, answer: str, context: str) -> Dict:
        """
        ê·¼ê±°ì„± í‰ê°€: ë‹µë³€ì˜ ëª¨ë“  ì£¼ì¥ì´ ëª…í™•í•œ ê·¼ê±°ë¥¼ ê°€ì§€ëŠ”ì§€
        **RDB ê²€ì¦ í•„ìˆ˜**
        """
        rdb_verification = self._verify_against_rdb(answer)

        rdb_info = f"""
[RDB ê²€ì¦ ê²°ê³¼ - ìµœìš°ì„ !]
- ì¸ìš© ì¡´ì¬: {'ì˜ˆ' if rdb_verification['has_citations'] else 'ì•„ë‹ˆì˜¤'}
- ê²€ì¦ëœ ì¸ìš©: {rdb_verification['verified_citations']}/{rdb_verification['total_citations']}
- ì •í™•ë„: {rdb_verification['accuracy_rate']}%
"""

        prompt = f"""ë‹µë³€ì˜ ê·¼ê±°ì„±ì„ í‰ê°€í•˜ì„¸ìš”.

**RDB ê²€ì¦ ê²°ê³¼ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”!**
{rdb_info}

[ì§ˆë¬¸] {question}
[ë‹µë³€] {answer[:1500]}...

[í‰ê°€ ê¸°ì¤€]
5ì : ì¸ìš© ìˆê³  RDB ì •í™•ë„ 100%, ëª¨ë“  ì£¼ì¥ì— ëª…í™•í•œ ê·¼ê±°
4ì : ì¸ìš© ìˆê³  RDB ì •í™•ë„ 90% ì´ìƒ
3ì : ì¸ìš© ìˆê³  RDB ì •í™•ë„ 70-89%
2ì : ì¸ìš© ìˆê³  RDB ì •í™•ë„ 50-69%
1ì : ì¸ìš© ì—†ìŒ ë˜ëŠ” RDB ì •í™•ë„ 50% ë¯¸ë§Œ

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{"score": 1-5, "reasoning": "RDB ê²€ì¦ ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€"}}"""

        result = self._call_judge(prompt)
        result["rdb_verification"] = rdb_verification
        return result

    def evaluate_relevancy(self, question: str, answer: str, context: str = None) -> Dict:
        """ê´€ë ¨ì„± í‰ê°€: ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ”ì§€"""
        prompt = f"""ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

[ì§ˆë¬¸] {question}
[ë‹µë³€] {answer[:1500]}...

[í‰ê°€ ê¸°ì¤€]
5ì : ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ì§ì ‘ ë‹µë³€, ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ìŒ
4ì : ì§ˆë¬¸ì— ì˜ ë‹µí•˜ë‚˜ ì¼ë¶€ ë¶€ê°€ ì •ë³´ í¬í•¨
3ì : ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìœ¼ë‚˜ ìš°íšŒì ì´ê±°ë‚˜ ì¼ë¶€ ë¬´ê´€í•œ ë‚´ìš©
2ì : ì§ˆë¬¸ê³¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ë ¨, ì£¼ìš” ë‚´ìš© ëˆ„ë½
1ì : ì§ˆë¬¸ê³¼ ê±°ì˜ ë¬´ê´€

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{"score": 1-5, "reasoning": "í‰ê°€ ê·¼ê±°"}}"""

        return self._call_judge(prompt)

    def evaluate_correctness(
        self,
        question: str,
        answer: str,
        context: str,
        reference_answer: str = None
    ) -> Dict:
        """
        ì •í™•ì„± í‰ê°€: ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€
        **RDB ê²€ì¦ í•„ìˆ˜**
        """
        rdb_verification = self._verify_against_rdb(answer)

        rdb_info = f"""
[RDB ê²€ì¦ ê²°ê³¼ - ìµœìš°ì„ !]
- ì¸ìš© ì¡´ì¬: {'ì˜ˆ' if rdb_verification['has_citations'] else 'ì•„ë‹ˆì˜¤'}
- ê²€ì¦ëœ ì¸ìš©: {rdb_verification['verified_citations']}/{rdb_verification['total_citations']}
- ì •í™•ë„: {rdb_verification['accuracy_rate']}%
- í‹€ë¦° ì¸ìš©: {', '.join(rdb_verification['incorrect_citations'][:5])}
"""

        ref_section = f"\\n[ì°¸ì¡° ë‹µë³€] {reference_answer}\\n" if reference_answer else ""

        prompt = f"""ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•œì§€ í‰ê°€í•˜ì„¸ìš”.

**RDB ê²€ì¦ ê²°ê³¼ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”!**
{rdb_info}

[ì§ˆë¬¸] {question}
[ì»¨í…ìŠ¤íŠ¸] {context[:2000]}...
{ref_section}
[ë‹µë³€] {answer[:1500]}...

[í‰ê°€ ê¸°ì¤€ - RDB ê²€ì¦ ì¤‘ì‹¬]
5ì : RDB ì •í™•ë„ 100%, ëª¨ë“  ë‚´ìš© ì •í™•, í•„ìš”í•œ ì •ë³´ ëª¨ë‘ í¬í•¨
4ì : RDB ì •í™•ë„ 90% ì´ìƒ, ëŒ€ì²´ë¡œ ì •í™•í•˜ë‚˜ ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ ëˆ„ë½
3ì : RDB ì •í™•ë„ 70-89%, í•µì‹¬ì€ ë§ìœ¼ë‚˜ ì¤‘ìš” ì •ë³´ ëˆ„ë½
2ì : RDB ì •í™•ë„ 50-69%, ì—¬ëŸ¬ ì˜¤ë¥˜ ì¡´ì¬
1ì : RDB ì •í™•ë„ 50% ë¯¸ë§Œ, ì¸ìš© ì—†ìŒ, ì‹¬ê°í•œ ì˜¤ë¥˜ ë‹¤ìˆ˜

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{"score": 1-5, "reasoning": "RDB ê²€ì¦ ê²°ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€. í‹€ë¦° ì¸ìš© ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì "}}"""

        result = self._call_judge(prompt)
        result["rdb_verification"] = rdb_verification
        return result

    def evaluate_single(
        self,
        question: str,
        answer: str,
        context: str = "",
        metrics: List[str] = None,
        reference_answer: str = None
    ) -> Dict[str, Dict]:
        """
        ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€ ìŒì— ëŒ€í•œ ì¢…í•© í‰ê°€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            answer: ì—ì´ì „íŠ¸ ë‹µë³€
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            metrics: í‰ê°€í•  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: ì „ì²´)
            reference_answer: ì°¸ì¡° ë‹µë³€ (ì˜µì…˜)

        Returns:
            {"faithfulness": {...}, "groundness": {...}, "relevancy": {...}, "correctness": {...}}
        """
        if metrics is None:
            metrics = ["faithfulness", "groundness", "relevancy", "correctness"]

        results = {}

        print("\\n" + "="*80)
        print("ğŸ” LLM as a Judge í‰ê°€ ì‹œì‘ (RDB ê²€ì¦ í¬í•¨)")
        print("="*80)

        if "faithfulness" in metrics and context:
            print("\\n[1/4] Faithfulness í‰ê°€ ì¤‘...")
            results["faithfulness"] = self.evaluate_faithfulness(question, answer, context)
            print(f"   ì ìˆ˜: {results['faithfulness'].get('score', 0)}/5")

        if "groundness" in metrics and context:
            print("\\n[2/4] Groundness í‰ê°€ ì¤‘...")
            results["groundness"] = self.evaluate_groundness(question, answer, context)
            print(f"   ì ìˆ˜: {results['groundness'].get('score', 0)}/5")

        if "relevancy" in metrics:
            print("\\n[3/4] Relevancy í‰ê°€ ì¤‘...")
            results["relevancy"] = self.evaluate_relevancy(question, answer, context)
            print(f"   ì ìˆ˜: {results['relevancy'].get('score', 0)}/5")

        if "correctness" in metrics and context:
            print("\\n[4/4] Correctness í‰ê°€ ì¤‘...")
            results["correctness"] = self.evaluate_correctness(question, answer, context, reference_answer)
            print(f"   ì ìˆ˜: {results['correctness'].get('score', 0)}/5")

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        scores = [r.get("score", 0) for r in results.values() if "score" in r]
        avg_score = sum(scores) / len(scores) if scores else 0

        print("\\n" + "="*80)
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ - í‰ê·  ì ìˆ˜: {avg_score:.1f}/5")
        print("="*80)

        results["average_score"] = round(avg_score, 1)
        return results
