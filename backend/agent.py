"""
SOP ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ v14.0
- Orchestrator (Main): OpenAI (GPT-4o-mini) - ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…, ìµœì¢… ë‹µë³€
- Specialized Sub-Agents: OpenAI (GPT-4o-mini) - ì‹¤í–‰ ë° ë°ì´í„° ì²˜ë¦¬
  1. Retrieval Agent: ë¬¸ì„œ ê²€ìƒ‰ ë° ì¶”ì¶œ
  2. Summarization Agent: ë¬¸ì„œ/ì¡°í•­ ìš”ì•½
  3. Comparison Agent: ë²„ì „ ë¹„êµ
  4. Graph Agent: ì°¸ì¡° ê´€ê³„ ì¡°íšŒ
"""

import os
import re
import json
import operator
import hashlib
import difflib
from typing import List, Dict, Optional, Any, Annotated, TypedDict, Literal
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„í¬íŠ¸ ë° ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from openai import OpenAI
    from langchain_openai import ChatOpenAI
except ImportError:
    OpenAI = None
    ChatOpenAI = None

try:
    from zai import ZaiClient
    ZAI_AVAILABLE = True
except ImportError:
    ZAI_AVAILABLE = False
    pass

try:
    from langchain_core.tools import tool
    from langgraph.graph import StateGraph, START, END
    from langsmith import traceable
    LANGCHAIN_AVAILABLE = True
    LANGGRAPH_AGENT_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°: ì•ˆì „í•œ íŒŒì‹± ë° ì •ê·œí™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def safe_json_loads(text: str) -> dict:
    """ë§ˆí¬ë‹¤ìš´ íƒœê·¸ë‚˜ íŠ¸ë ˆì¼ë§ ì½¤ë§ˆê°€ í¬í•¨ëœ LLMì˜ JSON ì‘ë‹µì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±"""
    if not text: return {}
    if isinstance(text, dict): return text
    
    try:
        # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        clean_text = re.sub(r'^```(?:json)?\s*', '', text.strip())
        clean_text = re.sub(r'\s*```$', '', clean_text.strip())
        
        # 2. íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±°
        clean_text = re.sub(r',\s*}', '}', clean_text)
        
        return json.loads(clean_text)
    except:
        # ì •ê·œì‹ìœ¼ë¡œ í•µì‹¬ í•„ë“œ ì¶”ì¶œ ì‹œë„ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        res = {}
        for key in ["doc_id", "target_clause", "intent", "next_action", "plan", "mode"]:
            match = re.search(f'"{key}"\\s*:\\s*"([^"]+)"', text)
            if match: res[key] = match.group(1)
        return res

def normalize_doc_id(text: Optional[str]) -> Optional[str]:
    """ì˜¤íƒ€ê°€ ì„ì¸ ID(eEQ-SOP-00009)ë¥¼ ì •ê·œí™”í•˜ì—¬ ì‹¤ì œ IDë¥¼ ë°˜í™˜"""
    if not text: return None
    # SOP-00000 ë˜ëŠ” SOP-000 í˜•ì‹ ì¶”ì¶œ
    match = re.search(r'([A-Z0-9]+-SOP-\d+)', text.upper())
    if match:
        return match.group(1)
    return text.upper()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì „ì—­ ìŠ¤í† ì–´ ë° í´ë¼ì´ì–¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_graph_store = None
_sql_store = None

_openai_client = None
_zai_client = None

def init_agent_tools(vector_store_module, graph_store_instance, sql_store_instance=None):
    global _vector_store, _graph_store, _sql_store
    _vector_store = vector_store_module
    _graph_store = graph_store_instance
    _sql_store = sql_store_instance
    
    # ì„œë¸Œ ì—ì´ì „íŠ¸ ìŠ¤í† ì–´ ì´ˆê¸°í™” (ê·¸ë˜í”„ ìŠ¤í† ì–´ ì¶”ê°€)
    try:
        from backend.sub_agent.search import init_search_stores
        init_search_stores(vector_store_module, sql_store_instance, graph_store_instance)
    except ImportError:
        pass

def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ì§ì ‘ API í˜¸ì¶œìš©)"""
    global _openai_client
    if not _openai_client:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            _openai_client = OpenAI(api_key=api_key)
    return _openai_client

_langchain_llm = None

def get_langchain_llm(model: str = "gpt-4o-mini", temperature: float = 0.0):
    """LangChain ChatOpenAI ë°˜í™˜ (LangSmith ì¶”ì ìš©)"""
    if ChatOpenAI is None:
        raise ImportError("langchain-openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

def get_zai_client():
    global _zai_client
    if not _zai_client:
        api_key = os.getenv("ZAI_API_KEY")
        if api_key:
            _zai_client = ZaiClient(api_key=api_key)
    return _zai_client

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë„êµ¬ ì •ì˜ (Tools)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool
def search_sop_tool(query: str, extract_english: bool = False, keywords: List[str] = None, target_doc_id: str = None) -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬.
    Hybrid Search(BM25 + Vector) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œì™€ ì˜ë¯¸ë¡ ì  ì—°ê´€ì„±ì„ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.
    extract_english: Trueë©´ ì˜ë¬¸ ë‚´ìš© ìœ„ì£¼ë¡œ ì¶”ì¶œ
    target_doc_id: íŠ¹ì • ë¬¸ì„œ ID(ì˜ˆ: EQ-SOP-00001)ë¡œ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í•œì •í•  ë•Œ ì‚¬ìš©
    """
    global _vector_store, _sql_store
    
    results = []
    seen_content = set() # ì¤‘ë³µ ë‚´ìš© ë°©ì§€
    
    # 1. ë²¡í„° ìŠ¤í† ì–´ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œìš© (v8.0+)
    if _vector_store:
        search_query = query
        if keywords:
            # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì— ë³´ê°•í•˜ì—¬ BM25 ì ìˆ˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬
            search_query += " " + " ".join(keywords)
            
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (alpha=0.4: í‚¤ì›Œë“œ ë¹„ì¤‘ ì•½ê°„ ë†’ì„)
        vec_res = []
        try:
            # vector_store ëª¨ë“ˆì— êµ¬í˜„ëœ search_hybrid í˜¸ì¶œ
            vec_res = _vector_store.search_hybrid(search_query, n_results=10, alpha=0.4)
        except AttributeError:
            # ë§Œì•½ êµ¬í˜„ì´ ì•„ì§ ì•ˆë˜ì—ˆë‹¤ë©´ ê¸°ë³¸ search ì‚¬ìš©
            vec_res = _vector_store.search(search_query, n_results=10)
            
        for r in vec_res:
            meta = r.get('metadata', {})
            doc_id = meta.get('doc_id') or meta.get('doc_id') or meta.get('doc_name', 'Unknown')
            clause_id = meta.get('clause_id', '')
            title = meta.get('title', '')
            section = f"{clause_id} {title}" if clause_id and title else (meta.get('section') or meta.get('clause') or "ë³¸ë¬¸")
            content = r.get('text', '')
            
            if target_doc_id and doc_id.upper() != target_doc_id.upper():
                continue
            
            if not content: continue
            
            # í•´ì‹œë¡œ ì¤‘ë³µ ì²´í¬
            content_hash = hashlib.md5(content.encode()).hexdigest()
            if content_hash in seen_content: continue
            seen_content.add(content_hash)

            display_header = f"[ê²€ìƒ‰] {doc_id} > {section}"
            
            # ìš”ì•½ìš© ì •ë°€ ê²€ìƒ‰(target_doc_id ì§€ì •) ì‹œì—ëŠ” ê¸€ì ìˆ˜ ì œí•œ ëŒ€í­ ì™„í™”
            limit = 8000 if target_doc_id else 1500
            
            if extract_english:
                # ì˜ë¬¸ ì¶”ì¶œ ë¡œì§: ì•ŒíŒŒë²³ ë¹„ìœ¨ì´ í•œê¸€ë³´ë‹¤ ë†’ì€ ë¬¸ë‹¨ í•„í„°ë§
                paragraphs = content.split('\n\n')
                eng_paras = []
                for p in paragraphs:
                    eng_count = len(re.findall(r'[a-zA-Z]', p))
                    kor_count = len(re.findall(r'[ê°€-í£]', p))
                    if eng_count > kor_count and eng_count > 10:
                        eng_paras.append(p)
                
                if eng_paras:
                    results.append(f"{display_header} (ì˜ë¬¸):\n" + "\n\n".join(eng_paras[:3]))
                else:
                    results.append(f"{display_header}:\n{content[:limit]}...")
            else:
                results.append(f"{display_header}:\n{content[:limit]}")

    # 2. ê²°ê³¼ê°€ ì „í˜€ ì—†ê±°ë‚˜ ë§¤ìš° ì ì„ ê²½ìš° SQL í‚¤ì›Œë“œ ë§¤ì¹­ (ë³´ì¡°/í™•ì •ì  ê²€ìƒ‰)
    if len(results) < 2 and _sql_store and keywords:
        all_docs = _sql_store.list_documents()
        for doc in all_docs:
            doc_name = doc.get('doc_name', '')
            # ë¬¸ì„œëª…ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
            if any(k.upper() in doc_name.upper() for k in keywords):
                doc_id = doc.get('id')
                sop_doc = _sql_store.get_document_by_id(doc_id)
                if sop_doc:
                    # íŠ¹ì • ë¬¸ì„œ íƒ€ê²ŸíŒ… ì‹œì—ëŠ” SQLì—ì„œë„ ë” ë§ì´ ê°€ì ¸ì˜´
                    sql_limit = 10000 if target_doc_id else 2000
                    full_content = sop_doc.get("content", "")
                    if full_content:
                        results.append(f"[ë¬¸ì„œ ì „ì²´ ê°€ì´ë“œ] {doc_name}:\n{full_content[:sql_limit]}...")
                
    return "\n\n".join(results) if results else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. ê²€ìƒ‰ì–´ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”."

@tool
def get_version_history_tool(doc_id: str) -> str:

    """íŠ¹ì • ë¬¸ì„œì˜ ë²„ì „ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒ"""
    global _sql_store
    if not _sql_store: return "SQL ì €ì¥ì†Œ ì—°ê²° ì‹¤íŒ¨"
    versions = _sql_store.get_document_versions(doc_id)
    if not versions: return f"{doc_id} ë¬¸ì„œì˜ ë²„ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n".join([f"- v{v['version']} ({v['created_at']})" for v in versions])

@tool
def compare_versions_tool(doc_id: str, v1: str, v2: str) -> str:

    """ë‘ ë²„ì „ì˜ ë¬¸ì„œ ë‚´ìš©ì„ ë¹„êµí•˜ì—¬ ë°˜í™˜"""
    global _sql_store
    if not _sql_store: return ""
    

    doc1 = _sql_store.get_document_by_id(doc_id, v1)
    doc2 = _sql_store.get_document_by_id(doc_id, v2)
    
    if not doc1 or not doc2: return "ë¹„êµí•  ë²„ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return f"=== v{v1} ===\n{doc1.get('content')[:2000]}\n\n=== v{v2} ===\n{doc2.get('content')[:2000]}"

@tool
def get_references_tool(doc_id: str) -> str:
    """ì°¸ì¡° ê´€ê³„ ì¡°íšŒ"""
    import json
    from datetime import datetime

    global _graph_store
    if not _graph_store:
        return ""

    refs = _graph_store.get_document_relations(doc_id)

    if not refs:
        return ""

    # Neo4j DateTime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    def serialize_neo4j(obj):
        if hasattr(obj, 'to_native'):
            return obj.to_native().isoformat()
        elif isinstance(obj, dict):
            return {k: serialize_neo4j(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [serialize_neo4j(item) for item in obj]
        else:
            return obj

    refs_serialized = serialize_neo4j(refs)
    result = json.dumps(refs_serialized, ensure_ascii=False)
    return result

@tool
def get_sop_headers_tool(doc_id: str) -> str:
    """íŠ¹ì • ë¬¸ì„œì˜ ì‹¤ì œ ì¡°í•­(Clause) ëª©ë¡ê³¼ ì œëª©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    AIê°€ ìš”ì•½ ê³„íšì„ ì„¸ìš¸ ë•Œ 'ì§ì‘'í•˜ì§€ ì•Šê³  ì‹¤ì œ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _sql_store
    if not _sql_store: return "SQL ì €ì¥ì†Œ ì—°ê²° ì‹¤íŒ¨"
    
    doc = _sql_store.get_document_by_name(doc_id)
    if not doc: return f"'{doc_id}' ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    chunks = _sql_store.get_chunks_by_document(doc['id'])
    if not chunks: return f"'{doc_id}' ë¬¸ì„œì˜ ì¡°í•­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    headers = []
    seen_clauses = set()
    for c in chunks:
        clause = c.get('clause')
        if clause and clause not in seen_clauses:
            meta = c.get('metadata') or {}
            section = meta.get('section') or ""
            headers.append(f"- {clause}: {section}")
            seen_clauses.add(clause)
            
    return f"[{doc_id} ì¡°í•­ ëª©ë¡]\n" + "\n".join(headers)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Agent State
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AgentState(TypedDict):
    query: str
    messages: Annotated[List[Any], operator.add]

    next_agent: Literal["retrieval", "graph", "comparison", "answer", "end"]
    final_answer: str
    context: Annotated[List[str], operator.add]
    model_name: Optional[str]
    worker_model: Optional[str]
    orchestrator_model: Optional[str]
    loop_count: int
    # ì¶”ì  ì •ë³´ (í‰ê°€ìš©)
    agent_calls: Optional[Dict[str, int]]  # ì—ì´ì „íŠ¸ë³„ í˜¸ì¶œ íšŸìˆ˜
    tool_calls_log: Optional[List[Dict[str, Any]]]  # ë„êµ¬ í˜¸ì¶œ ë¡œê·¸
    validation_results: Optional[Dict[str, Any]]  # ê²€ì¦ ê²°ê³¼

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ ì •ì˜ (Nodes)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def orchestrator_node(state: AgentState):
    """ë©”ì¸ ì—ì´ì „íŠ¸ (OpenAI GPT-4o-mini) - ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…"""

    # ì¶”ì  ì •ë³´ ì´ˆê¸°í™”
    agent_calls = state.get("agent_calls") or {}
    agent_calls["orchestrator"] = agent_calls.get("orchestrator", 0) + 1

    # ë¬´í•œ ë£¨í”„ ë°©ì§€: 4ë²ˆ ì´ìƒ ë°˜ë³µí•˜ë©´ ê°•ì œ ì¢…ë£Œ
    # (ì •ìƒ íë¦„: retrieval -> orch -> graph -> orch -> answer ë„ 3íšŒ í•„ìš”)
    loop_count = state.get("loop_count", 0)
    if loop_count >= 4:
        print(f"ğŸ”´ ë£¨í”„ ì œí•œ ë„ë‹¬ ({loop_count}íšŒ), ê°•ì œ ì¢…ë£Œ")
        return {"next_agent": "answer", "loop_count": loop_count + 1, "agent_calls": agent_calls}
    
    client = get_openai_client()
    if not client:
        print("ğŸ”´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, retrievalë¡œ ë¼ìš°íŒ…")
        return {"next_agent": "retrieval", "loop_count": loop_count + 1}
    
    messages = state["messages"]
    
    system_prompt = """You are the orchestrator of the GMP regulatory system.
You direct sub-agents to resolve user questions and verify reported results.

## Routing (top-down, first match applies)

| Priority | Agent | Trigger Condition | Example |
|----------|-------|-------------------|---------|
| 1 | `comparison` | Questions about versions, changes, history, differences, or comparisons | "Show me the change history of SOP-001", "What changed?" |
| 2 | `graph` | References, citations, parent/child relationships, impact analysis | "Show me the reference list", "Find related regulations" |
| 3 | `chat` | Conversation context (History) questions or casual conversation | "What did I ask earlier?", "Hello", "Thanks" |
| 4 | `retrieval` | All regulation/knowledge questions not matching the above three | "What is the procedure when a deviation occurs?" |

> Note: `chat` is used only when asking about **conversation context**. "What is the purpose of SOP-001?" goes to `retrieval`.

## Workflow

1. Check History for any previously completed agent reports.
2. **If the report is sufficient** -> Proceed to `finish` immediately (confirm the sub-agent answer as-is; do not rewrite or summarize).
3. **If the report is insufficient** -> Call the appropriate agent.

## Core Rules

- **Immediate termination**: If the sub-agent answer contains `[DONE]` or sufficiently addresses the question, do not make any additional calls; proceed to `finish`.
- **When document ID is unconfirmed**: Before calling `comparison` or `graph`, first obtain the document ID via `retrieval`.
- **When results are excessive**: Do not ask the user for clarification; select the most relevant document and proceed.
- **Loop prevention**:
  - Do not repeat the same agent for the same purpose **more than 3 times**.
  - If `retrieval` reports "no results," do not re-call with the same search term -> change the search term or proceed to `finish`.
  - Do not make additional calls when sufficient evidence already exists.

## Output Format
```json
{"next_action": "retrieval | comparison | graph | chat | finish", "reason": "One-line justification"}
```"""
    
    # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ context ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•˜ì—¬ ë£¨í”„ ë°©ì§€
    current_context = state.get("context", [])
    combined_context_str = "\n".join([f"- {c[:1500]}..." for c in current_context]) if current_context else "ì—†ìŒ"
    
    # [DONE] íƒœê·¸ í™•ì¸ (ë£¨í”„ ê°•ì œ ì¢…ë£Œ ì¡°ê±´ - íŒŒì´ì¬ ë ˆë²¨ì—ì„œ í•˜ë“œì½”ë”©)
    has_done = any("[DONE]" in c for c in current_context)
    
    if has_done:
        print(f" [Orchestrator] [DONE] ì‹ í˜¸ ê°ì§€ -> ì¦‰ì‹œ ì¢…ë£Œ(finish) ê²°ì •")
        return {"next_agent": "answer"}

    orchestrator_input = f"""í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì—ì´ì „íŠ¸ë“¤ì˜ ë³´ê³ ì„œ ìš”ì•½:
    {combined_context_str}
    
    ìœ„ ë³´ê³ ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì„¸ìš”. ë§Œì•½ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆë‹¤ë©´ 'finish'ë¥¼ ì„ íƒí•˜ì„¸ìš”."""

    # [Guardrail] ë©”íƒ€ ì¸ì§€ ì§ˆë¬¸ ê°•ì œ ë¼ìš°íŒ… (LLM ì‹¤ìˆ˜ ë°©ì§€)
    # "ë°©ê¸ˆ", "ì´ì „", "ë­ë¼ê³ ", "ë‚´ ì§ˆë¬¸" ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆê³ , ì•„ì§ chat ì—ì´ì „íŠ¸ë¥¼ ë¶€ë¥´ì§€ ì•Šì•˜ë‹¤ë©´
    last_user_msg = messages[-1]["content"] if messages else ""
    meta_keywords = ["ë°©ê¸ˆ", "ë­ë¼ê³ ", "ì´ì „ ì§ˆë¬¸", "ë‚´ ì§ˆë¬¸", "ë¬´ìŠ¨ ë§", "ë¬´ìŠ¨ ì§ˆë¬¸", "ì§ì „", "ì²˜ìŒ ì§ˆë¬¸", "ì²« ì§ˆë¬¸", "ë§ˆì§€ë§‰ ì§ˆë¬¸", "ì•„ê¹Œ ì§ˆë¬¸"]
    is_meta_query = any(k in last_user_msg for k in meta_keywords)
    
    # ì´ë¯¸ chatì„ ë‹¤ë…€ì™”ê±°ë‚˜ ë£¨í”„ ì¤‘ì´ë¼ë©´ ë¬´ì‹œ
    if is_meta_query and "chat" not in agent_calls and loop_count == 0:
        print(f" [Guardrail] ë©”íƒ€ ì§ˆë¬¸ ê°ì§€ -> 'chat' ê°•ì œ ë¼ìš°íŒ… ('{last_user_msg}')")
        return {"next_agent": "chat", "loop_count": loop_count + 1, "agent_calls": agent_calls}

    # [Guardrail] ê´€ê³„/ì°¸ì¡°/ì˜í–¥ ì§ˆë¬¸ì€ graph ì—ì´ì „íŠ¸ ìš°ì„  ë¼ìš°íŒ…
    relation_keywords = [
        "ê´€ê³„", "ì°¸ì¡°", "ì¸ìš©", "ì—°ê²°", "ìƒìœ„ë¬¸ì„œ", "í•˜ìœ„ë¬¸ì„œ", "ê·¼ê±° ë¬¸ì„œ", "ì˜í–¥", "íŒŒê¸‰",
        "reference", "citation", "dependency", "impact", "relationship", "related regulation"
    ]
    is_relation_query = any(k.lower() in last_user_msg.lower() for k in relation_keywords)
    if is_relation_query and "graph" not in agent_calls and loop_count == 0:
        print(f" [Guardrail] ê´€ê³„ ì§ˆë¬¸ ê°ì§€ -> 'graph' ê°•ì œ ë¼ìš°íŒ… ('{last_user_msg}')")
        return {"next_agent": "graph", "loop_count": loop_count + 1, "agent_calls": agent_calls}

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                *messages,
                {"role": "user", "content": orchestrator_input}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        decision = safe_json_loads(content)
        
        print(f"[DEBUG Orchestrator] LLM ì‘ë‹µ: {content}")
        print(f"[DEBUG Orchestrator] íŒŒì‹±ëœ ê²°ì •: {decision}")

        next_agent = decision.get("next_action", "answer")  # LLMì´ next_actionì„ ë°˜í™˜í•¨
        if next_agent == "finish":
            next_agent = "answer"
        print(f"[DEBUG Orchestrator] next_agent ì¶”ì¶œ: {next_agent}")

        # ê²€ì¦: í—ˆìš©ëœ ê°’ë§Œ í†µê³¼ (stateì™€ ì •í™•íˆ ì¼ì¹˜)
        ALLOWED_AGENTS = {"retrieval", "graph", "comparison", "answer", "chat"}
        if next_agent not in ALLOWED_AGENTS:
            print(f"ğŸ”´ ì˜ëª»ëœ next_agent '{next_agent}' ê°ì§€, answerë¡œ ë³€ê²½")
            next_agent = "answer"
        else:
            print(f"âœ… next_agent '{next_agent}' ê²€ì¦ í†µê³¼")

        return {"next_agent": next_agent, "loop_count": loop_count + 1, "agent_calls": agent_calls}

    except Exception as e:
        print(f"Orchestrator Error: {e}")
        return {"next_agent": "answer", "final_answer": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "loop_count": loop_count + 1, "agent_calls": agent_calls}

    except Exception as e:
        print(f"Orchestrator Error: {e}")
        return {"next_agent": "answer", "final_answer": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "loop_count": loop_count + 1, "agent_calls": agent_calls}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_workflow():
    # ì„œë¸Œ ì—ì´ì „íŠ¸ ë…¸ë“œë“¤ì„ ì§€ì—° ì„í¬íŠ¸í•˜ì—¬ ìˆœí™˜ ì°¸ì¡°(Circular Import) ë°©ì§€
    try:
        from backend.sub_agent.search import retrieval_agent_node as node_retrieval
        from backend.sub_agent.graph import graph_agent_node as node_graph
        from backend.sub_agent.answer import answer_agent_node as node_answer
        from backend.sub_agent.compare import comparison_agent_node as node_comparison
        from backend.sub_agent.chat import chat_agent_node as node_chat
    except ImportError as e:
        error_msg = str(e)
        print(f" ì„œë¸Œ ì—ì´ì „íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í•¸ë“¤ëŸ¬ ì •ì˜ (ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜)
        def error_node(state): return {"messages": [{"role": "assistant", "content": f"ì—ì´ì „íŠ¸ ë¡œë”© ì—ëŸ¬: {error_msg}"}]}
        node_retrieval = error_node
        node_comparison = error_node
        node_graph = error_node
        node_answer = error_node
        node_chat = error_node

    workflow = StateGraph(AgentState)

    # Nodes
    workflow.add_node("orchestrator", orchestrator_node)
    workflow.add_node("retrieval", node_retrieval)
    workflow.add_node("comparison", node_comparison)
    workflow.add_node("graph", node_graph)
    workflow.add_node("answer", node_answer)
    workflow.add_node("chat", node_chat)
    
    # Edges
    workflow.add_edge(START, "orchestrator")
    
    # Router
    def router(state: AgentState):
        return state["next_agent"]
    
    workflow.add_conditional_edges(
        "orchestrator",
        router,
        {
            "retrieval": "retrieval",
            "comparison": "comparison",
            "graph": "graph",
            "answer": "answer",
            "chat": "chat",
            "end": END
        }
    )
    
    # ê° ì„œë¸Œ ì—ì´ì „íŠ¸ëŠ” ë‹¤ì‹œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ ëŒì•„ì™€ì„œ ê²°ê³¼ë¥¼ ë³´ê³ í•¨
    workflow.add_edge("retrieval", "orchestrator")
    workflow.add_edge("comparison", "orchestrator")
    workflow.add_edge("graph", "orchestrator")
    workflow.add_edge("chat", "orchestrator")
    
    # ë‹µë³€ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ë‹µë³€ì€ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œ
    workflow.add_edge("answer", END)
    
    return workflow.compile()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_app = None

def run_agent(query: str, session_id: str = "default", model_name: str = None, embedding_model: str = None, **kwargs):
    global _app
    if not _app:
        _app = create_workflow()

    # main.py -> run_agent(chat_history=...) ì „ë‹¬ê°’ ë°˜ì˜
    chat_history = kwargs.get("chat_history") or []
    messages = []
    if isinstance(chat_history, list):
        for msg in chat_history:
            if not isinstance(msg, dict):
                continue
            role = msg.get("role")
            content = msg.get("content")
            if role in {"system", "user", "assistant"} and content:
                messages.append({"role": role, "content": str(content)})
    messages.append({"role": "user", "content": query})

    initial_state = {
        "query": query,
        "messages": messages,
        "next_agent": "orchestrator",
        "worker_model": model_name or "gpt-4o-mini",
        "orchestrator_model": "gpt-4o-mini",
        "model_name": model_name,
        "loop_count": 0,
        "agent_calls": {},  # ì—ì´ì „íŠ¸ í˜¸ì¶œ ì¶”ì 
        "tool_calls_log": [],  # Tool í˜¸ì¶œ ë¡œê·¸
        "validation_results": {}  # ê²€ì¦ ê²°ê³¼
    }

    # LangGraph ì‹¤í–‰
    result = _app.invoke(initial_state, config={"recursion_limit": 10})

    # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
    final_answer = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    if "messages" in result and result["messages"]:
        last_msg = result["messages"][-1]
        if hasattr(last_msg, "content"):
            final_answer = last_msg.content
        elif isinstance(last_msg, dict):
            final_answer = last_msg.get("content", final_answer)

    # context ì¶”ì¶œ (í‰ê°€ìš©)
    context = result.get("context", [])
    context_str = "\n\n".join(context) if isinstance(context, list) else context

    # ========================================
    # í‰ê°€ ë¡œê·¸ ìƒì„± (agent_log)
    # ========================================

    # 1. ì—ì´ì „íŠ¸ í˜¸ì¶œ í†µê³„
    agent_calls = result.get("agent_calls", {})

    # 2. [USE: ...] íƒœê·¸ ë¶„ì„
    use_tags = re.findall(r'\[USE:\s*([^\|]+)\s*\|\s*([^\]]+)\]', final_answer)
    use_tag_count = len(use_tags)

    # 3. Tool í˜¸ì¶œ ë¡œê·¸ ì¶”ì¶œ (messagesì—ì„œ)
    tool_calls_log = []
    for msg in result.get("messages", []):
        if isinstance(msg, dict) and msg.get("role") == "tool":
            tool_calls_log.append({
                "role": "tool",
                "content_preview": msg.get("content", "")[:200]
            })
        elif hasattr(msg, "role") and msg.role == "tool":
            content = msg.content if hasattr(msg, "content") else str(msg)
            tool_calls_log.append({
                "role": "tool",
                "content_preview": content[:200]
            })

    # 4. ê²€ì¦ ê²°ê³¼ ë¶„ì„
    validation_summary = {
        "grounding": "unknown",
        "format": "unknown",
        "has_use_tags": use_tag_count > 0,
        "no_info_found": False
    }

    # NO_INFO_FOUND ê°ì§€
    if "ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†" in final_answer or \
       "ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in final_answer or \
       "[NO_INFO_FOUND]" in final_answer:
        validation_summary["no_info_found"] = True

    # 5. ê²€ìƒ‰ ì¡°ê±´ ì¶”ì¶œ (contextì—ì„œ)
    search_conditions = []
    for ctx in context:
        # [Deep Search] ë¡œê·¸ì—ì„œ ê²€ìƒ‰ ì¡°ê±´ ì¶”ì¶œ ì‹œë„
        if "Deep Search" in ctx or "ê²€ìƒ‰" in ctx:
            search_conditions.append({
                "query": query,
                "preview": ctx[:150]
            })

    return {
        "answer": final_answer,
        "agent_log": {
            # ê¸°ë³¸ ì •ë³´
            "query": query,
            "context": context_str,
            "next_agent": result.get("next_agent"),
            "loop_count": result.get("loop_count", 0),

            # ì—ì´ì „íŠ¸ í˜¸ì¶œ í†µê³„
            "agent_calls": agent_calls,
            "total_agent_calls": sum(agent_calls.values()) if agent_calls else 0,

            # Tool í˜¸ì¶œ ì •ë³´
            "tool_calls_count": len(tool_calls_log),
            "tool_calls_log": tool_calls_log[:5],  # ìµœëŒ€ 5ê°œë§Œ

            # íƒœê·¸ ë¶„ì„
            "use_tag_count": use_tag_count,
            "use_tags_sample": use_tags[:3] if use_tags else [],  # ìƒ˜í”Œ 3ê°œ

            # ê²€ì¦ ê²°ê³¼
            "validation_summary": validation_summary,

            # ê²€ìƒ‰ ì¡°ê±´
            "search_conditions": search_conditions[:3]  # ìµœëŒ€ 3ê°œ
        },
        "wrapper": True
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì™¸ë¶€ ë…¸ì¶œ ë„êµ¬ ëª©ë¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AGENT_TOOLS = [
    search_sop_tool,
    get_version_history_tool,
    compare_versions_tool,
    get_references_tool,
    get_sop_headers_tool,
    compare_versions_tool
]
