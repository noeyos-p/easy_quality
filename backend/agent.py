"""
SOP ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ v13.0
- Orchestrator (Main): OpenAI (GPT-4o) - ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…, ìµœì¢… ë‹µë³€
- Specialized Sub-Agents: Z.AI (GLM-4.7) - ì‹¤í–‰ ë° ë°ì´í„° ì²˜ë¦¬
  1. Retrieval Agent: ë¬¸ì„œ ê²€ìƒ‰ ë° ì¶”ì¶œ
  2. Summarization Agent: ë¬¸ì„œ/ì¡°í•­ ìš”ì•½
  3. Comparison Agent: ë²„ì „ ë¹„êµ
  4. Graph Agent: ì°¸ì¡° ê´€ê³„ ì¡°íšŒ
"""

import os
import re
import json
import operator
from typing import List, Dict, Optional, Any, Annotated, TypedDict, Literal
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„í¬íŠ¸ ë° ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    from openai import OpenAI
except ImportError:
    pass

try:
    from zai import ZaiClient
    ZAI_AVAILABLE = True
except ImportError:
    ZAI_AVAILABLE = False
    pass

try:
    from langchain_core.tools import tool
    from langgraph.graph import StateGraph, START, END
    from langsmith import traceable
    LANGCHAIN_AVAILABLE = True
    LANGGRAPH_AGENT_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AGENT_AVAILABLE = False
    pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì „ì—­ ìŠ¤í† ì–´ ë° í´ë¼ì´ì–¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_vector_store = None
_graph_store = None
_sql_store = None

_openai_client = None
_zai_client = None

def init_agent_tools(vector_store_module, graph_store_instance, sql_store_instance=None):
    global _vector_store, _graph_store, _sql_store
    _vector_store = vector_store_module
    _graph_store = graph_store_instance
    _sql_store = sql_store_instance

def get_openai_client():
    global _openai_client
    if not _openai_client:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            _openai_client = OpenAI(api_key=api_key)
    return _openai_client

def get_zai_client():
    global _zai_client
    if not _zai_client:
        api_key = os.getenv("ZAI_API_KEY")
        if api_key:
            _zai_client = ZaiClient(api_key=api_key)
    return _zai_client

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë„êµ¬ ì •ì˜ (Tools)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool
def search_sop_tool(query: str, extract_english: bool = False, keywords: List[str] = None) -> str:
    """SOP ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬.
    extract_english: Trueë©´ ì˜ë¬¸ ë‚´ìš© ìœ„ì£¼ë¡œ ì¶”ì¶œ
    """
    global _sql_store, _vector_store
    
    results = []
    seen_ids = set()
    
    # 1. SQL í‚¤ì›Œë“œ ê²€ìƒ‰
    if _sql_store and keywords:
        all_docs = _sql_store.list_documents()
        for doc in all_docs:
            sop_doc = _sql_store.get_document_by_id(doc['sop_id'])
            if not sop_doc: continue
            
            content = sop_doc.get("markdown_content", "")
            # í‚¤ì›Œë“œ ë§¤ì¹­ (ë‹¨ìˆœí™”)
            if any(k.upper() in doc['sop_id'].upper() for k in keywords):
                if extract_english:
                    # ì˜ë¬¸ ì¶”ì¶œ ë¡œì§ (ì•ŒíŒŒë²³ ë¹„ìœ¨ì´ ë†’ì€ ë¬¸ë‹¨ë§Œ í•„í„°ë§)
                    eng_paras = [p for p in content.split('\n\n') if len(re.findall(r'[a-zA-Z]', p)) > len(re.findall(r'[ê°€-í£]', p))]
                    results.append(f"ğŸ“„ [ì˜ë¬¸ ë°œì·Œ] {doc['sop_id']}:\n" + "\n".join(eng_paras[:5]))
                else:
                    results.append(f"ğŸ“„ [ì „ì²´ ë³¸ë¬¸] {doc['sop_id']}:\n{content[:3000]}...")
                seen_ids.add(doc['sop_id'])
    
    # 2. ë²¡í„° ê²€ìƒ‰
    if _vector_store:
        vec_res = _vector_store.search(query, n_results=5)
        for r in vec_res:
            meta = r.get('metadata', {})
            sop_id = meta.get('sop_id')
            if sop_id not in seen_ids:
                results.append(f"ğŸ“„ [ê²€ìƒ‰] {sop_id} > {meta.get('section', '')}:\n{r.get('text', '')}")
                
    return "\n\n".join(results) if results else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

@tool
def get_version_history_tool(sop_id: str) -> str:
    """íŠ¹ì • ë¬¸ì„œì˜ ë²„ì „ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒ"""
    global _sql_store
    if not _sql_store: return "SQL ì €ì¥ì†Œ ì—°ê²° ì‹¤íŒ¨"
    
    versions = _sql_store.get_document_versions(sop_id)
    if not versions: return f"{sop_id} ë¬¸ì„œì˜ ë²„ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return "\n".join([f"- v{v['doc_metadata'].get('version')} ({v['created_at']})" for v in versions])

@tool
def compare_versions_tool(sop_id: str, v1: str, v2: str) -> str:
    """ë‘ ë²„ì „ì˜ ë¬¸ì„œ ë‚´ìš©ì„ ë¹„êµí•˜ì—¬ ë°˜í™˜"""
    global _sql_store
    if not _sql_store: return ""
    
    doc1 = _sql_store.get_document_by_id(sop_id, v1)
    doc2 = _sql_store.get_document_by_id(sop_id, v2)
    
    if not doc1 or not doc2: return "ë¹„êµí•  ë²„ì „ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return f"=== v{v1} ===\n{doc1.get('markdown_content')[:2000]}\n\n=== v{v2} ===\n{doc2.get('markdown_content')[:2000]}"

@tool
def get_references_tool(sop_id: str) -> str:
    """ì°¸ì¡° ê´€ê³„ ì¡°íšŒ"""
    global _graph_store
    if not _graph_store: return ""
    refs = _graph_store.get_document_references(sop_id)
    return str(refs)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Agent State
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AgentState(TypedDict):
    query: str
    messages: Annotated[List[Any], operator.add]
    next_agent: Literal["retrieval", "summary", "comparison", "graph", "end"]
    final_answer: str
    context: str
    model_name: Optional[str] # ë™ì  ëª¨ë¸ ì„ íƒì„ ìœ„í•œ í•„ë“œ ì¶”ê°€

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë…¸ë“œ ì •ì˜ (Nodes)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def orchestrator_node(state: AgentState):
    """
    ë©”ì¸ ì—ì´ì „íŠ¸ (OpenAI GPT-4o)
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì„œë¸Œ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…í•˜ê±°ë‚˜ ìµœì¢… ë‹µë³€ì„ ìƒì„±
    """
    client = get_openai_client()
    messages = state["messages"]
    
    system_prompt = """ë‹¹ì‹ ì€ GMP ê·œì • ì‹œìŠ¤í…œì˜ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°(Manager)ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í•˜ìœ„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ë“¤ì„ ì§€íœ˜í•˜ê³ , ê·¸ë“¤ì˜ ë³´ê³ ë¥¼ ê²€ì¦í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    [ì‘ì—… íë¦„]
    1. **History ë¶„ì„**: ì´ì „ ëŒ€í™” ë‚´ìš©(History)ì„ ë³´ê³ , ì´ë¯¸ ìˆ˜í–‰ëœ ì—ì´ì „íŠ¸ì˜ ë³´ê³ ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    2. **íŒë‹¨(Judgement)**: 
       - ë³´ê³  ë‚´ìš©ì´ ì¶©ë¶„í•˜ë‹¤ë©´ -> 'finish'ë¥¼ ì„ íƒí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
       - ë³´ê³  ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ -> ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜, ê²€ìƒ‰ ì¡°ê±´ì„ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•˜ê²Œ í•˜ì„¸ìš”.
       - ì•„ì§ ì‹œì‘ ë‹¨ê³„ë¼ë©´ -> ì ì ˆí•œ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
    
    [ì—ì´ì „íŠ¸ ëª©ë¡]
    1. retrieval: ê·œì • ê²€ìƒ‰, ì •ë³´ ì¡°íšŒ (SQL:í‚¤ì›Œë“œ/IDìš°ì„  -> Vector:ì˜ë¯¸ê²€ìƒ‰)
    2. summary: ë¬¸ì„œë‚˜ ì¡°í•­ì˜ ìš”ì•½ (SQL ì›ë¬¸ ì¡°íšŒ í›„ ìš”ì•½)
    3. comparison: êµ¬ë²„ì „/ì‹ ë²„ì „ ë¹„êµ (SQL ë²„ì „ íˆìŠ¤í† ë¦¬)
    4. graph: ì°¸ì¡°/ì¸ìš© ê´€ê³„ í™•ì¸ (Graph DB)
    
    [ì¶œë ¥ í˜•ì‹]
    JSON í˜•ì‹ìœ¼ë¡œ 'next_action' (agent ì´ë¦„ ë˜ëŠ” 'finish')ê³¼ 'reason'ì„ ë°˜í™˜í•˜ì„¸ìš”.
    ì˜ˆ: {"next_action": "retrieval", "reason": "ê·œì • ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ì¬ê²€ìƒ‰ í•„ìš”"}
    """
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë„êµ¬ ê²°ê³¼(Context)ë¼ë©´ ë‹µë³€ ìƒì„± ëª¨ë“œë¡œ ì§„ì… í™•ë¥  ë†’ìŒ
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": system_prompt}] + messages,
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        decision = json.loads(content)
        
        next_action = decision.get("next_action", "finish")
        
        # ë§Œì•½ finishë¼ë©´ ìµœì¢… ë‹µë³€ ìƒì„±
        if next_action == "finish":
            # í•œ ë²ˆ ë” í˜¸ì¶œí•˜ì—¬ ìì—°ì–´ ë‹µë³€ ìƒì„±
            final_res = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "system", "content": "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ìµœì¢… ë‹µë³€ì„ í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´."}] + messages
            )
            return {"next_agent": "end", "final_answer": final_res.choices[0].message.content}
            
        return {"next_agent": next_action}
        
    except Exception as e:
        print(f"Orchestrator Error: {e}")
        return {"next_agent": "end", "final_answer": "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

def retrieval_agent_node(state: AgentState):
    """[ì„œë¸Œ] ê²€ìƒ‰ ì—ì´ì „íŠ¸ (Z.AI)"""
    client = get_zai_client()
    query = state["query"]
    
    # í•œê¸€/ì˜ë¬¸ ìš”ì²­ ë¶„ì„
    is_english_req = "ì˜ë¬¸" in query or "ì˜ì–´" in query or "english" in query.lower()
    
    # 1. ë„êµ¬ ì‹¤í–‰ (ì§ì ‘ í˜¸ì¶œ)
    # ì‹¤ì œë¡œëŠ” LLMì´ ë„êµ¬ ì¸ìë¥¼ ê²°ì •í•˜ê²Œ í•  ìˆ˜ ìˆìœ¼ë‚˜ ì—¬ê¸°ì„  ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”
    search_res = search_sop_tool.invoke({"query": query, "extract_english": is_english_req, "keywords": query.split()})
    
    # 2. ê²°ê³¼ ì •ë¦¬ (Z.AI)
    prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ì§ˆë¬¸: {query}
    
    [ê²€ìƒ‰ ê²°ê³¼]
    {search_res}
    
    ìš”ì•½í•´ì„œ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”."""
    
    res = client.chat.completions.create(
        model=state.get("model_name") or "glm-4.7-flash", # ë™ì  ëª¨ë¸ ì ìš©
        messages=[{"role": "user", "content": prompt}]
    )
    
    return {"messages": [{"role": "assistant", "content": f"[ê²€ìƒ‰ ì—ì´ì „íŠ¸ ë³´ê³ ]\n{res.choices[0].message.content}"}]}

def summary_agent_node(state: AgentState):
    """[ì„œë¸Œ] ìš”ì•½ ì—ì´ì „íŠ¸ (Z.AI)"""
    client = get_zai_client()
    query = state["query"]
    
    search_res = search_sop_tool.invoke({"query": query})
    
    res = client.chat.completions.create(
        model=state.get("model_name") or "glm-4.7-flash", # ë™ì  ëª¨ë¸ ì ìš©
        messages=[
            {"role": "system", "content": "ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”."},
            {"role": "user", "content": f"ì§ˆë¬¸: {query}\n\në¬¸ì„œ ë‚´ìš©:\n{search_res}"}
        ]
    )
    return {"messages": [{"role": "assistant", "content": f"[ìš”ì•½ ì—ì´ì „íŠ¸ ë³´ê³ ]\n{res.choices[0].message.content}"}]}

def comparison_agent_node(state: AgentState):
    """[ì„œë¸Œ] ë¹„êµ ì—ì´ì „íŠ¸ (Z.AI)"""
    client = get_zai_client()
    query = state["query"]
    
    # ID ë° ë²„ì „ ì¶”ì¶œ ë¡œì§ì€ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ ê°€ì •
    # ì˜ˆ: "SOP-001 ë²„ì „ 1.0ê³¼ 2.0 ë¹„êµí•´ì¤˜" -> ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš©ìœ¼ë¡œ í•˜ë“œì½”ë”©ëœ ë¡œì§ ëŒ€ì‹  LLMì—ê²Œ ì¶”ì¶œ ìœ ë„ ê°€ëŠ¥
    
    res = client.chat.completions.create(
        model=state.get("model_name") or "glm-4.7-flash", # ë™ì  ëª¨ë¸ ì ìš©
        messages=[{"role": "user", "content": f"ì‚¬ìš©ì ì§ˆë¬¸ '{query}'ì—ì„œ ë¬¸ì„œ IDì™€ ë²„ì „ ë‘ ê°œë¥¼ ì¶”ì¶œí•´ì„œ JSONìœ¼ë¡œ ì¤˜. í˜•ì‹: {{'id': '...', 'v1': '...', 'v2': '...'}} "}]
    )
    try:
        info = json.loads(res.choices[0].message.content)
        comp_res = compare_versions_tool.invoke({"sop_id": info['id'], "v1": info['v1'], "v2": info['v2']})
        
        final_res = client.chat.completions.create(
            model=state.get("model_name") or "glm-4.7-flash", # ë™ì  ëª¨ë¸ ì ìš©
            messages=[{"role": "user", "content": f"ë‘ ë²„ì „ì˜ ì°¨ì´ì ì„ ë¶„ì„í•´ì¤˜:\n{comp_res}"}]
        )
        content = final_res.choices[0].message.content
    except:
        content = "ë²„ì „ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆ: SOP-001 1.0ê³¼ 2.0 ë¹„êµí•´ì¤˜)"

    return {"messages": [{"role": "assistant", "content": f"[ë¹„êµ ì—ì´ì „íŠ¸ ë³´ê³ ]\n{content}"}]}

def graph_agent_node(state: AgentState):
    """[ì„œë¸Œ] ê·¸ë˜í”„ ì—ì´ì „íŠ¸ (Z.AI)"""
    client = get_zai_client()
    query = state["query"]
    
    # SOP ID ì¶”ì¶œ
    match = re.search(r'([A-Z]{2}-SOP-\d+)', query.upper())
    if match:
        sop_id = match.group(1)
        refs = get_references_tool.invoke({"sop_id": sop_id})
        content = f"ë¬¸ì„œ {sop_id}ì˜ ì°¸ì¡° ê´€ê³„ì…ë‹ˆë‹¤:\n{refs}"
    else:
        content = "ë¬¸ì„œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    return {"messages": [{"role": "assistant", "content": f"[ê·¸ë˜í”„ ì—ì´ì „íŠ¸ ë³´ê³ ]\n{content}"}]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_workflow():
    workflow = StateGraph(AgentState)
    
    # Nodes
    workflow.add_node("orchestrator", orchestrator_node)
    workflow.add_node("retrieval", retrieval_agent_node)
    workflow.add_node("summary", summary_agent_node)
    workflow.add_node("comparison", comparison_agent_node)
    workflow.add_node("graph", graph_agent_node)
    
    # Edges
    workflow.add_edge(START, "orchestrator")
    
    # Router
    def router(state: AgentState):
        return state["next_agent"]
    
    workflow.add_conditional_edges(
        "orchestrator",
        router,
        {
            "retrieval": "retrieval",
            "summary": "summary",
            "comparison": "comparison",
            "graph": "graph",
            "end": END
        }
    )
    
    # ê° ì„œë¸Œ ì—ì´ì „íŠ¸ëŠ” ë‹¤ì‹œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¡œ ëŒì•„ì™€ì„œ ê²°ê³¼ë¥¼ ë³´ê³ í•¨
    workflow.add_edge("retrieval", "orchestrator")
    workflow.add_edge("summary", "orchestrator")
    workflow.add_edge("comparison", "orchestrator")
    workflow.add_edge("graph", "orchestrator")
    
    return workflow.compile()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_app = None

def run_agent(query: str, session_id: str = "default", model_name: str = None, embedding_model: str = None, **kwargs):
    global _app
    if not _app:
        _app = create_workflow()
        
    initial_state = {
        "query": query,
        "messages": [{"role": "user", "content": query}],
        "next_agent": "orchestrator",
        "model_name": model_name # ëª¨ë¸ ì •ë³´ ì£¼ì…
    }
    
    # LangGraph ì‹¤í–‰
    result = _app.invoke(initial_state)
    
    return {
        "answer": result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
        "wrapper": True # í˜¸í™˜ì„±
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì™¸ë¶€ ë…¸ì¶œ ë„êµ¬ ëª©ë¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AGENT_TOOLS = [
    search_sop_tool,
    get_version_history_tool,
    compare_versions_tool,
    get_references_tool
]
